{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "vertical-harvard",
   "metadata": {},
   "source": [
    "# 1. Abstract\n",
    "\n",
    "This document shows applying Deep Q-Learning Network model to a game SpaceInvaders(SpaceInvaders-v0) from Open AI Gym Atari.\n",
    "\n",
    "Deep Q learning, as published in (Mnih et al, 2013), leverages advances in deep learning to learn policies from high dimensional sensory input. Specifically, it learns with raw pixels from Atari 2600 games using convolutional networks, instead of low-dimensional feature vectors.On a higher level, Deep Q learning works as such:\n",
    "Gather and store samples in a replay buffer with current policy; Random sample batches of experiences from the replay buffer (known as Experience Replay);Use the sampled experiences to update the Q network; and repeat above[4].\n",
    "\n",
    "After implementing for more than 11 hours, we get about 2000 episode results and several gif visualization results.Even though the outcome is not very perfect, it shows this game agent needs much more hours to train andif applied deep q-learning network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statutory-wisdom",
   "metadata": {},
   "source": [
    "# 2. Setting up the enivironment\n",
    "Import libraries and install tensorflow(version=2.4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "angry-blair",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run in environment tensorflow in the local, not base\n",
    "\n",
    "#Import the libraiies\n",
    "from tensorflow.keras import Sequential, layers\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import copy\n",
    "import gym\n",
    "import os\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aerial-basketball",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce tensorflow warning level\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "\n",
    "# Configure GPU memory\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.compat.v1.InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "essential-window",
   "metadata": {},
   "source": [
    "# 3. Create deep Q-learning and train the agent\n",
    "Initialize the environment, build deep Q-learning Network model, save memory, train the agent and choose action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "committed-error",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    #Set up hyperparameters\n",
    "    def __init__(self, env):\n",
    "        self._env = env\n",
    "        self.memory = deque(maxlen=128)\n",
    "        self.gamma = 0.5 # Discounting rate\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_decay = 0.999 # exponential decay rate for exploration prob\n",
    "        self.epsilon_min = 0.2 \n",
    "\n",
    "        self.simple_size = 16\n",
    "\n",
    "        self.model = self._Build_Deep_Q_Network()\n",
    "        self.opt = tf.keras.optimizers.Adam()\n",
    "\n",
    "    def _Build_Deep_Q_Network(self):\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(layers.Conv2D(filters=32, kernel_size=(3, 3), strides=1, padding='valid'))\n",
    "        model.add(layers.Activation('relu'))\n",
    "        model.add(layers.MaxPooling2D(pool_size=(2, 2), strides=2, padding='same'))\n",
    "\n",
    "        model.add(layers.Flatten())\n",
    "        model.add(layers.Dense(units=128, activation='relu'))\n",
    "        model.add(layers.Dense(units=32, activation='relu'))\n",
    "        model.add(layers.Dense(units=6))\n",
    "\n",
    "        model.build(input_shape=(None, 40, 40, 1))\n",
    "        model.summary()\n",
    "        return model\n",
    "\n",
    "    def save_memory(self, state, action, reward, next_state, done):\n",
    "\n",
    "        _state = np.array([state], dtype=np.float64)/255.0\n",
    "        _next_state = np.array([next_state], dtype=np.float64)/255.0\n",
    "\n",
    "        self.memory.append((_state, action, reward, _next_state, done))\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        batchs = min(self.simple_size, len(self.memory))\n",
    "        training_data = random.sample(self.memory, batchs)\n",
    "\n",
    "        loss_sum = 0.\n",
    "        count = 0\n",
    "        for i in training_data:\n",
    "            state, action, reward, next_state, done = i\n",
    "\n",
    "            y_reward = reward\n",
    "            if not done:\n",
    "                y_reward = reward + self.gamma * np.amax(self.model.predict(next_state)[0])\n",
    "\n",
    "            _y = self.model.predict(state)\n",
    "            _y[0][action] = y_reward\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                loss = tf.losses.mean_squared_error(y_true=_y, y_pred=self.model(state))\n",
    "                grad = tape.gradient(loss, self.model.trainable_variables)\n",
    "                self.opt.apply_gradients(zip(grad, self.model.trainable_variables))\n",
    "            loss_sum += loss\n",
    "\n",
    "            count += 1\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "        print(\" loss : {}\".format(loss_sum / count))\n",
    "        return loss_sum / count\n",
    "\n",
    "\n",
    "    def choice_action(self, state):\n",
    "        _state = np.array([state], dtype=np.float64)/255.0\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return self._env.action_space.sample()\n",
    "        else:\n",
    "            action = self.model.predict(_state)\n",
    "            return np.argmax(action[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "capital-hacker",
   "metadata": {},
   "source": [
    "# 4.Define the preprocessing function\n",
    "Preprocessing is an important step, which can help reduce the complexity of our states to reduce the computation time needed for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "australian-matthew",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(state):\n",
    "    state = state[20:]\n",
    "    state = np.mean(state, axis=2)\n",
    "    state = cv2.resize(state, dsize=(40, 40))\n",
    "    state = np.reshape(state, newshape=(40, 40, 1))\n",
    "\n",
    "    state[state < 10] = 0\n",
    "    state[state >= 10] = 255\n",
    "\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flying-motivation",
   "metadata": {},
   "source": [
    "# 5. Show episode result and visualize agent play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "distinct-somerset",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 38, 38, 32)        320       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 38, 38, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 19, 19, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 11552)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               1478784   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 198       \n",
      "=================================================================\n",
      "Total params: 1,483,430\n",
      "Trainable params: 1,483,430\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "episode: 1/10000, steps: 881, epsilon: 1.0\n",
      " loss : [632.06256]\n",
      "episode: 2/10000, steps: 611, epsilon: 0.999\n",
      " loss : [440.18216]\n",
      "episode: 3/10000, steps: 643, epsilon: 0.998001\n",
      " loss : [436.10822]\n",
      "episode: 4/10000, steps: 799, epsilon: 0.997002999\n",
      " loss : [14.990583]\n",
      "episode: 5/10000, steps: 584, epsilon: 0.996005996001\n",
      " loss : [757.63354]\n",
      "episode: 6/10000, steps: 642, epsilon: 0.995009990004999\n",
      " loss : [384.98346]\n",
      "episode: 7/10000, steps: 735, epsilon: 0.994014980014994\n",
      " loss : [359.30823]\n",
      "episode: 8/10000, steps: 376, epsilon: 0.993020965034979\n",
      " loss : [646.95544]\n",
      "episode: 9/10000, steps: 1070, epsilon: 0.9920279440699441\n",
      " loss : [243.54402]\n",
      "episode: 10/10000, steps: 518, epsilon: 0.9910359161258742\n",
      " loss : [36.710926]\n",
      "episode: 11/10000, steps: 383, epsilon: 0.9900448802097482\n",
      " loss : [37.816036]\n",
      "episode: 12/10000, steps: 681, epsilon: 0.9890548353295385\n",
      " loss : [19.003458]\n",
      "episode: 13/10000, steps: 970, epsilon: 0.988065780494209\n",
      " loss : [11.030337]\n",
      "episode: 14/10000, steps: 390, epsilon: 0.9870777147137147\n",
      " loss : [11.1870775]\n",
      "episode: 15/10000, steps: 691, epsilon: 0.986090636999001\n",
      " loss : [8.077616]\n",
      "episode: 16/10000, steps: 520, epsilon: 0.9851045463620021\n",
      " loss : [15.389388]\n",
      "episode: 17/10000, steps: 502, epsilon: 0.98411944181564\n",
      " loss : [356.28888]\n",
      "episode: 18/10000, steps: 570, epsilon: 0.9831353223738244\n",
      " loss : [13.774286]\n",
      "episode: 19/10000, steps: 655, epsilon: 0.9821521870514506\n",
      " loss : [24.157219]\n",
      "episode: 20/10000, steps: 598, epsilon: 0.9811700348643991\n",
      " loss : [7.534718]\n",
      "episode: 21/10000, steps: 553, epsilon: 0.9801888648295347\n",
      " loss : [13.884939]\n",
      "episode: 22/10000, steps: 658, epsilon: 0.9792086759647052\n",
      " loss : [14.053358]\n",
      "episode: 23/10000, steps: 779, epsilon: 0.9782294672887405\n",
      " loss : [7.120727]\n",
      "episode: 24/10000, steps: 937, epsilon: 0.9772512378214517\n",
      " loss : [8.362764]\n",
      "episode: 25/10000, steps: 969, epsilon: 0.9762739865836303\n",
      " loss : [11.408486]\n",
      "episode: 26/10000, steps: 634, epsilon: 0.9752977125970467\n",
      " loss : [14.684895]\n",
      "episode: 27/10000, steps: 1416, epsilon: 0.9743224148844496\n",
      " loss : [10.374186]\n",
      "episode: 28/10000, steps: 725, epsilon: 0.9733480924695651\n",
      " loss : [10.627067]\n",
      "episode: 29/10000, steps: 421, epsilon: 0.9723747443770956\n",
      " loss : [9.55609]\n",
      "episode: 30/10000, steps: 609, epsilon: 0.9714023696327185\n",
      " loss : [11.373124]\n",
      "episode: 31/10000, steps: 392, epsilon: 0.9704309672630859\n",
      " loss : [7.77879]\n",
      "episode: 32/10000, steps: 938, epsilon: 0.9694605362958227\n",
      " loss : [5.9702883]\n",
      "episode: 33/10000, steps: 626, epsilon: 0.9684910757595269\n",
      " loss : [12.9589205]\n",
      "episode: 34/10000, steps: 758, epsilon: 0.9675225846837673\n",
      " loss : [12.249049]\n",
      "episode: 35/10000, steps: 419, epsilon: 0.9665550620990835\n",
      " loss : [13.609913]\n",
      "episode: 36/10000, steps: 483, epsilon: 0.9655885070369844\n",
      " loss : [9.923152]\n",
      "episode: 37/10000, steps: 345, epsilon: 0.9646229185299474\n",
      " loss : [11.556563]\n",
      "episode: 38/10000, steps: 1018, epsilon: 0.9636582956114175\n",
      " loss : [17.773724]\n",
      "episode: 39/10000, steps: 618, epsilon: 0.9626946373158061\n",
      " loss : [9.998256]\n",
      "episode: 40/10000, steps: 408, epsilon: 0.9617319426784903\n",
      " loss : [21.630135]\n",
      "episode: 41/10000, steps: 528, epsilon: 0.9607702107358118\n",
      " loss : [8.801116]\n",
      "episode: 42/10000, steps: 650, epsilon: 0.959809440525076\n",
      " loss : [10.982203]\n",
      "episode: 43/10000, steps: 1268, epsilon: 0.9588496310845509\n",
      " loss : [13.688333]\n",
      "episode: 44/10000, steps: 702, epsilon: 0.9578907814534664\n",
      " loss : [16.59453]\n",
      "episode: 45/10000, steps: 482, epsilon: 0.9569328906720129\n",
      " loss : [17.205263]\n",
      "episode: 46/10000, steps: 773, epsilon: 0.9559759577813409\n",
      " loss : [8.020264]\n",
      "episode: 47/10000, steps: 827, epsilon: 0.9550199818235596\n",
      " loss : [10.653117]\n",
      "episode: 48/10000, steps: 630, epsilon: 0.9540649618417361\n",
      " loss : [18.1959]\n",
      "episode: 49/10000, steps: 551, epsilon: 0.9531108968798944\n",
      " loss : [9.816962]\n",
      "episode: 50/10000, steps: 522, epsilon: 0.9521577859830145\n",
      " loss : [13.509703]\n",
      "episode: 51/10000, steps: 695, epsilon: 0.9512056281970315\n",
      " loss : [9.894327]\n",
      "episode: 52/10000, steps: 611, epsilon: 0.9502544225688344\n",
      " loss : [372.44583]\n",
      "episode: 53/10000, steps: 523, epsilon: 0.9493041681462656\n",
      " loss : [312.50317]\n",
      "episode: 54/10000, steps: 843, epsilon: 0.9483548639781193\n",
      " loss : [225.32301]\n",
      "episode: 55/10000, steps: 373, epsilon: 0.9474065091141411\n",
      " loss : [446.25125]\n",
      "episode: 56/10000, steps: 1029, epsilon: 0.946459102605027\n",
      " loss : [195.6272]\n",
      "episode: 57/10000, steps: 668, epsilon: 0.9455126435024219\n",
      " loss : [37.17642]\n",
      "episode: 58/10000, steps: 685, epsilon: 0.9445671308589195\n",
      " loss : [5.439761]\n",
      "episode: 59/10000, steps: 1170, epsilon: 0.9436225637280606\n",
      " loss : [9.423395]\n",
      "episode: 60/10000, steps: 1021, epsilon: 0.9426789411643326\n",
      " loss : [13.010718]\n",
      "episode: 61/10000, steps: 413, epsilon: 0.9417362622231683\n",
      " loss : [13.485638]\n",
      "episode: 62/10000, steps: 848, epsilon: 0.9407945259609451\n",
      " loss : [10.67758]\n",
      "episode: 63/10000, steps: 681, epsilon: 0.9398537314349842\n",
      " loss : [16.821728]\n",
      "episode: 64/10000, steps: 487, epsilon: 0.9389138777035492\n",
      " loss : [12.964053]\n",
      "episode: 65/10000, steps: 585, epsilon: 0.9379749638258457\n",
      " loss : [4.163401]\n",
      "episode: 66/10000, steps: 765, epsilon: 0.9370369888620198\n",
      " loss : [16.234638]\n",
      "episode: 67/10000, steps: 405, epsilon: 0.9360999518731578\n",
      " loss : [9.996754]\n",
      "episode: 68/10000, steps: 718, epsilon: 0.9351638519212846\n",
      " loss : [8.932375]\n",
      "episode: 69/10000, steps: 368, epsilon: 0.9342286880693633\n",
      " loss : [12.231791]\n",
      "episode: 70/10000, steps: 887, epsilon: 0.933294459381294\n",
      " loss : [13.210805]\n",
      "episode: 71/10000, steps: 798, epsilon: 0.9323611649219127\n",
      " loss : [10.354658]\n",
      "episode: 72/10000, steps: 741, epsilon: 0.9314288037569908\n",
      " loss : [12.548955]\n",
      "episode: 73/10000, steps: 634, epsilon: 0.9304973749532338\n",
      " loss : [7.0449786]\n",
      "episode: 74/10000, steps: 928, epsilon: 0.9295668775782806\n",
      " loss : [7.9758053]\n",
      "episode: 75/10000, steps: 950, epsilon: 0.9286373107007023\n",
      " loss : [9.905135]\n",
      "episode: 76/10000, steps: 977, epsilon: 0.9277086733900016\n",
      " loss : [349.8234]\n",
      "episode: 77/10000, steps: 1158, epsilon: 0.9267809647166116\n",
      " loss : [285.1688]\n",
      "episode: 78/10000, steps: 594, epsilon: 0.925854183751895\n",
      " loss : [99.19239]\n",
      "episode: 79/10000, steps: 910, epsilon: 0.9249283295681431\n",
      " loss : [9.124174]\n",
      "episode: 80/10000, steps: 751, epsilon: 0.9240034012385749\n",
      " loss : [346.31522]\n",
      "episode: 81/10000, steps: 578, epsilon: 0.9230793978373364\n",
      " loss : [12.629901]\n",
      "episode: 82/10000, steps: 462, epsilon: 0.9221563184394991\n",
      " loss : [11.985897]\n",
      "episode: 83/10000, steps: 520, epsilon: 0.9212341621210596\n",
      " loss : [16.375011]\n",
      "episode: 84/10000, steps: 786, epsilon: 0.9203129279589385\n",
      " loss : [14.987133]\n",
      "episode: 85/10000, steps: 956, epsilon: 0.9193926150309796\n",
      " loss : [6.474112]\n",
      "episode: 86/10000, steps: 474, epsilon: 0.9184732224159486\n",
      " loss : [16.010345]\n",
      "episode: 87/10000, steps: 377, epsilon: 0.9175547491935327\n",
      " loss : [9.039815]\n",
      "episode: 88/10000, steps: 538, epsilon: 0.9166371944443392\n",
      " loss : [6.542642]\n",
      "episode: 89/10000, steps: 370, epsilon: 0.9157205572498949\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " loss : [8.331831]\n",
      "episode: 90/10000, steps: 660, epsilon: 0.914804836692645\n",
      " loss : [9.676029]\n",
      "episode: 91/10000, steps: 733, epsilon: 0.9138900318559524\n",
      " loss : [8.729343]\n",
      "episode: 92/10000, steps: 392, epsilon: 0.9129761418240965\n",
      " loss : [9.406696]\n",
      "episode: 93/10000, steps: 629, epsilon: 0.9120631656822724\n",
      " loss : [13.725804]\n",
      "episode: 94/10000, steps: 884, epsilon: 0.9111511025165902\n",
      " loss : [7.220252]\n",
      "episode: 95/10000, steps: 656, epsilon: 0.9102399514140735\n",
      " loss : [12.306911]\n",
      "episode: 96/10000, steps: 1249, epsilon: 0.9093297114626595\n",
      " loss : [8.255973]\n",
      "episode: 97/10000, steps: 676, epsilon: 0.9084203817511969\n",
      " loss : [3.3792112]\n",
      "episode: 98/10000, steps: 535, epsilon: 0.9075119613694457\n",
      " loss : [8.643132]\n",
      "episode: 99/10000, steps: 1419, epsilon: 0.9066044494080763\n",
      " loss : [356.7812]\n",
      "episode: 100/10000, steps: 437, epsilon: 0.9056978449586682\n",
      " loss : [14.098992]\n",
      "episode: 101/10000, steps: 906, epsilon: 0.9047921471137096\n",
      " loss : [25.687578]\n",
      "episode: 102/10000, steps: 459, epsilon: 0.9038873549665959\n",
      " loss : [10.867044]\n",
      "episode: 103/10000, steps: 635, epsilon: 0.9029834676116293\n",
      " loss : [16.575232]\n",
      "episode: 104/10000, steps: 453, epsilon: 0.9020804841440176\n",
      " loss : [16.59628]\n",
      "episode: 105/10000, steps: 547, epsilon: 0.9011784036598737\n",
      " loss : [16.095135]\n",
      "episode: 106/10000, steps: 767, epsilon: 0.9002772252562138\n",
      " loss : [14.937125]\n",
      "episode: 107/10000, steps: 655, epsilon: 0.8993769480309576\n",
      " loss : [6.0409517]\n",
      "episode: 108/10000, steps: 513, epsilon: 0.8984775710829266\n",
      " loss : [8.665394]\n",
      "episode: 109/10000, steps: 502, epsilon: 0.8975790935118436\n",
      " loss : [9.165191]\n",
      "episode: 110/10000, steps: 706, epsilon: 0.8966815144183318\n",
      " loss : [6.5921817]\n",
      "episode: 111/10000, steps: 604, epsilon: 0.8957848329039134\n",
      " loss : [12.323073]\n",
      "episode: 112/10000, steps: 909, epsilon: 0.8948890480710096\n",
      " loss : [7.186944]\n",
      "episode: 113/10000, steps: 712, epsilon: 0.8939941590229386\n",
      " loss : [17.51989]\n",
      "episode: 114/10000, steps: 861, epsilon: 0.8931001648639156\n",
      " loss : [12.049533]\n",
      "episode: 115/10000, steps: 630, epsilon: 0.8922070646990518\n",
      " loss : [13.72787]\n",
      "episode: 116/10000, steps: 1512, epsilon: 0.8913148576343527\n",
      " loss : [14.178734]\n",
      "episode: 117/10000, steps: 643, epsilon: 0.8904235427767183\n",
      " loss : [8.801018]\n",
      "episode: 118/10000, steps: 986, epsilon: 0.8895331192339416\n",
      " loss : [9.565236]\n",
      "episode: 119/10000, steps: 902, epsilon: 0.8886435861147077\n",
      " loss : [10.529473]\n",
      "episode: 120/10000, steps: 784, epsilon: 0.887754942528593\n",
      " loss : [10.440704]\n",
      "episode: 121/10000, steps: 974, epsilon: 0.8868671875860644\n",
      " loss : [715.55194]\n",
      "episode: 122/10000, steps: 837, epsilon: 0.8859803203984784\n",
      " loss : [28.144764]\n",
      "episode: 123/10000, steps: 905, epsilon: 0.88509434007808\n",
      " loss : [320.4047]\n",
      "episode: 124/10000, steps: 701, epsilon: 0.8842092457380019\n",
      " loss : [279.32312]\n",
      "episode: 125/10000, steps: 810, epsilon: 0.8833250364922639\n",
      " loss : [48.714165]\n",
      "episode: 126/10000, steps: 812, epsilon: 0.8824417114557717\n",
      " loss : [23.363342]\n",
      "episode: 127/10000, steps: 391, epsilon: 0.8815592697443159\n",
      " loss : [15.535737]\n",
      "episode: 128/10000, steps: 892, epsilon: 0.8806777104745716\n",
      " loss : [362.73438]\n",
      "episode: 129/10000, steps: 389, epsilon: 0.8797970327640969\n",
      " loss : [24.438099]\n",
      "episode: 130/10000, steps: 830, epsilon: 0.8789172357313328\n",
      " loss : [16.56688]\n",
      "episode: 131/10000, steps: 836, epsilon: 0.8780383184956015\n",
      " loss : [16.289919]\n",
      "episode: 132/10000, steps: 813, epsilon: 0.8771602801771059\n",
      " loss : [306.83255]\n",
      "episode: 133/10000, steps: 622, epsilon: 0.8762831198969288\n",
      " loss : [26.31917]\n",
      "episode: 134/10000, steps: 604, epsilon: 0.8754068367770318\n",
      " loss : [322.94113]\n",
      "episode: 135/10000, steps: 771, epsilon: 0.8745314299402548\n",
      " loss : [36.2573]\n",
      "episode: 136/10000, steps: 800, epsilon: 0.8736568985103146\n",
      " loss : [30.70338]\n",
      "episode: 137/10000, steps: 630, epsilon: 0.8727832416118043\n",
      " loss : [20.68504]\n",
      "episode: 138/10000, steps: 939, epsilon: 0.8719104583701925\n",
      " loss : [8.565616]\n",
      "episode: 139/10000, steps: 572, epsilon: 0.8710385479118223\n",
      " loss : [10.099457]\n",
      "episode: 140/10000, steps: 1163, epsilon: 0.8701675093639105\n",
      " loss : [351.2883]\n",
      "episode: 141/10000, steps: 678, epsilon: 0.8692973418545467\n",
      " loss : [314.952]\n",
      "episode: 142/10000, steps: 849, epsilon: 0.8684280445126921\n",
      " loss : [30.502876]\n",
      "episode: 143/10000, steps: 802, epsilon: 0.8675596164681794\n",
      " loss : [24.96266]\n",
      "episode: 144/10000, steps: 1056, epsilon: 0.8666920568517111\n",
      " loss : [7.8096895]\n",
      "episode: 145/10000, steps: 623, epsilon: 0.8658253647948594\n",
      " loss : [8.614625]\n",
      "episode: 146/10000, steps: 825, epsilon: 0.8649595394300645\n",
      " loss : [5.2968435]\n",
      "episode: 147/10000, steps: 783, epsilon: 0.8640945798906344\n",
      " loss : [13.375251]\n",
      "episode: 148/10000, steps: 807, epsilon: 0.8632304853107438\n",
      " loss : [12.846208]\n",
      "episode: 149/10000, steps: 815, epsilon: 0.862367254825433\n",
      " loss : [296.95193]\n",
      "episode: 150/10000, steps: 430, epsilon: 0.8615048875706075\n",
      " loss : [13.072093]\n",
      "episode: 151/10000, steps: 848, epsilon: 0.8606433826830369\n",
      " loss : [12.843877]\n",
      "episode: 152/10000, steps: 848, epsilon: 0.8597827393003539\n",
      " loss : [7.500764]\n",
      "episode: 153/10000, steps: 808, epsilon: 0.8589229565610536\n",
      " loss : [11.92841]\n",
      "episode: 154/10000, steps: 617, epsilon: 0.8580640336044925\n",
      " loss : [16.837374]\n",
      "episode: 155/10000, steps: 659, epsilon: 0.857205969570888\n",
      " loss : [6.317097]\n",
      "episode: 156/10000, steps: 809, epsilon: 0.8563487636013172\n",
      " loss : [9.453482]\n",
      "episode: 157/10000, steps: 806, epsilon: 0.8554924148377159\n",
      " loss : [7.5686393]\n",
      "episode: 158/10000, steps: 599, epsilon: 0.8546369224228781\n",
      " loss : [9.580577]\n",
      "episode: 159/10000, steps: 401, epsilon: 0.8537822855004553\n",
      " loss : [5.972604]\n",
      "episode: 160/10000, steps: 638, epsilon: 0.8529285032149548\n",
      " loss : [17.748133]\n",
      "episode: 161/10000, steps: 1365, epsilon: 0.8520755747117399\n",
      " loss : [3.3375514]\n",
      "episode: 162/10000, steps: 620, epsilon: 0.8512234991370281\n",
      " loss : [7.494892]\n",
      "episode: 163/10000, steps: 755, epsilon: 0.8503722756378911\n",
      " loss : [8.131544]\n",
      "episode: 164/10000, steps: 627, epsilon: 0.8495219033622532\n",
      " loss : [11.15424]\n",
      "episode: 165/10000, steps: 1071, epsilon: 0.8486723814588909\n",
      " loss : [352.24744]\n",
      "episode: 166/10000, steps: 608, epsilon: 0.847823709077432\n",
      " loss : [270.22662]\n",
      "episode: 167/10000, steps: 644, epsilon: 0.8469758853683546\n",
      " loss : [31.912537]\n",
      "episode: 168/10000, steps: 564, epsilon: 0.8461289094829862\n",
      " loss : [37.400177]\n",
      "episode: 169/10000, steps: 788, epsilon: 0.8452827805735033\n",
      " loss : [9.110472]\n",
      "episode: 170/10000, steps: 918, epsilon: 0.8444374977929298\n",
      " loss : [8.508363]\n",
      "episode: 171/10000, steps: 603, epsilon: 0.8435930602951368\n",
      " loss : [9.139342]\n",
      "episode: 172/10000, steps: 623, epsilon: 0.8427494672348417\n",
      " loss : [13.254685]\n",
      "episode: 173/10000, steps: 884, epsilon: 0.8419067177676068\n",
      " loss : [15.094187]\n",
      "episode: 174/10000, steps: 460, epsilon: 0.8410648110498392\n",
      " loss : [8.903418]\n",
      "episode: 175/10000, steps: 469, epsilon: 0.8402237462387894\n",
      " loss : [354.6123]\n",
      "episode: 176/10000, steps: 857, epsilon: 0.8393835224925505\n",
      " loss : [10.773329]\n",
      "episode: 177/10000, steps: 773, epsilon: 0.838544138970058\n",
      " loss : [10.203851]\n",
      "episode: 178/10000, steps: 700, epsilon: 0.8377055948310879\n",
      " loss : [11.481068]\n",
      "episode: 179/10000, steps: 784, epsilon: 0.8368678892362568\n",
      " loss : [8.979716]\n",
      "episode: 180/10000, steps: 549, epsilon: 0.8360310213470206\n",
      " loss : [10.720603]\n",
      "episode: 181/10000, steps: 825, epsilon: 0.8351949903256736\n",
      " loss : [13.287443]\n",
      "episode: 182/10000, steps: 729, epsilon: 0.8343597953353479\n",
      " loss : [8.434572]\n",
      "episode: 183/10000, steps: 823, epsilon: 0.8335254355400126\n",
      " loss : [9.753478]\n",
      "episode: 184/10000, steps: 683, epsilon: 0.8326919101044725\n",
      " loss : [10.463745]\n",
      "episode: 185/10000, steps: 919, epsilon: 0.831859218194368\n",
      " loss : [7.4061704]\n",
      "episode: 186/10000, steps: 680, epsilon: 0.8310273589761736\n",
      " loss : [10.798224]\n",
      "episode: 187/10000, steps: 510, epsilon: 0.8301963316171974\n",
      " loss : [5.8329268]\n",
      "episode: 188/10000, steps: 775, epsilon: 0.8293661352855802\n",
      " loss : [6.264984]\n",
      "episode: 189/10000, steps: 854, epsilon: 0.8285367691502946\n",
      " loss : [6.2940083]\n",
      "episode: 190/10000, steps: 460, epsilon: 0.8277082323811443\n",
      " loss : [5.3686867]\n",
      "episode: 191/10000, steps: 1126, epsilon: 0.8268805241487632\n",
      " loss : [12.488546]\n",
      "episode: 192/10000, steps: 503, epsilon: 0.8260536436246144\n",
      " loss : [5.9095097]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 193/10000, steps: 568, epsilon: 0.8252275899809898\n",
      " loss : [8.295223]\n",
      "episode: 194/10000, steps: 685, epsilon: 0.8244023623910088\n",
      " loss : [7.2063775]\n",
      "episode: 195/10000, steps: 669, epsilon: 0.8235779600286178\n",
      " loss : [8.444803]\n",
      "episode: 196/10000, steps: 675, epsilon: 0.8227543820685892\n",
      " loss : [7.1811056]\n",
      "episode: 197/10000, steps: 779, epsilon: 0.8219316276865206\n",
      " loss : [5.8777356]\n",
      "episode: 198/10000, steps: 652, epsilon: 0.8211096960588341\n",
      " loss : [11.860527]\n",
      "episode: 199/10000, steps: 649, epsilon: 0.8202885863627752\n",
      " loss : [7.7345734]\n",
      "episode: 200/10000, steps: 363, epsilon: 0.8194682977764125\n",
      " loss : [8.980623]\n",
      "episode: 201/10000, steps: 751, epsilon: 0.818648829478636\n",
      " loss : [12.229623]\n",
      "episode: 202/10000, steps: 602, epsilon: 0.8178301806491574\n",
      " loss : [4.390532]\n",
      "episode: 203/10000, steps: 920, epsilon: 0.8170123504685082\n",
      " loss : [4.604148]\n",
      "episode: 204/10000, steps: 842, epsilon: 0.8161953381180397\n",
      " loss : [8.751116]\n",
      "episode: 205/10000, steps: 678, epsilon: 0.8153791427799216\n",
      " loss : [4.31732]\n",
      "episode: 206/10000, steps: 562, epsilon: 0.8145637636371417\n",
      " loss : [7.4570494]\n",
      "episode: 207/10000, steps: 436, epsilon: 0.8137491998735046\n",
      " loss : [4.3883085]\n",
      "episode: 208/10000, steps: 797, epsilon: 0.812935450673631\n",
      " loss : [4.1959877]\n",
      "episode: 209/10000, steps: 1306, epsilon: 0.8121225152229574\n",
      " loss : [5.943361]\n",
      "episode: 210/10000, steps: 818, epsilon: 0.8113103927077344\n",
      " loss : [342.6865]\n",
      "episode: 211/10000, steps: 1306, epsilon: 0.8104990823150267\n",
      " loss : [10.724772]\n",
      "episode: 212/10000, steps: 907, epsilon: 0.8096885832327116\n",
      " loss : [23.31441]\n",
      "episode: 213/10000, steps: 567, epsilon: 0.8088788946494789\n",
      " loss : [21.40913]\n",
      "episode: 214/10000, steps: 795, epsilon: 0.8080700157548294\n",
      " loss : [8.735712]\n",
      "episode: 215/10000, steps: 785, epsilon: 0.8072619457390746\n",
      " loss : [5.9820337]\n",
      "episode: 216/10000, steps: 1066, epsilon: 0.8064546837933355\n",
      " loss : [2.0854123]\n",
      "episode: 217/10000, steps: 499, epsilon: 0.8056482291095421\n",
      " loss : [9.924469]\n",
      "episode: 218/10000, steps: 624, epsilon: 0.8048425808804326\n",
      " loss : [7.362947]\n",
      "episode: 219/10000, steps: 1066, epsilon: 0.8040377382995522\n",
      " loss : [11.399204]\n",
      "episode: 220/10000, steps: 476, epsilon: 0.8032337005612527\n",
      " loss : [10.433229]\n",
      "episode: 221/10000, steps: 830, epsilon: 0.8024304668606914\n",
      " loss : [8.886768]\n",
      "episode: 222/10000, steps: 616, epsilon: 0.8016280363938307\n",
      " loss : [342.2152]\n",
      "episode: 223/10000, steps: 386, epsilon: 0.8008264083574369\n",
      " loss : [7.458849]\n",
      "episode: 224/10000, steps: 637, epsilon: 0.8000255819490795\n",
      " loss : [8.872369]\n",
      "episode: 225/10000, steps: 771, epsilon: 0.7992255563671304\n",
      " loss : [10.463686]\n",
      "episode: 226/10000, steps: 1113, epsilon: 0.7984263308107633\n",
      " loss : [19.983807]\n",
      "episode: 227/10000, steps: 1144, epsilon: 0.7976279044799526\n",
      " loss : [11.647244]\n",
      "episode: 228/10000, steps: 400, epsilon: 0.7968302765754727\n",
      " loss : [19.390265]\n",
      "episode: 229/10000, steps: 822, epsilon: 0.7960334462988972\n",
      " loss : [11.690805]\n",
      "episode: 230/10000, steps: 758, epsilon: 0.7952374128525983\n",
      " loss : [15.072693]\n",
      "episode: 231/10000, steps: 872, epsilon: 0.7944421754397457\n",
      " loss : [12.463205]\n",
      "episode: 232/10000, steps: 1096, epsilon: 0.7936477332643059\n",
      " loss : [7.912971]\n",
      "episode: 233/10000, steps: 670, epsilon: 0.7928540855310416\n",
      " loss : [2.9806042]\n",
      "episode: 234/10000, steps: 696, epsilon: 0.7920612314455105\n",
      " loss : [6.33232]\n",
      "episode: 235/10000, steps: 640, epsilon: 0.7912691702140651\n",
      " loss : [4.8555346]\n",
      "episode: 236/10000, steps: 655, epsilon: 0.790477901043851\n",
      " loss : [356.78308]\n",
      "episode: 237/10000, steps: 800, epsilon: 0.7896874231428072\n",
      " loss : [20.156704]\n",
      "episode: 238/10000, steps: 704, epsilon: 0.7888977357196644\n",
      " loss : [11.0171]\n",
      "episode: 239/10000, steps: 614, epsilon: 0.7881088379839447\n",
      " loss : [12.681705]\n",
      "episode: 240/10000, steps: 413, epsilon: 0.7873207291459607\n",
      " loss : [5.4079423]\n",
      "episode: 241/10000, steps: 1290, epsilon: 0.7865334084168147\n",
      " loss : [12.757326]\n",
      "episode: 242/10000, steps: 810, epsilon: 0.7857468750083979\n",
      " loss : [11.371596]\n",
      "episode: 243/10000, steps: 975, epsilon: 0.7849611281333895\n",
      " loss : [355.63226]\n",
      "episode: 244/10000, steps: 784, epsilon: 0.784176167005256\n",
      " loss : [17.173683]\n",
      "episode: 245/10000, steps: 402, epsilon: 0.7833919908382508\n",
      " loss : [13.057776]\n",
      "episode: 246/10000, steps: 535, epsilon: 0.7826085988474126\n",
      " loss : [332.7294]\n",
      "episode: 247/10000, steps: 634, epsilon: 0.7818259902485653\n",
      " loss : [7.5165653]\n",
      "episode: 248/10000, steps: 849, epsilon: 0.7810441642583167\n",
      " loss : [10.986086]\n",
      "episode: 249/10000, steps: 786, epsilon: 0.7802631200940584\n",
      " loss : [11.911594]\n",
      "episode: 250/10000, steps: 1069, epsilon: 0.7794828569739644\n",
      " loss : [376.29492]\n",
      "episode: 251/10000, steps: 812, epsilon: 0.7787033741169904\n",
      " loss : [582.10956]\n",
      "episode: 252/10000, steps: 908, epsilon: 0.7779246707428734\n",
      " loss : [363.71567]\n",
      "episode: 253/10000, steps: 665, epsilon: 0.7771467460721305\n",
      " loss : [29.034687]\n",
      "episode: 254/10000, steps: 610, epsilon: 0.7763695993260584\n",
      " loss : [11.82969]\n",
      "episode: 255/10000, steps: 778, epsilon: 0.7755932297267324\n",
      " loss : [7.236601]\n",
      "episode: 256/10000, steps: 829, epsilon: 0.7748176364970056\n",
      " loss : [6.6268687]\n",
      "episode: 257/10000, steps: 858, epsilon: 0.7740428188605086\n",
      " loss : [10.3535185]\n",
      "episode: 258/10000, steps: 808, epsilon: 0.7732687760416481\n",
      " loss : [7.577419]\n",
      "episode: 259/10000, steps: 665, epsilon: 0.7724955072656065\n",
      " loss : [10.594278]\n",
      "episode: 260/10000, steps: 614, epsilon: 0.7717230117583408\n",
      " loss : [4.259462]\n",
      "episode: 261/10000, steps: 821, epsilon: 0.7709512887465825\n",
      " loss : [4.899206]\n",
      "episode: 262/10000, steps: 626, epsilon: 0.7701803374578359\n",
      " loss : [13.477667]\n",
      "episode: 263/10000, steps: 816, epsilon: 0.7694101571203781\n",
      " loss : [3.9058857]\n",
      "episode: 264/10000, steps: 811, epsilon: 0.7686407469632577\n",
      " loss : [8.871289]\n",
      "episode: 265/10000, steps: 801, epsilon: 0.7678721062162944\n",
      " loss : [1.9162302]\n",
      "episode: 266/10000, steps: 706, epsilon: 0.7671042341100781\n",
      " loss : [4.437227]\n",
      "episode: 267/10000, steps: 817, epsilon: 0.766337129875968\n",
      " loss : [4.939924]\n",
      "episode: 268/10000, steps: 834, epsilon: 0.7655707927460921\n",
      " loss : [6.61641]\n",
      "episode: 269/10000, steps: 815, epsilon: 0.764805221953346\n",
      " loss : [2.3160796]\n",
      "episode: 270/10000, steps: 783, epsilon: 0.7640404167313927\n",
      " loss : [5.766336]\n",
      "episode: 271/10000, steps: 655, epsilon: 0.7632763763146613\n",
      " loss : [2.7879605]\n",
      "episode: 272/10000, steps: 808, epsilon: 0.7625130999383466\n",
      " loss : [7.0638804]\n",
      "episode: 273/10000, steps: 998, epsilon: 0.7617505868384082\n",
      " loss : [4.09855]\n",
      "episode: 274/10000, steps: 628, epsilon: 0.7609888362515699\n",
      " loss : [9.39123]\n",
      "episode: 275/10000, steps: 793, epsilon: 0.7602278474153183\n",
      " loss : [8.697493]\n",
      "episode: 276/10000, steps: 1525, epsilon: 0.759467619567903\n",
      " loss : [7.0632153]\n",
      "episode: 277/10000, steps: 942, epsilon: 0.7587081519483351\n",
      " loss : [5.1748424]\n",
      "episode: 278/10000, steps: 638, epsilon: 0.7579494437963867\n",
      " loss : [7.918214]\n",
      "episode: 279/10000, steps: 814, epsilon: 0.7571914943525904\n",
      " loss : [7.011367]\n",
      "episode: 280/10000, steps: 716, epsilon: 0.7564343028582378\n",
      " loss : [9.944463]\n",
      "episode: 281/10000, steps: 827, epsilon: 0.7556778685553796\n",
      " loss : [8.110813]\n",
      "episode: 282/10000, steps: 1106, epsilon: 0.7549221906868242\n",
      " loss : [7.5056643]\n",
      "episode: 283/10000, steps: 458, epsilon: 0.7541672684961374\n",
      " loss : [7.5015817]\n",
      "episode: 284/10000, steps: 1027, epsilon: 0.7534131012276413\n",
      " loss : [3.8715189]\n",
      "episode: 285/10000, steps: 990, epsilon: 0.7526596881264136\n",
      " loss : [9.957332]\n",
      "episode: 286/10000, steps: 530, epsilon: 0.7519070284382872\n",
      " loss : [8.490159]\n",
      "episode: 287/10000, steps: 682, epsilon: 0.7511551214098489\n",
      " loss : [12.096675]\n",
      "episode: 288/10000, steps: 552, epsilon: 0.750403966288439\n",
      " loss : [6.769482]\n",
      "episode: 289/10000, steps: 874, epsilon: 0.7496535623221505\n",
      " loss : [12.094772]\n",
      "episode: 290/10000, steps: 691, epsilon: 0.7489039087598284\n",
      " loss : [12.8661585]\n",
      "episode: 291/10000, steps: 966, epsilon: 0.7481550048510686\n",
      " loss : [4.4132466]\n",
      "episode: 292/10000, steps: 530, epsilon: 0.7474068498462175\n",
      " loss : [5.236117]\n",
      "episode: 293/10000, steps: 702, epsilon: 0.7466594429963713\n",
      " loss : [11.385514]\n",
      "episode: 294/10000, steps: 654, epsilon: 0.745912783553375\n",
      " loss : [343.50348]\n",
      "episode: 295/10000, steps: 862, epsilon: 0.7451668707698216\n",
      " loss : [283.2084]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 296/10000, steps: 790, epsilon: 0.7444217038990517\n",
      " loss : [289.84592]\n",
      "episode: 297/10000, steps: 806, epsilon: 0.7436772821951526\n",
      " loss : [77.4027]\n",
      "episode: 298/10000, steps: 657, epsilon: 0.7429336049129575\n",
      " loss : [11.333845]\n",
      "episode: 299/10000, steps: 797, epsilon: 0.7421906713080445\n",
      " loss : [7.4509788]\n",
      "episode: 300/10000, steps: 616, epsilon: 0.7414484806367364\n",
      " loss : [3.9758067]\n",
      "episode: 301/10000, steps: 384, epsilon: 0.7407070321560997\n",
      " loss : [6.3101835]\n",
      "episode: 302/10000, steps: 591, epsilon: 0.7399663251239436\n",
      " loss : [6.7263174]\n",
      "episode: 303/10000, steps: 625, epsilon: 0.7392263587988196\n",
      " loss : [10.140472]\n",
      "episode: 304/10000, steps: 733, epsilon: 0.7384871324400207\n",
      " loss : [10.032749]\n",
      "episode: 305/10000, steps: 769, epsilon: 0.7377486453075807\n",
      " loss : [13.851644]\n",
      "episode: 306/10000, steps: 1208, epsilon: 0.737010896662273\n",
      " loss : [322.3882]\n",
      "episode: 307/10000, steps: 915, epsilon: 0.7362738857656108\n",
      " loss : [13.040939]\n",
      "episode: 308/10000, steps: 638, epsilon: 0.7355376118798452\n",
      " loss : [16.805067]\n",
      "episode: 309/10000, steps: 423, epsilon: 0.7348020742679654\n",
      " loss : [18.760033]\n",
      "episode: 310/10000, steps: 826, epsilon: 0.7340672721936974\n",
      " loss : [14.3984995]\n",
      "episode: 311/10000, steps: 843, epsilon: 0.7333332049215037\n",
      " loss : [12.473693]\n",
      "episode: 312/10000, steps: 680, epsilon: 0.7325998717165821\n",
      " loss : [9.417216]\n",
      "episode: 313/10000, steps: 347, epsilon: 0.7318672718448656\n",
      " loss : [13.861418]\n",
      "episode: 314/10000, steps: 673, epsilon: 0.7311354045730207\n",
      " loss : [10.526018]\n",
      "episode: 315/10000, steps: 711, epsilon: 0.7304042691684477\n",
      " loss : [8.5839205]\n",
      "episode: 316/10000, steps: 651, epsilon: 0.7296738648992792\n",
      " loss : [291.35013]\n",
      "episode: 317/10000, steps: 749, epsilon: 0.7289441910343799\n",
      " loss : [12.855792]\n",
      "episode: 318/10000, steps: 661, epsilon: 0.7282152468433455\n",
      " loss : [8.312233]\n",
      "episode: 319/10000, steps: 853, epsilon: 0.7274870315965022\n",
      " loss : [245.21011]\n",
      "episode: 320/10000, steps: 784, epsilon: 0.7267595445649057\n",
      " loss : [14.673403]\n",
      "episode: 321/10000, steps: 655, epsilon: 0.7260327850203407\n",
      " loss : [4.7071695]\n",
      "episode: 322/10000, steps: 791, epsilon: 0.7253067522353204\n",
      " loss : [11.755741]\n",
      "episode: 323/10000, steps: 795, epsilon: 0.724581445483085\n",
      " loss : [3.435296]\n",
      "episode: 324/10000, steps: 1006, epsilon: 0.723856864037602\n",
      " loss : [8.159636]\n",
      "episode: 325/10000, steps: 673, epsilon: 0.7231330071735643\n",
      " loss : [5.3334928]\n",
      "episode: 326/10000, steps: 649, epsilon: 0.7224098741663908\n",
      " loss : [5.7359815]\n",
      "episode: 327/10000, steps: 803, epsilon: 0.7216874642922244\n",
      " loss : [2.9769454]\n",
      "episode: 328/10000, steps: 982, epsilon: 0.7209657768279322\n",
      " loss : [12.307235]\n",
      "episode: 329/10000, steps: 778, epsilon: 0.7202448110511043\n",
      " loss : [8.146912]\n",
      "episode: 330/10000, steps: 793, epsilon: 0.7195245662400531\n",
      " loss : [703.59106]\n",
      "episode: 331/10000, steps: 553, epsilon: 0.7188050416738131\n",
      " loss : [40.10952]\n",
      "episode: 332/10000, steps: 689, epsilon: 0.7180862366321393\n",
      " loss : [277.84155]\n",
      "episode: 333/10000, steps: 656, epsilon: 0.7173681503955072\n",
      " loss : [19.980318]\n",
      "episode: 334/10000, steps: 524, epsilon: 0.7166507822451117\n",
      " loss : [9.478254]\n",
      "episode: 335/10000, steps: 609, epsilon: 0.7159341314628666\n",
      " loss : [8.904404]\n",
      "episode: 336/10000, steps: 1060, epsilon: 0.7152181973314037\n",
      " loss : [16.901087]\n",
      "episode: 337/10000, steps: 1456, epsilon: 0.7145029791340722\n",
      " loss : [11.017287]\n",
      "episode: 338/10000, steps: 583, epsilon: 0.7137884761549381\n",
      " loss : [5.992502]\n",
      "episode: 339/10000, steps: 759, epsilon: 0.7130746876787832\n",
      " loss : [14.982122]\n",
      "episode: 340/10000, steps: 609, epsilon: 0.7123616129911045\n",
      " loss : [16.19765]\n",
      "episode: 341/10000, steps: 534, epsilon: 0.7116492513781133\n",
      " loss : [10.898597]\n",
      "episode: 342/10000, steps: 1362, epsilon: 0.7109376021267352\n",
      " loss : [5.641894]\n",
      "episode: 343/10000, steps: 809, epsilon: 0.7102266645246085\n",
      " loss : [12.839367]\n",
      "episode: 344/10000, steps: 770, epsilon: 0.7095164378600839\n",
      " loss : [6.2383237]\n",
      "episode: 345/10000, steps: 846, epsilon: 0.7088069214222238\n",
      " loss : [10.080201]\n",
      "episode: 346/10000, steps: 1060, epsilon: 0.7080981145008015\n",
      " loss : [5.1759562]\n",
      "episode: 347/10000, steps: 664, epsilon: 0.7073900163863007\n",
      " loss : [10.258973]\n",
      "episode: 348/10000, steps: 826, epsilon: 0.7066826263699144\n",
      " loss : [15.586968]\n",
      "episode: 349/10000, steps: 688, epsilon: 0.7059759437435444\n",
      " loss : [8.815645]\n",
      "episode: 350/10000, steps: 559, epsilon: 0.7052699677998009\n",
      " loss : [15.430149]\n",
      "episode: 351/10000, steps: 681, epsilon: 0.704564697832001\n",
      " loss : [10.84062]\n",
      "episode: 352/10000, steps: 724, epsilon: 0.7038601331341691\n",
      " loss : [12.63357]\n",
      "episode: 353/10000, steps: 390, epsilon: 0.7031562730010349\n",
      " loss : [7.8321457]\n",
      "episode: 354/10000, steps: 500, epsilon: 0.7024531167280339\n",
      " loss : [11.889661]\n",
      "episode: 355/10000, steps: 602, epsilon: 0.7017506636113059\n",
      " loss : [12.49588]\n",
      "episode: 356/10000, steps: 629, epsilon: 0.7010489129476946\n",
      " loss : [9.708458]\n",
      "episode: 357/10000, steps: 698, epsilon: 0.7003478640347469\n",
      " loss : [5.956083]\n",
      "episode: 358/10000, steps: 601, epsilon: 0.6996475161707122\n",
      " loss : [4.560439]\n",
      "episode: 359/10000, steps: 789, epsilon: 0.6989478686545415\n",
      " loss : [14.486804]\n",
      "episode: 360/10000, steps: 414, epsilon: 0.698248920785887\n",
      " loss : [13.295706]\n",
      "episode: 361/10000, steps: 484, epsilon: 0.6975506718651011\n",
      " loss : [10.427553]\n",
      "episode: 362/10000, steps: 634, epsilon: 0.6968531211932361\n",
      " loss : [9.755413]\n",
      "episode: 363/10000, steps: 485, epsilon: 0.6961562680720428\n",
      " loss : [7.746761]\n",
      "episode: 364/10000, steps: 592, epsilon: 0.6954601118039707\n",
      " loss : [5.868184]\n",
      "episode: 365/10000, steps: 646, epsilon: 0.6947646516921667\n",
      " loss : [11.920936]\n",
      "episode: 366/10000, steps: 791, epsilon: 0.6940698870404746\n",
      " loss : [12.791284]\n",
      "episode: 367/10000, steps: 968, epsilon: 0.6933758171534341\n",
      " loss : [13.413201]\n",
      "episode: 368/10000, steps: 823, epsilon: 0.6926824413362807\n",
      " loss : [325.06638]\n",
      "episode: 369/10000, steps: 801, epsilon: 0.6919897588949444\n",
      " loss : [10.620487]\n",
      "episode: 370/10000, steps: 566, epsilon: 0.6912977691360495\n",
      " loss : [11.527849]\n",
      "episode: 371/10000, steps: 757, epsilon: 0.6906064713669134\n",
      " loss : [306.03076]\n",
      "episode: 372/10000, steps: 892, epsilon: 0.6899158648955466\n",
      " loss : [228.0027]\n",
      "episode: 373/10000, steps: 1187, epsilon: 0.689225949030651\n",
      " loss : [53.12091]\n",
      "episode: 374/10000, steps: 408, epsilon: 0.6885367230816204\n",
      " loss : [29.831285]\n",
      "episode: 375/10000, steps: 731, epsilon: 0.6878481863585387\n",
      " loss : [37.567104]\n",
      "episode: 376/10000, steps: 633, epsilon: 0.6871603381721801\n",
      " loss : [14.25177]\n",
      "episode: 377/10000, steps: 337, epsilon: 0.686473177834008\n",
      " loss : [12.0885315]\n",
      "episode: 378/10000, steps: 1038, epsilon: 0.685786704656174\n",
      " loss : [300.99524]\n",
      "episode: 379/10000, steps: 967, epsilon: 0.6851009179515178\n",
      " loss : [7.6365633]\n",
      "episode: 380/10000, steps: 397, epsilon: 0.6844158170335664\n",
      " loss : [19.952374]\n",
      "episode: 381/10000, steps: 479, epsilon: 0.6837314012165328\n",
      " loss : [7.06534]\n",
      "episode: 382/10000, steps: 398, epsilon: 0.6830476698153162\n",
      " loss : [12.166064]\n",
      "episode: 383/10000, steps: 903, epsilon: 0.6823646221455009\n",
      " loss : [9.500653]\n",
      "episode: 384/10000, steps: 503, epsilon: 0.6816822575233553\n",
      " loss : [11.15013]\n",
      "episode: 385/10000, steps: 867, epsilon: 0.6810005752658319\n",
      " loss : [10.556646]\n",
      "episode: 386/10000, steps: 1208, epsilon: 0.680319574690566\n",
      " loss : [351.15103]\n",
      "episode: 387/10000, steps: 379, epsilon: 0.6796392551158754\n",
      " loss : [23.8447]\n",
      "episode: 388/10000, steps: 562, epsilon: 0.6789596158607596\n",
      " loss : [355.4804]\n",
      "episode: 389/10000, steps: 355, epsilon: 0.6782806562448989\n",
      " loss : [23.478529]\n",
      "episode: 390/10000, steps: 651, epsilon: 0.677602375588654\n",
      " loss : [328.3209]\n",
      "episode: 391/10000, steps: 653, epsilon: 0.6769247732130653\n",
      " loss : [23.017164]\n",
      "episode: 392/10000, steps: 793, epsilon: 0.6762478484398523\n",
      " loss : [17.57098]\n",
      "episode: 393/10000, steps: 426, epsilon: 0.6755716005914124\n",
      " loss : [14.258021]\n",
      "episode: 394/10000, steps: 576, epsilon: 0.674896028990821\n",
      " loss : [7.4163175]\n",
      "episode: 395/10000, steps: 920, epsilon: 0.6742211329618302\n",
      " loss : [19.038523]\n",
      "episode: 396/10000, steps: 788, epsilon: 0.6735469118288684\n",
      " loss : [20.762014]\n",
      "episode: 397/10000, steps: 675, epsilon: 0.6728733649170395\n",
      " loss : [344.05267]\n",
      "episode: 398/10000, steps: 808, epsilon: 0.6722004915521225\n",
      " loss : [20.239292]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 399/10000, steps: 580, epsilon: 0.6715282910605703\n",
      " loss : [10.65386]\n",
      "episode: 400/10000, steps: 676, epsilon: 0.6708567627695098\n",
      " loss : [11.885257]\n",
      "episode: 401/10000, steps: 755, epsilon: 0.6701859060067403\n",
      " loss : [10.882977]\n",
      "episode: 402/10000, steps: 775, epsilon: 0.6695157201007336\n",
      " loss : [9.335434]\n",
      "episode: 403/10000, steps: 962, epsilon: 0.6688462043806328\n",
      " loss : [13.813673]\n",
      "episode: 404/10000, steps: 723, epsilon: 0.6681773581762521\n",
      " loss : [6.4608693]\n",
      "episode: 405/10000, steps: 919, epsilon: 0.6675091808180759\n",
      " loss : [353.25345]\n",
      "episode: 406/10000, steps: 506, epsilon: 0.6668416716372578\n",
      " loss : [5.695059]\n",
      "episode: 407/10000, steps: 833, epsilon: 0.6661748299656206\n",
      " loss : [12.34824]\n",
      "episode: 408/10000, steps: 796, epsilon: 0.6655086551356549\n",
      " loss : [5.810475]\n",
      "episode: 409/10000, steps: 809, epsilon: 0.6648431464805192\n",
      " loss : [10.610001]\n",
      "episode: 410/10000, steps: 646, epsilon: 0.6641783033340387\n",
      " loss : [7.414697]\n",
      "episode: 411/10000, steps: 513, epsilon: 0.6635141250307047\n",
      " loss : [14.522053]\n",
      "episode: 412/10000, steps: 678, epsilon: 0.662850610905674\n",
      " loss : [5.5672398]\n",
      "episode: 413/10000, steps: 667, epsilon: 0.6621877602947683\n",
      " loss : [10.309442]\n",
      "episode: 414/10000, steps: 993, epsilon: 0.6615255725344735\n",
      " loss : [5.0223136]\n",
      "episode: 415/10000, steps: 648, epsilon: 0.660864046961939\n",
      " loss : [6.9366655]\n",
      "episode: 416/10000, steps: 657, epsilon: 0.660203182914977\n",
      " loss : [12.309115]\n",
      "episode: 417/10000, steps: 765, epsilon: 0.6595429797320621\n",
      " loss : [6.5987477]\n",
      "episode: 418/10000, steps: 828, epsilon: 0.6588834367523301\n",
      " loss : [4.8211594]\n",
      "episode: 419/10000, steps: 789, epsilon: 0.6582245533155777\n",
      " loss : [8.44148]\n",
      "episode: 420/10000, steps: 839, epsilon: 0.6575663287622622\n",
      " loss : [9.95354]\n",
      "episode: 421/10000, steps: 641, epsilon: 0.6569087624334999\n",
      " loss : [5.7044287]\n",
      "episode: 422/10000, steps: 1404, epsilon: 0.6562518536710664\n",
      " loss : [4.778591]\n",
      "episode: 423/10000, steps: 842, epsilon: 0.6555956018173954\n",
      " loss : [8.128429]\n",
      "episode: 424/10000, steps: 873, epsilon: 0.654940006215578\n",
      " loss : [7.2233577]\n",
      "episode: 425/10000, steps: 794, epsilon: 0.6542850662093624\n",
      " loss : [3.5237706]\n",
      "episode: 426/10000, steps: 725, epsilon: 0.6536307811431531\n",
      " loss : [3.1016767]\n",
      "episode: 427/10000, steps: 839, epsilon: 0.65297715036201\n",
      " loss : [5.34877]\n",
      "episode: 428/10000, steps: 380, epsilon: 0.6523241732116479\n",
      " loss : [8.358764]\n",
      "episode: 429/10000, steps: 573, epsilon: 0.6516718490384363\n",
      " loss : [3.4801786]\n",
      "episode: 430/10000, steps: 822, epsilon: 0.6510201771893979\n",
      " loss : [3.5510402]\n",
      "episode: 431/10000, steps: 818, epsilon: 0.6503691570122084\n",
      " loss : [303.58438]\n",
      "episode: 432/10000, steps: 690, epsilon: 0.6497187878551962\n",
      " loss : [8.295881]\n",
      "episode: 433/10000, steps: 455, epsilon: 0.649069069067341\n",
      " loss : [9.435935]\n",
      "episode: 434/10000, steps: 737, epsilon: 0.6484199999982736\n",
      " loss : [5.8753567]\n",
      "episode: 435/10000, steps: 1671, epsilon: 0.6477715799982753\n",
      " loss : [20.264864]\n",
      "episode: 436/10000, steps: 882, epsilon: 0.647123808418277\n",
      " loss : [8.16789]\n",
      "episode: 437/10000, steps: 650, epsilon: 0.6464766846098587\n",
      " loss : [10.202951]\n",
      "episode: 438/10000, steps: 807, epsilon: 0.6458302079252489\n",
      " loss : [30.029335]\n",
      "episode: 439/10000, steps: 941, epsilon: 0.6451843777173236\n",
      " loss : [12.100153]\n",
      "episode: 440/10000, steps: 860, epsilon: 0.6445391933396063\n",
      " loss : [11.205002]\n",
      "episode: 441/10000, steps: 707, epsilon: 0.6438946541462667\n",
      " loss : [9.655775]\n",
      "episode: 442/10000, steps: 1161, epsilon: 0.6432507594921204\n",
      " loss : [15.780935]\n",
      "episode: 443/10000, steps: 876, epsilon: 0.6426075087326283\n",
      " loss : [349.30017]\n",
      "episode: 444/10000, steps: 668, epsilon: 0.6419649012238956\n",
      " loss : [315.98245]\n",
      "episode: 445/10000, steps: 367, epsilon: 0.6413229363226717\n",
      " loss : [21.935106]\n",
      "episode: 446/10000, steps: 542, epsilon: 0.640681613386349\n",
      " loss : [383.094]\n",
      "episode: 447/10000, steps: 722, epsilon: 0.6400409317729626\n",
      " loss : [218.7463]\n",
      "episode: 448/10000, steps: 1114, epsilon: 0.6394008908411897\n",
      " loss : [57.155792]\n",
      "episode: 449/10000, steps: 426, epsilon: 0.6387614899503485\n",
      " loss : [19.572657]\n",
      "episode: 450/10000, steps: 557, epsilon: 0.6381227284603982\n",
      " loss : [21.092459]\n",
      "episode: 451/10000, steps: 690, epsilon: 0.6374846057319378\n",
      " loss : [10.978623]\n",
      "episode: 452/10000, steps: 796, epsilon: 0.6368471211262058\n",
      " loss : [11.108779]\n",
      "episode: 453/10000, steps: 511, epsilon: 0.6362102740050796\n",
      " loss : [10.4519005]\n",
      "episode: 454/10000, steps: 805, epsilon: 0.6355740637310745\n",
      " loss : [11.554169]\n",
      "episode: 455/10000, steps: 666, epsilon: 0.6349384896673435\n",
      " loss : [7.643726]\n",
      "episode: 456/10000, steps: 858, epsilon: 0.6343035511776761\n",
      " loss : [5.7100153]\n",
      "episode: 457/10000, steps: 654, epsilon: 0.6336692476264985\n",
      " loss : [4.227214]\n",
      "episode: 458/10000, steps: 728, epsilon: 0.6330355783788719\n",
      " loss : [1.9586401]\n",
      "episode: 459/10000, steps: 856, epsilon: 0.632402542800493\n",
      " loss : [7.3939657]\n",
      "episode: 460/10000, steps: 821, epsilon: 0.6317701402576925\n",
      " loss : [3.7507446]\n",
      "episode: 461/10000, steps: 663, epsilon: 0.6311383701174348\n",
      " loss : [4.802319]\n",
      "episode: 462/10000, steps: 824, epsilon: 0.6305072317473174\n",
      " loss : [5.591749]\n",
      "episode: 463/10000, steps: 692, epsilon: 0.6298767245155701\n",
      " loss : [7.9289827]\n",
      "episode: 464/10000, steps: 784, epsilon: 0.6292468477910546\n",
      " loss : [2.8825738]\n",
      "episode: 465/10000, steps: 492, epsilon: 0.6286176009432635\n",
      " loss : [6.0122533]\n",
      "episode: 466/10000, steps: 623, epsilon: 0.6279889833423202\n",
      " loss : [5.086715]\n",
      "episode: 467/10000, steps: 1475, epsilon: 0.6273609943589779\n",
      " loss : [2.7050362]\n",
      "episode: 468/10000, steps: 810, epsilon: 0.6267336333646188\n",
      " loss : [10.520623]\n",
      "episode: 469/10000, steps: 1193, epsilon: 0.6261068997312542\n",
      " loss : [320.4544]\n",
      "episode: 470/10000, steps: 1247, epsilon: 0.6254807928315229\n",
      " loss : [19.8936]\n",
      "episode: 471/10000, steps: 724, epsilon: 0.6248553120386914\n",
      " loss : [253.21593]\n",
      "episode: 472/10000, steps: 509, epsilon: 0.6242304567266527\n",
      " loss : [13.475391]\n",
      "episode: 473/10000, steps: 613, epsilon: 0.623606226269926\n",
      " loss : [30.042942]\n",
      "episode: 474/10000, steps: 882, epsilon: 0.6229826200436561\n",
      " loss : [19.014385]\n",
      "episode: 475/10000, steps: 526, epsilon: 0.6223596374236124\n",
      " loss : [344.4737]\n",
      "episode: 476/10000, steps: 576, epsilon: 0.6217372777861888\n",
      " loss : [11.808687]\n",
      "episode: 477/10000, steps: 890, epsilon: 0.6211155405084027\n",
      " loss : [29.721804]\n",
      "episode: 478/10000, steps: 648, epsilon: 0.6204944249678943\n",
      " loss : [13.696371]\n",
      "episode: 479/10000, steps: 1135, epsilon: 0.6198739305429264\n",
      " loss : [5.482356]\n",
      "episode: 480/10000, steps: 670, epsilon: 0.6192540566123834\n",
      " loss : [17.102915]\n",
      "episode: 481/10000, steps: 765, epsilon: 0.6186348025557711\n",
      " loss : [3.4299636]\n",
      "episode: 482/10000, steps: 409, epsilon: 0.6180161677532153\n",
      " loss : [331.42838]\n",
      "episode: 483/10000, steps: 743, epsilon: 0.6173981515854621\n",
      " loss : [12.102476]\n",
      "episode: 484/10000, steps: 698, epsilon: 0.6167807534338766\n",
      " loss : [298.23022]\n",
      "episode: 485/10000, steps: 647, epsilon: 0.6161639726804428\n",
      " loss : [16.831133]\n",
      "episode: 486/10000, steps: 381, epsilon: 0.6155478087077623\n",
      " loss : [10.825586]\n",
      "episode: 487/10000, steps: 737, epsilon: 0.6149322608990545\n",
      " loss : [6.526532]\n",
      "episode: 488/10000, steps: 952, epsilon: 0.6143173286381555\n",
      " loss : [293.90494]\n",
      "episode: 489/10000, steps: 485, epsilon: 0.6137030113095173\n",
      " loss : [41.56019]\n",
      "episode: 490/10000, steps: 488, epsilon: 0.6130893082982078\n",
      " loss : [606.74097]\n",
      "episode: 491/10000, steps: 767, epsilon: 0.6124762189899097\n",
      " loss : [44.90832]\n",
      "episode: 492/10000, steps: 543, epsilon: 0.6118637427709198\n",
      " loss : [27.915476]\n",
      "episode: 493/10000, steps: 658, epsilon: 0.6112518790281489\n",
      " loss : [7.807105]\n",
      "episode: 494/10000, steps: 484, epsilon: 0.6106406271491208\n",
      " loss : [22.323355]\n",
      "episode: 495/10000, steps: 823, epsilon: 0.6100299865219717\n",
      " loss : [4.928642]\n",
      "episode: 496/10000, steps: 403, epsilon: 0.6094199565354498\n",
      " loss : [270.74512]\n",
      "episode: 497/10000, steps: 642, epsilon: 0.6088105365789144\n",
      " loss : [7.8026]\n",
      "episode: 498/10000, steps: 446, epsilon: 0.6082017260423355\n",
      " loss : [40.215504]\n",
      "episode: 499/10000, steps: 666, epsilon: 0.6075935243162931\n",
      " loss : [329.44116]\n",
      "episode: 500/10000, steps: 669, epsilon: 0.6069859307919768\n",
      " loss : [276.6228]\n",
      "episode: 501/10000, steps: 853, epsilon: 0.6063789448611848\n",
      " loss : [21.101158]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 502/10000, steps: 651, epsilon: 0.6057725659163237\n",
      " loss : [20.762182]\n",
      "episode: 503/10000, steps: 622, epsilon: 0.6051667933504074\n",
      " loss : [27.543438]\n",
      "episode: 504/10000, steps: 868, epsilon: 0.6045616265570569\n",
      " loss : [23.569103]\n",
      "episode: 505/10000, steps: 890, epsilon: 0.6039570649304998\n",
      " loss : [8.840931]\n",
      "episode: 506/10000, steps: 808, epsilon: 0.6033531078655693\n",
      " loss : [10.18484]\n",
      "episode: 507/10000, steps: 839, epsilon: 0.6027497547577038\n",
      " loss : [15.215061]\n",
      "episode: 508/10000, steps: 735, epsilon: 0.602147005002946\n",
      " loss : [397.75272]\n",
      "episode: 509/10000, steps: 519, epsilon: 0.6015448579979431\n",
      " loss : [25.18781]\n",
      "episode: 510/10000, steps: 614, epsilon: 0.6009433131399452\n",
      " loss : [15.330053]\n",
      "episode: 511/10000, steps: 643, epsilon: 0.6003423698268052\n",
      " loss : [9.338488]\n",
      "episode: 512/10000, steps: 740, epsilon: 0.5997420274569785\n",
      " loss : [11.831255]\n",
      "episode: 513/10000, steps: 367, epsilon: 0.5991422854295215\n",
      " loss : [16.508245]\n",
      "episode: 514/10000, steps: 783, epsilon: 0.598543143144092\n",
      " loss : [9.210077]\n",
      "episode: 515/10000, steps: 691, epsilon: 0.5979446000009478\n",
      " loss : [9.64115]\n",
      "episode: 516/10000, steps: 768, epsilon: 0.5973466554009469\n",
      " loss : [5.21146]\n",
      "episode: 517/10000, steps: 470, epsilon: 0.596749308745546\n",
      " loss : [7.223281]\n",
      "episode: 518/10000, steps: 670, epsilon: 0.5961525594368005\n",
      " loss : [5.3674183]\n",
      "episode: 519/10000, steps: 393, epsilon: 0.5955564068773637\n",
      " loss : [5.9853177]\n",
      "episode: 520/10000, steps: 380, epsilon: 0.5949608504704863\n",
      " loss : [15.584972]\n",
      "episode: 521/10000, steps: 434, epsilon: 0.5943658896200158\n",
      " loss : [4.131166]\n",
      "episode: 522/10000, steps: 559, epsilon: 0.5937715237303958\n",
      " loss : [6.0195403]\n",
      "episode: 523/10000, steps: 532, epsilon: 0.5931777522066654\n",
      " loss : [3.69338]\n",
      "episode: 524/10000, steps: 613, epsilon: 0.5925845744544587\n",
      " loss : [342.07825]\n",
      "episode: 525/10000, steps: 482, epsilon: 0.5919919898800042\n",
      " loss : [7.1288323]\n",
      "episode: 526/10000, steps: 355, epsilon: 0.5913999978901242\n",
      " loss : [9.387432]\n",
      "episode: 527/10000, steps: 439, epsilon: 0.590808597892234\n",
      " loss : [5.619181]\n",
      "episode: 528/10000, steps: 366, epsilon: 0.5902177892943418\n",
      " loss : [6.060371]\n",
      "episode: 529/10000, steps: 380, epsilon: 0.5896275715050474\n",
      " loss : [270.09183]\n",
      "episode: 530/10000, steps: 1116, epsilon: 0.5890379439335424\n",
      " loss : [9.211098]\n",
      "episode: 531/10000, steps: 932, epsilon: 0.5884489059896089\n",
      " loss : [9.775783]\n",
      "episode: 532/10000, steps: 561, epsilon: 0.5878604570836192\n",
      " loss : [17.575226]\n",
      "episode: 533/10000, steps: 553, epsilon: 0.5872725966265356\n",
      " loss : [372.03577]\n",
      "episode: 534/10000, steps: 689, epsilon: 0.5866853240299091\n",
      " loss : [36.24063]\n",
      "episode: 535/10000, steps: 644, epsilon: 0.5860986387058792\n",
      " loss : [11.083432]\n",
      "episode: 536/10000, steps: 729, epsilon: 0.5855125400671733\n",
      " loss : [11.1349125]\n",
      "episode: 537/10000, steps: 682, epsilon: 0.5849270275271061\n",
      " loss : [350.68964]\n",
      "episode: 538/10000, steps: 343, epsilon: 0.584342100499579\n",
      " loss : [10.267975]\n",
      "episode: 539/10000, steps: 619, epsilon: 0.5837577583990794\n",
      " loss : [7.5924015]\n",
      "episode: 540/10000, steps: 1078, epsilon: 0.5831740006406804\n",
      " loss : [7.823665]\n",
      "episode: 541/10000, steps: 627, epsilon: 0.5825908266400397\n",
      " loss : [6.0642047]\n",
      "episode: 542/10000, steps: 541, epsilon: 0.5820082358133997\n",
      " loss : [8.409048]\n",
      "episode: 543/10000, steps: 643, epsilon: 0.5814262275775862\n",
      " loss : [7.0685983]\n",
      "episode: 544/10000, steps: 307, epsilon: 0.5808448013500086\n",
      " loss : [10.301727]\n",
      "episode: 545/10000, steps: 526, epsilon: 0.5802639565486586\n",
      " loss : [7.1626406]\n",
      "episode: 546/10000, steps: 498, epsilon: 0.57968369259211\n",
      " loss : [2.5852208]\n",
      "episode: 547/10000, steps: 664, epsilon: 0.5791040088995179\n",
      " loss : [355.97763]\n",
      "episode: 548/10000, steps: 579, epsilon: 0.5785249048906184\n",
      " loss : [11.622025]\n",
      "episode: 549/10000, steps: 768, epsilon: 0.5779463799857277\n",
      " loss : [6.5390596]\n",
      "episode: 550/10000, steps: 795, epsilon: 0.577368433605742\n",
      " loss : [305.08307]\n",
      "episode: 551/10000, steps: 817, epsilon: 0.5767910651721362\n",
      " loss : [19.434795]\n",
      "episode: 552/10000, steps: 979, epsilon: 0.576214274106964\n",
      " loss : [30.595112]\n",
      "episode: 553/10000, steps: 648, epsilon: 0.5756380598328571\n",
      " loss : [12.7915325]\n",
      "episode: 554/10000, steps: 704, epsilon: 0.5750624217730242\n",
      " loss : [4.4386053]\n",
      "episode: 555/10000, steps: 922, epsilon: 0.5744873593512512\n",
      " loss : [13.108898]\n",
      "episode: 556/10000, steps: 569, epsilon: 0.5739128719918999\n",
      " loss : [343.57996]\n",
      "episode: 557/10000, steps: 649, epsilon: 0.573338959119908\n",
      " loss : [7.6348042]\n",
      "episode: 558/10000, steps: 557, epsilon: 0.572765620160788\n",
      " loss : [6.007027]\n",
      "episode: 559/10000, steps: 729, epsilon: 0.5721928545406272\n",
      " loss : [13.928312]\n",
      "episode: 560/10000, steps: 742, epsilon: 0.5716206616860866\n",
      " loss : [10.123242]\n",
      "episode: 561/10000, steps: 702, epsilon: 0.5710490410244006\n",
      " loss : [9.484067]\n",
      "episode: 562/10000, steps: 775, epsilon: 0.5704779919833761\n",
      " loss : [26.597446]\n",
      "episode: 563/10000, steps: 1285, epsilon: 0.5699075139913927\n",
      " loss : [10.737562]\n",
      "episode: 564/10000, steps: 657, epsilon: 0.5693376064774013\n",
      " loss : [8.782132]\n",
      "episode: 565/10000, steps: 913, epsilon: 0.5687682688709239\n",
      " loss : [5.379622]\n",
      "episode: 566/10000, steps: 828, epsilon: 0.5681995006020529\n",
      " loss : [19.43544]\n",
      "episode: 567/10000, steps: 760, epsilon: 0.5676313011014509\n",
      " loss : [34.38499]\n",
      "episode: 568/10000, steps: 655, epsilon: 0.5670636698003494\n",
      " loss : [19.22668]\n",
      "episode: 569/10000, steps: 855, epsilon: 0.5664966061305491\n",
      " loss : [10.083234]\n",
      "episode: 570/10000, steps: 802, epsilon: 0.5659301095244186\n",
      " loss : [9.472121]\n",
      "episode: 571/10000, steps: 858, epsilon: 0.5653641794148941\n",
      " loss : [11.0756645]\n",
      "episode: 572/10000, steps: 855, epsilon: 0.5647988152354793\n",
      " loss : [8.536578]\n",
      "episode: 573/10000, steps: 800, epsilon: 0.5642340164202437\n",
      " loss : [15.950874]\n",
      "episode: 574/10000, steps: 815, epsilon: 0.5636697824038235\n",
      " loss : [13.619017]\n",
      "episode: 575/10000, steps: 788, epsilon: 0.5631061126214196\n",
      " loss : [316.42776]\n",
      "episode: 576/10000, steps: 795, epsilon: 0.5625430065087982\n",
      " loss : [14.800748]\n",
      "episode: 577/10000, steps: 775, epsilon: 0.5619804635022894\n",
      " loss : [11.230451]\n",
      "episode: 578/10000, steps: 650, epsilon: 0.561418483038787\n",
      " loss : [9.550541]\n",
      "episode: 579/10000, steps: 819, epsilon: 0.5608570645557482\n",
      " loss : [11.388642]\n",
      "episode: 580/10000, steps: 841, epsilon: 0.5602962074911925\n",
      " loss : [12.184164]\n",
      "episode: 581/10000, steps: 851, epsilon: 0.5597359112837013\n",
      " loss : [16.697725]\n",
      "episode: 582/10000, steps: 782, epsilon: 0.5591761753724176\n",
      " loss : [22.48099]\n",
      "episode: 583/10000, steps: 1382, epsilon: 0.5586169991970452\n",
      " loss : [343.30017]\n",
      "episode: 584/10000, steps: 773, epsilon: 0.5580583821978482\n",
      " loss : [9.289801]\n",
      "episode: 585/10000, steps: 667, epsilon: 0.5575003238156504\n",
      " loss : [21.959982]\n",
      "episode: 586/10000, steps: 695, epsilon: 0.5569428234918348\n",
      " loss : [12.542931]\n",
      "episode: 587/10000, steps: 781, epsilon: 0.5563858806683429\n",
      " loss : [10.665298]\n",
      "episode: 588/10000, steps: 831, epsilon: 0.5558294947876746\n",
      " loss : [4.8557644]\n",
      "episode: 589/10000, steps: 806, epsilon: 0.5552736652928869\n",
      " loss : [964.077]\n",
      "episode: 590/10000, steps: 646, epsilon: 0.5547183916275941\n",
      " loss : [309.91275]\n",
      "episode: 591/10000, steps: 648, epsilon: 0.5541636732359665\n",
      " loss : [28.94333]\n",
      "episode: 592/10000, steps: 688, epsilon: 0.5536095095627305\n",
      " loss : [12.601796]\n",
      "episode: 593/10000, steps: 1136, epsilon: 0.5530559000531677\n",
      " loss : [17.18299]\n",
      "episode: 594/10000, steps: 632, epsilon: 0.5525028441531146\n",
      " loss : [14.08763]\n",
      "episode: 595/10000, steps: 626, epsilon: 0.5519503413089615\n",
      " loss : [2.5333555]\n",
      "episode: 596/10000, steps: 454, epsilon: 0.5513983909676525\n",
      " loss : [288.86835]\n",
      "episode: 597/10000, steps: 702, epsilon: 0.5508469925766849\n",
      " loss : [17.953012]\n",
      "episode: 598/10000, steps: 616, epsilon: 0.5502961455841082\n",
      " loss : [310.37686]\n",
      "episode: 599/10000, steps: 1064, epsilon: 0.5497458494385241\n",
      " loss : [436.82297]\n",
      "episode: 600/10000, steps: 620, epsilon: 0.5491961035890855\n",
      " loss : [45.14465]\n",
      "episode: 601/10000, steps: 939, epsilon: 0.5486469074854965\n",
      " loss : [14.894006]\n",
      "episode: 602/10000, steps: 602, epsilon: 0.548098260578011\n",
      " loss : [10.649796]\n",
      "episode: 603/10000, steps: 495, epsilon: 0.547550162317433\n",
      " loss : [7.4566894]\n",
      "episode: 604/10000, steps: 939, epsilon: 0.5470026121551156\n",
      " loss : [28.424692]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 605/10000, steps: 1191, epsilon: 0.5464556095429605\n",
      " loss : [11.448648]\n",
      "episode: 606/10000, steps: 403, epsilon: 0.5459091539334175\n",
      " loss : [352.34488]\n",
      "episode: 607/10000, steps: 505, epsilon: 0.5453632447794842\n",
      " loss : [21.319815]\n",
      "episode: 608/10000, steps: 365, epsilon: 0.5448178815347047\n",
      " loss : [16.423155]\n",
      "episode: 609/10000, steps: 630, epsilon: 0.54427306365317\n",
      " loss : [21.605484]\n",
      "episode: 610/10000, steps: 1122, epsilon: 0.5437287905895168\n",
      " loss : [13.688934]\n",
      "episode: 611/10000, steps: 640, epsilon: 0.5431850617989273\n",
      " loss : [9.027692]\n",
      "episode: 612/10000, steps: 679, epsilon: 0.5426418767371284\n",
      " loss : [7.8997245]\n",
      "episode: 613/10000, steps: 900, epsilon: 0.5420992348603912\n",
      " loss : [8.22858]\n",
      "episode: 614/10000, steps: 966, epsilon: 0.5415571356255309\n",
      " loss : [19.601566]\n",
      "episode: 615/10000, steps: 646, epsilon: 0.5410155784899053\n",
      " loss : [13.733649]\n",
      "episode: 616/10000, steps: 318, epsilon: 0.5404745629114154\n",
      " loss : [9.608287]\n",
      "episode: 617/10000, steps: 328, epsilon: 0.539934088348504\n",
      " loss : [18.456945]\n",
      "episode: 618/10000, steps: 970, epsilon: 0.5393941542601555\n",
      " loss : [9.501101]\n",
      "episode: 619/10000, steps: 475, epsilon: 0.5388547601058953\n",
      " loss : [6.2783594]\n",
      "episode: 620/10000, steps: 1194, epsilon: 0.5383159053457894\n",
      " loss : [10.017854]\n",
      "episode: 621/10000, steps: 641, epsilon: 0.5377775894404436\n",
      " loss : [8.191026]\n",
      "episode: 622/10000, steps: 1124, epsilon: 0.5372398118510032\n",
      " loss : [17.4994]\n",
      "episode: 623/10000, steps: 619, epsilon: 0.5367025720391522\n",
      " loss : [9.680032]\n",
      "episode: 624/10000, steps: 1038, epsilon: 0.536165869467113\n",
      " loss : [361.72437]\n",
      "episode: 625/10000, steps: 1180, epsilon: 0.5356297035976458\n",
      " loss : [20.79393]\n",
      "episode: 626/10000, steps: 672, epsilon: 0.5350940738940482\n",
      " loss : [10.482291]\n",
      "episode: 627/10000, steps: 406, epsilon: 0.5345589798201541\n",
      " loss : [16.047543]\n",
      "episode: 628/10000, steps: 562, epsilon: 0.534024420840334\n",
      " loss : [12.9087715]\n",
      "episode: 629/10000, steps: 993, epsilon: 0.5334903964194936\n",
      " loss : [11.761391]\n",
      "episode: 630/10000, steps: 501, epsilon: 0.5329569060230741\n",
      " loss : [7.478754]\n",
      "episode: 631/10000, steps: 925, epsilon: 0.532423949117051\n",
      " loss : [19.520203]\n",
      "episode: 632/10000, steps: 636, epsilon: 0.531891525167934\n",
      " loss : [5.011583]\n",
      "episode: 633/10000, steps: 2163, epsilon: 0.5313596336427661\n",
      " loss : [339.87152]\n",
      "episode: 634/10000, steps: 816, epsilon: 0.5308282740091232\n",
      " loss : [10.280327]\n",
      "episode: 635/10000, steps: 837, epsilon: 0.5302974457351142\n",
      " loss : [17.478453]\n",
      "episode: 636/10000, steps: 975, epsilon: 0.5297671482893791\n",
      " loss : [13.72766]\n",
      "episode: 637/10000, steps: 643, epsilon: 0.5292373811410898\n",
      " loss : [19.2477]\n",
      "episode: 638/10000, steps: 807, epsilon: 0.5287081437599487\n",
      " loss : [17.099957]\n",
      "episode: 639/10000, steps: 812, epsilon: 0.5281794356161887\n",
      " loss : [12.747576]\n",
      "episode: 640/10000, steps: 790, epsilon: 0.5276512561805725\n",
      " loss : [13.894226]\n",
      "episode: 641/10000, steps: 795, epsilon: 0.5271236049243919\n",
      " loss : [5.5786085]\n",
      "episode: 642/10000, steps: 398, epsilon: 0.5265964813194676\n",
      " loss : [11.332425]\n",
      "episode: 643/10000, steps: 682, epsilon: 0.5260698848381481\n",
      " loss : [8.761626]\n",
      "episode: 644/10000, steps: 647, epsilon: 0.5255438149533099\n",
      " loss : [10.644849]\n",
      "episode: 645/10000, steps: 719, epsilon: 0.5250182711383566\n",
      " loss : [12.910772]\n",
      "episode: 646/10000, steps: 783, epsilon: 0.5244932528672183\n",
      " loss : [6.7712374]\n",
      "episode: 647/10000, steps: 760, epsilon: 0.5239687596143511\n",
      " loss : [5.3570323]\n",
      "episode: 648/10000, steps: 644, epsilon: 0.5234447908547367\n",
      " loss : [7.0367475]\n",
      "episode: 649/10000, steps: 773, epsilon: 0.522921346063882\n",
      " loss : [7.2828183]\n",
      "episode: 650/10000, steps: 731, epsilon: 0.522398424717818\n",
      " loss : [10.345248]\n",
      "episode: 651/10000, steps: 1265, epsilon: 0.5218760262931003\n",
      " loss : [674.8626]\n",
      "episode: 652/10000, steps: 934, epsilon: 0.5213541502668072\n",
      " loss : [21.538576]\n",
      "episode: 653/10000, steps: 777, epsilon: 0.5208327961165404\n",
      " loss : [80.364624]\n",
      "episode: 654/10000, steps: 913, epsilon: 0.5203119633204238\n",
      " loss : [7.8752513]\n",
      "episode: 655/10000, steps: 715, epsilon: 0.5197916513571034\n",
      " loss : [653.7709]\n",
      "episode: 656/10000, steps: 810, epsilon: 0.5192718597057463\n",
      " loss : [22.995064]\n",
      "episode: 657/10000, steps: 594, epsilon: 0.5187525878460405\n",
      " loss : [50.90109]\n",
      "episode: 658/10000, steps: 971, epsilon: 0.5182338352581944\n",
      " loss : [384.99857]\n",
      "episode: 659/10000, steps: 488, epsilon: 0.5177156014229363\n",
      " loss : [8.928971]\n",
      "episode: 660/10000, steps: 648, epsilon: 0.5171978858215134\n",
      " loss : [9.652298]\n",
      "episode: 661/10000, steps: 514, epsilon: 0.5166806879356919\n",
      " loss : [11.007626]\n",
      "episode: 662/10000, steps: 538, epsilon: 0.5161640072477562\n",
      " loss : [15.587957]\n",
      "episode: 663/10000, steps: 660, epsilon: 0.5156478432405085\n",
      " loss : [17.802612]\n",
      "episode: 664/10000, steps: 370, epsilon: 0.515132195397268\n",
      " loss : [348.416]\n",
      "episode: 665/10000, steps: 480, epsilon: 0.5146170632018707\n",
      " loss : [20.940266]\n",
      "episode: 666/10000, steps: 599, epsilon: 0.5141024461386688\n",
      " loss : [10.339929]\n",
      "episode: 667/10000, steps: 764, epsilon: 0.5135883436925301\n",
      " loss : [3.8326402]\n",
      "episode: 668/10000, steps: 941, epsilon: 0.5130747553488376\n",
      " loss : [11.834114]\n",
      "episode: 669/10000, steps: 405, epsilon: 0.5125616805934888\n",
      " loss : [333.90408]\n",
      "episode: 670/10000, steps: 639, epsilon: 0.5120491189128954\n",
      " loss : [13.532839]\n",
      "episode: 671/10000, steps: 665, epsilon: 0.5115370697939825\n",
      " loss : [307.40988]\n",
      "episode: 672/10000, steps: 950, epsilon: 0.5110255327241885\n",
      " loss : [28.54163]\n",
      "episode: 673/10000, steps: 799, epsilon: 0.5105145071914643\n",
      " loss : [8.875284]\n",
      "episode: 674/10000, steps: 726, epsilon: 0.5100039926842729\n",
      " loss : [19.604282]\n",
      "episode: 675/10000, steps: 1366, epsilon: 0.5094939886915886\n",
      " loss : [324.30344]\n",
      "episode: 676/10000, steps: 700, epsilon: 0.508984494702897\n",
      " loss : [28.012966]\n",
      "episode: 677/10000, steps: 952, epsilon: 0.508475510208194\n",
      " loss : [310.52448]\n",
      "episode: 678/10000, steps: 603, epsilon: 0.5079670346979859\n",
      " loss : [314.5814]\n",
      "episode: 679/10000, steps: 516, epsilon: 0.5074590676632879\n",
      " loss : [18.087599]\n",
      "episode: 680/10000, steps: 713, epsilon: 0.5069516085956246\n",
      " loss : [14.049984]\n",
      "episode: 681/10000, steps: 521, epsilon: 0.506444656987029\n",
      " loss : [25.897583]\n",
      "episode: 682/10000, steps: 455, epsilon: 0.505938212330042\n",
      " loss : [20.961216]\n",
      "episode: 683/10000, steps: 971, epsilon: 0.505432274117712\n",
      " loss : [18.55137]\n",
      "episode: 684/10000, steps: 788, epsilon: 0.5049268418435943\n",
      " loss : [11.742103]\n",
      "episode: 685/10000, steps: 877, epsilon: 0.5044219150017507\n",
      " loss : [18.487226]\n",
      "episode: 686/10000, steps: 1432, epsilon: 0.5039174930867489\n",
      " loss : [13.821351]\n",
      "episode: 687/10000, steps: 995, epsilon: 0.5034135755936622\n",
      " loss : [11.687795]\n",
      "episode: 688/10000, steps: 665, epsilon: 0.5029101620180685\n",
      " loss : [8.088648]\n",
      "episode: 689/10000, steps: 833, epsilon: 0.5024072518560504\n",
      " loss : [8.910574]\n",
      "episode: 690/10000, steps: 970, epsilon: 0.5019048446041944\n",
      " loss : [10.22137]\n",
      "episode: 691/10000, steps: 373, epsilon: 0.5014029397595902\n",
      " loss : [7.625996]\n",
      "episode: 692/10000, steps: 636, epsilon: 0.5009015368198305\n",
      " loss : [8.714823]\n",
      "episode: 693/10000, steps: 859, epsilon: 0.5004006352830107\n",
      " loss : [7.0213294]\n",
      "episode: 694/10000, steps: 1096, epsilon: 0.4999002346477277\n",
      " loss : [6.957836]\n",
      "episode: 695/10000, steps: 663, epsilon: 0.49940033441307996\n",
      " loss : [309.62573]\n",
      "episode: 696/10000, steps: 1015, epsilon: 0.49890093407866687\n",
      " loss : [617.4638]\n",
      "episode: 697/10000, steps: 823, epsilon: 0.4984020331445882\n",
      " loss : [340.0576]\n",
      "episode: 698/10000, steps: 806, epsilon: 0.4979036311114436\n",
      " loss : [15.405743]\n",
      "episode: 699/10000, steps: 784, epsilon: 0.4974057274803321\n",
      " loss : [238.6248]\n",
      "episode: 700/10000, steps: 639, epsilon: 0.49690832175285177\n",
      " loss : [25.870754]\n",
      "episode: 701/10000, steps: 959, epsilon: 0.4964114134310989\n",
      " loss : [26.603329]\n",
      "episode: 702/10000, steps: 841, epsilon: 0.4959150020176678\n",
      " loss : [10.247066]\n",
      "episode: 703/10000, steps: 832, epsilon: 0.49541908701565013\n",
      " loss : [16.186184]\n",
      "episode: 704/10000, steps: 804, epsilon: 0.49492366792863446\n",
      " loss : [350.5433]\n",
      "episode: 705/10000, steps: 814, epsilon: 0.4944287442607058\n",
      " loss : [15.933278]\n",
      "episode: 706/10000, steps: 638, epsilon: 0.4939343155164451\n",
      " loss : [16.444674]\n",
      "episode: 707/10000, steps: 644, epsilon: 0.4934403812009287\n",
      " loss : [5.100848]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 708/10000, steps: 649, epsilon: 0.4929469408197278\n",
      " loss : [333.39835]\n",
      "episode: 709/10000, steps: 833, epsilon: 0.49245399387890804\n",
      " loss : [7.5943446]\n",
      "episode: 710/10000, steps: 966, epsilon: 0.4919615398850291\n",
      " loss : [6.2653856]\n",
      "episode: 711/10000, steps: 830, epsilon: 0.4914695783451441\n",
      " loss : [3.9962382]\n",
      "episode: 712/10000, steps: 864, epsilon: 0.4909781087667989\n",
      " loss : [5.656607]\n",
      "episode: 713/10000, steps: 827, epsilon: 0.4904871306580321\n",
      " loss : [7.5688057]\n",
      "episode: 714/10000, steps: 804, epsilon: 0.4899966435273741\n",
      " loss : [7.344096]\n",
      "episode: 715/10000, steps: 776, epsilon: 0.4895066468838467\n",
      " loss : [6.8100204]\n",
      "episode: 716/10000, steps: 772, epsilon: 0.48901714023696286\n",
      " loss : [7.6716433]\n",
      "episode: 717/10000, steps: 781, epsilon: 0.4885281230967259\n",
      " loss : [5.3294706]\n",
      "episode: 718/10000, steps: 888, epsilon: 0.48803959497362914\n",
      " loss : [3.400878]\n",
      "episode: 719/10000, steps: 843, epsilon: 0.4875515553786555\n",
      " loss : [7.6847486]\n",
      "episode: 720/10000, steps: 779, epsilon: 0.48706400382327686\n",
      " loss : [4.0558114]\n",
      "episode: 721/10000, steps: 790, epsilon: 0.4865769398194536\n",
      " loss : [9.15842]\n",
      "episode: 722/10000, steps: 638, epsilon: 0.48609036287963414\n",
      " loss : [4.591202]\n",
      "episode: 723/10000, steps: 831, epsilon: 0.48560427251675453\n",
      " loss : [7.5139365]\n",
      "episode: 724/10000, steps: 643, epsilon: 0.48511866824423777\n",
      " loss : [4.1346807]\n",
      "episode: 725/10000, steps: 346, epsilon: 0.4846335495759935\n",
      " loss : [9.390952]\n",
      "episode: 726/10000, steps: 639, epsilon: 0.4841489160264175\n",
      " loss : [8.355089]\n",
      "episode: 727/10000, steps: 384, epsilon: 0.4836647671103911\n",
      " loss : [3.886348]\n",
      "episode: 728/10000, steps: 652, epsilon: 0.4831811023432807\n",
      " loss : [6.8581533]\n",
      "episode: 729/10000, steps: 772, epsilon: 0.4826979212409374\n",
      " loss : [3.8041975]\n",
      "episode: 730/10000, steps: 957, epsilon: 0.48221522331969646\n",
      " loss : [6.0203123]\n",
      "episode: 731/10000, steps: 756, epsilon: 0.48173300809637676\n",
      " loss : [4.086136]\n",
      "episode: 732/10000, steps: 665, epsilon: 0.48125127508828036\n",
      " loss : [3.4678128]\n",
      "episode: 733/10000, steps: 662, epsilon: 0.48077002381319206\n",
      " loss : [7.399947]\n",
      "episode: 734/10000, steps: 808, epsilon: 0.48028925378937887\n",
      " loss : [5.027752]\n",
      "episode: 735/10000, steps: 796, epsilon: 0.47980896453558947\n",
      " loss : [3.6581714]\n",
      "episode: 736/10000, steps: 824, epsilon: 0.4793291555710539\n",
      " loss : [2.1008918]\n",
      "episode: 737/10000, steps: 654, epsilon: 0.47884982641548285\n",
      " loss : [2.2444613]\n",
      "episode: 738/10000, steps: 752, epsilon: 0.47837097658906735\n",
      " loss : [4.3694854]\n",
      "episode: 739/10000, steps: 940, epsilon: 0.47789260561247826\n",
      " loss : [4.3765635]\n",
      "episode: 740/10000, steps: 604, epsilon: 0.4774147130068658\n",
      " loss : [3.5169559]\n",
      "episode: 741/10000, steps: 682, epsilon: 0.4769372982938589\n",
      " loss : [3.8031774]\n",
      "episode: 742/10000, steps: 847, epsilon: 0.47646036099556505\n",
      " loss : [3.586898]\n",
      "episode: 743/10000, steps: 793, epsilon: 0.47598390063456947\n",
      " loss : [4.2702446]\n",
      "episode: 744/10000, steps: 799, epsilon: 0.4755079167339349\n",
      " loss : [3.208103]\n",
      "episode: 745/10000, steps: 829, epsilon: 0.47503240881720094\n",
      " loss : [4.026681]\n",
      "episode: 746/10000, steps: 650, epsilon: 0.47455737640838375\n",
      " loss : [6.7370315]\n",
      "episode: 747/10000, steps: 811, epsilon: 0.47408281903197536\n",
      " loss : [7.6893716]\n",
      "episode: 748/10000, steps: 662, epsilon: 0.47360873621294336\n",
      " loss : [6.6482797]\n",
      "episode: 749/10000, steps: 1115, epsilon: 0.4731351274767304\n",
      " loss : [3.153388]\n",
      "episode: 750/10000, steps: 836, epsilon: 0.4726619923492537\n",
      " loss : [1.8197194]\n",
      "episode: 751/10000, steps: 821, epsilon: 0.47218933035690447\n",
      " loss : [12.111398]\n",
      "episode: 752/10000, steps: 779, epsilon: 0.47171714102654755\n",
      " loss : [7.0323014]\n",
      "episode: 753/10000, steps: 839, epsilon: 0.471245423885521\n",
      " loss : [7.2654276]\n",
      "episode: 754/10000, steps: 1085, epsilon: 0.47077417846163544\n",
      " loss : [17.718605]\n",
      "episode: 755/10000, steps: 752, epsilon: 0.4703034042831738\n",
      " loss : [266.58493]\n",
      "episode: 756/10000, steps: 1604, epsilon: 0.46983310087889063\n",
      " loss : [312.13522]\n",
      "episode: 757/10000, steps: 882, epsilon: 0.46936326777801174\n",
      " loss : [46.320515]\n",
      "episode: 758/10000, steps: 1100, epsilon: 0.4688939045102337\n",
      " loss : [22.062834]\n",
      "episode: 759/10000, steps: 545, epsilon: 0.4684250106057235\n",
      " loss : [301.6685]\n",
      "episode: 760/10000, steps: 709, epsilon: 0.46795658559511777\n",
      " loss : [18.530851]\n",
      "episode: 761/10000, steps: 819, epsilon: 0.46748862900952265\n",
      " loss : [5.582811]\n",
      "episode: 762/10000, steps: 656, epsilon: 0.4670211403805131\n",
      " loss : [12.798994]\n",
      "episode: 763/10000, steps: 830, epsilon: 0.4665541192401326\n",
      " loss : [6.100705]\n",
      "episode: 764/10000, steps: 985, epsilon: 0.4660875651208925\n",
      " loss : [17.80845]\n",
      "episode: 765/10000, steps: 844, epsilon: 0.4656214775557716\n",
      " loss : [347.94885]\n",
      "episode: 766/10000, steps: 580, epsilon: 0.46515585607821586\n",
      " loss : [9.333159]\n",
      "episode: 767/10000, steps: 702, epsilon: 0.46469070022213765\n",
      " loss : [8.77081]\n",
      "episode: 768/10000, steps: 695, epsilon: 0.4642260095219155\n",
      " loss : [5.012431]\n",
      "episode: 769/10000, steps: 612, epsilon: 0.4637617835123936\n",
      " loss : [8.760151]\n",
      "episode: 770/10000, steps: 527, epsilon: 0.46329802172888124\n",
      " loss : [7.587189]\n",
      "episode: 771/10000, steps: 845, epsilon: 0.46283472370715234\n",
      " loss : [13.952774]\n",
      "episode: 772/10000, steps: 873, epsilon: 0.46237188898344517\n",
      " loss : [7.306016]\n",
      "episode: 773/10000, steps: 743, epsilon: 0.4619095170944617\n",
      " loss : [235.09674]\n",
      "episode: 774/10000, steps: 824, epsilon: 0.46144760757736725\n",
      " loss : [36.889305]\n",
      "episode: 775/10000, steps: 744, epsilon: 0.46098615996978987\n",
      " loss : [81.52156]\n",
      "episode: 776/10000, steps: 801, epsilon: 0.46052517380982005\n",
      " loss : [26.800537]\n",
      "episode: 777/10000, steps: 804, epsilon: 0.4600646486360102\n",
      " loss : [5.814103]\n",
      "episode: 778/10000, steps: 837, epsilon: 0.4596045839873742\n",
      " loss : [10.4052305]\n",
      "episode: 779/10000, steps: 812, epsilon: 0.45914497940338683\n",
      " loss : [12.674318]\n",
      "episode: 780/10000, steps: 990, epsilon: 0.4586858344239834\n",
      " loss : [5.75727]\n",
      "episode: 781/10000, steps: 644, epsilon: 0.45822714858955943\n",
      " loss : [4.7286906]\n",
      "episode: 782/10000, steps: 968, epsilon: 0.45776892144096987\n",
      " loss : [2.9724805]\n",
      "episode: 783/10000, steps: 858, epsilon: 0.4573111525195289\n",
      " loss : [2.637986]\n",
      "episode: 784/10000, steps: 936, epsilon: 0.4568538413670094\n",
      " loss : [11.236281]\n",
      "episode: 785/10000, steps: 996, epsilon: 0.4563969875256424\n",
      " loss : [4.652893]\n",
      "episode: 786/10000, steps: 682, epsilon: 0.45594059053811675\n",
      " loss : [6.1970935]\n",
      "episode: 787/10000, steps: 844, epsilon: 0.45548464994757865\n",
      " loss : [5.3983006]\n",
      "episode: 788/10000, steps: 818, epsilon: 0.45502916529763104\n",
      " loss : [7.929607]\n",
      "episode: 789/10000, steps: 845, epsilon: 0.45457413613233344\n",
      " loss : [4.945041]\n",
      "episode: 790/10000, steps: 936, epsilon: 0.4541195619962011\n",
      " loss : [4.3700542]\n",
      "episode: 791/10000, steps: 670, epsilon: 0.4536654424342049\n",
      " loss : [2.074085]\n",
      "episode: 792/10000, steps: 797, epsilon: 0.45321177699177073\n",
      " loss : [3.53364]\n",
      "episode: 793/10000, steps: 847, epsilon: 0.452758565214779\n",
      " loss : [7.051624]\n",
      "episode: 794/10000, steps: 873, epsilon: 0.4523058066495642\n",
      " loss : [291.8408]\n",
      "episode: 795/10000, steps: 810, epsilon: 0.45185350084291465\n",
      " loss : [14.110127]\n",
      "episode: 796/10000, steps: 645, epsilon: 0.4514016473420717\n",
      " loss : [10.181998]\n",
      "episode: 797/10000, steps: 806, epsilon: 0.45095024569472963\n",
      " loss : [241.5763]\n",
      "episode: 798/10000, steps: 667, epsilon: 0.4504992954490349\n",
      " loss : [16.34076]\n",
      "episode: 799/10000, steps: 648, epsilon: 0.45004879615358584\n",
      " loss : [19.125713]\n",
      "episode: 800/10000, steps: 690, epsilon: 0.44959874735743227\n",
      " loss : [6.64823]\n",
      "episode: 801/10000, steps: 821, epsilon: 0.4491491486100748\n",
      " loss : [5.0932074]\n",
      "episode: 802/10000, steps: 898, epsilon: 0.44869999946146477\n",
      " loss : [5.5841885]\n",
      "episode: 803/10000, steps: 788, epsilon: 0.4482512994620033\n",
      " loss : [3.5489943]\n",
      "episode: 804/10000, steps: 811, epsilon: 0.4478030481625413\n",
      " loss : [5.3355145]\n",
      "episode: 805/10000, steps: 832, epsilon: 0.44735524511437874\n",
      " loss : [2.3130133]\n",
      "episode: 806/10000, steps: 1909, epsilon: 0.4469078898692644\n",
      " loss : [4.784465]\n",
      "episode: 807/10000, steps: 864, epsilon: 0.4464609819793951\n",
      " loss : [3.9444273]\n",
      "episode: 808/10000, steps: 854, epsilon: 0.44601452099741573\n",
      " loss : [6.1522264]\n",
      "episode: 809/10000, steps: 797, epsilon: 0.4455685064764183\n",
      " loss : [11.6780615]\n",
      "episode: 810/10000, steps: 605, epsilon: 0.4451229379699419\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " loss : [339.77985]\n",
      "episode: 811/10000, steps: 673, epsilon: 0.44467781503197196\n",
      " loss : [15.44129]\n",
      "episode: 812/10000, steps: 654, epsilon: 0.44423313721693997\n",
      " loss : [7.290557]\n",
      "episode: 813/10000, steps: 1124, epsilon: 0.44378890407972305\n",
      " loss : [285.44354]\n",
      "episode: 814/10000, steps: 851, epsilon: 0.44334511517564335\n",
      " loss : [223.08075]\n",
      "episode: 815/10000, steps: 494, epsilon: 0.4429017700604677\n",
      " loss : [31.465508]\n",
      "episode: 816/10000, steps: 546, epsilon: 0.44245886829040726\n",
      " loss : [16.808292]\n",
      "episode: 817/10000, steps: 878, epsilon: 0.44201640942211684\n",
      " loss : [21.39233]\n",
      "episode: 818/10000, steps: 489, epsilon: 0.44157439301269474\n",
      " loss : [16.732914]\n",
      "episode: 819/10000, steps: 670, epsilon: 0.44113281861968207\n",
      " loss : [54.560585]\n",
      "episode: 820/10000, steps: 648, epsilon: 0.4406916858010624\n",
      " loss : [16.630707]\n",
      "episode: 821/10000, steps: 455, epsilon: 0.4402509941152613\n",
      " loss : [26.692577]\n",
      "episode: 822/10000, steps: 1117, epsilon: 0.43981074312114604\n",
      " loss : [370.77823]\n",
      "episode: 823/10000, steps: 1171, epsilon: 0.4393709323780249\n",
      " loss : [22.46807]\n",
      "episode: 824/10000, steps: 515, epsilon: 0.4389315614456469\n",
      " loss : [12.780755]\n",
      "episode: 825/10000, steps: 813, epsilon: 0.43849262988420123\n",
      " loss : [14.080676]\n",
      "episode: 826/10000, steps: 1142, epsilon: 0.438054137254317\n",
      " loss : [10.661089]\n",
      "episode: 827/10000, steps: 908, epsilon: 0.4376160831170627\n",
      " loss : [15.426477]\n",
      "episode: 828/10000, steps: 730, epsilon: 0.43717846703394564\n",
      " loss : [12.18538]\n",
      "episode: 829/10000, steps: 1426, epsilon: 0.4367412885669117\n",
      " loss : [14.075253]\n",
      "episode: 830/10000, steps: 1002, epsilon: 0.4363045472783448\n",
      " loss : [8.394114]\n",
      "episode: 831/10000, steps: 395, epsilon: 0.43586824273106645\n",
      " loss : [9.7794]\n",
      "episode: 832/10000, steps: 748, epsilon: 0.4354323744883354\n",
      " loss : [13.971691]\n",
      "episode: 833/10000, steps: 1232, epsilon: 0.43499694211384704\n",
      " loss : [10.963592]\n",
      "episode: 834/10000, steps: 1671, epsilon: 0.4345619451717332\n",
      " loss : [11.135249]\n",
      "episode: 835/10000, steps: 1140, epsilon: 0.43412738322656147\n",
      " loss : [15.82776]\n",
      "episode: 836/10000, steps: 840, epsilon: 0.4336932558433349\n",
      " loss : [11.156816]\n",
      "episode: 837/10000, steps: 660, epsilon: 0.43325956258749154\n",
      " loss : [6.904507]\n",
      "episode: 838/10000, steps: 820, epsilon: 0.43282630302490405\n",
      " loss : [8.955314]\n",
      "episode: 839/10000, steps: 807, epsilon: 0.43239347672187917\n",
      " loss : [343.38406]\n",
      "episode: 840/10000, steps: 1398, epsilon: 0.4319610832451573\n",
      " loss : [269.86853]\n",
      "episode: 841/10000, steps: 1122, epsilon: 0.43152912216191214\n",
      " loss : [57.928772]\n",
      "episode: 842/10000, steps: 1069, epsilon: 0.4310975930397502\n",
      " loss : [18.640663]\n",
      "episode: 843/10000, steps: 635, epsilon: 0.43066649544671043\n",
      " loss : [210.57097]\n",
      "episode: 844/10000, steps: 1086, epsilon: 0.4302358289512637\n",
      " loss : [155.82738]\n",
      "episode: 845/10000, steps: 831, epsilon: 0.4298055931223125\n",
      " loss : [98.36987]\n",
      "episode: 846/10000, steps: 939, epsilon: 0.42937578752919014\n",
      " loss : [30.46664]\n",
      "episode: 847/10000, steps: 534, epsilon: 0.4289464117416609\n",
      " loss : [19.212791]\n",
      "episode: 848/10000, steps: 409, epsilon: 0.42851746532991924\n",
      " loss : [13.222477]\n",
      "episode: 849/10000, steps: 767, epsilon: 0.42808894786458934\n",
      " loss : [21.320839]\n",
      "episode: 850/10000, steps: 351, epsilon: 0.42766085891672473\n",
      " loss : [16.876621]\n",
      "episode: 851/10000, steps: 1109, epsilon: 0.427233198057808\n",
      " loss : [17.319145]\n",
      "episode: 852/10000, steps: 1133, epsilon: 0.4268059648597502\n",
      " loss : [7.1641335]\n",
      "episode: 853/10000, steps: 1147, epsilon: 0.4263791588948904\n",
      " loss : [12.055917]\n",
      "episode: 854/10000, steps: 517, epsilon: 0.42595277973599555\n",
      " loss : [359.1097]\n",
      "episode: 855/10000, steps: 496, epsilon: 0.42552682695625954\n",
      " loss : [13.954263]\n",
      "episode: 856/10000, steps: 649, epsilon: 0.4251013001293033\n",
      " loss : [10.237846]\n",
      "episode: 857/10000, steps: 564, epsilon: 0.424676198829174\n",
      " loss : [12.715413]\n",
      "episode: 858/10000, steps: 1122, epsilon: 0.4242515226303448\n",
      " loss : [10.001598]\n",
      "episode: 859/10000, steps: 1100, epsilon: 0.42382727110771445\n",
      " loss : [360.57193]\n",
      "episode: 860/10000, steps: 559, epsilon: 0.4234034438366067\n",
      " loss : [10.697979]\n",
      "episode: 861/10000, steps: 859, epsilon: 0.4229800403927701\n",
      " loss : [5.9973316]\n",
      "episode: 862/10000, steps: 581, epsilon: 0.42255706035237733\n",
      " loss : [9.695159]\n",
      "episode: 863/10000, steps: 1123, epsilon: 0.42213450329202495\n",
      " loss : [9.651611]\n",
      "episode: 864/10000, steps: 707, epsilon: 0.4217123687887329\n",
      " loss : [9.619341]\n",
      "episode: 865/10000, steps: 598, epsilon: 0.4212906564199442\n",
      " loss : [7.902527]\n",
      "episode: 866/10000, steps: 687, epsilon: 0.42086936576352424\n",
      " loss : [13.546811]\n",
      "episode: 867/10000, steps: 603, epsilon: 0.4204484963977607\n",
      " loss : [20.981918]\n",
      "episode: 868/10000, steps: 403, epsilon: 0.42002804790136294\n",
      " loss : [7.1325207]\n",
      "episode: 869/10000, steps: 998, epsilon: 0.4196080198534616\n",
      " loss : [7.3067284]\n",
      "episode: 870/10000, steps: 662, epsilon: 0.4191884118336081\n",
      " loss : [11.787047]\n",
      "episode: 871/10000, steps: 653, epsilon: 0.41876922342177453\n",
      " loss : [9.908916]\n",
      "episode: 872/10000, steps: 639, epsilon: 0.41835045419835276\n",
      " loss : [26.64003]\n",
      "episode: 873/10000, steps: 654, epsilon: 0.4179321037441544\n",
      " loss : [28.706873]\n",
      "episode: 874/10000, steps: 1004, epsilon: 0.41751417164041027\n",
      " loss : [6.40785]\n",
      "episode: 875/10000, steps: 503, epsilon: 0.41709665746876984\n",
      " loss : [8.102482]\n",
      "episode: 876/10000, steps: 789, epsilon: 0.4166795608113011\n",
      " loss : [12.86521]\n",
      "episode: 877/10000, steps: 817, epsilon: 0.41626288125048977\n",
      " loss : [6.3870153]\n",
      "episode: 878/10000, steps: 537, epsilon: 0.41584661836923925\n",
      " loss : [14.103535]\n",
      "episode: 879/10000, steps: 643, epsilon: 0.41543077175087\n",
      " loss : [11.821038]\n",
      "episode: 880/10000, steps: 965, epsilon: 0.41501534097911913\n",
      " loss : [5.920139]\n",
      "episode: 881/10000, steps: 629, epsilon: 0.41460032563814003\n",
      " loss : [10.863493]\n",
      "episode: 882/10000, steps: 830, epsilon: 0.4141857253125019\n",
      " loss : [8.316851]\n",
      "episode: 883/10000, steps: 645, epsilon: 0.41377153958718943\n",
      " loss : [8.500811]\n",
      "episode: 884/10000, steps: 789, epsilon: 0.4133577680476022\n",
      " loss : [9.957867]\n",
      "episode: 885/10000, steps: 656, epsilon: 0.4129444102795546\n",
      " loss : [7.5819864]\n",
      "episode: 886/10000, steps: 811, epsilon: 0.412531465869275\n",
      " loss : [6.062171]\n",
      "episode: 887/10000, steps: 883, epsilon: 0.41211893440340575\n",
      " loss : [7.5048046]\n",
      "episode: 888/10000, steps: 798, epsilon: 0.41170681546900234\n",
      " loss : [9.07674]\n",
      "episode: 889/10000, steps: 832, epsilon: 0.41129510865353336\n",
      " loss : [8.21512]\n",
      "episode: 890/10000, steps: 827, epsilon: 0.41088381354487985\n",
      " loss : [323.46823]\n",
      "episode: 891/10000, steps: 829, epsilon: 0.410472929731335\n",
      " loss : [7.3786836]\n",
      "episode: 892/10000, steps: 785, epsilon: 0.41006245680160364\n",
      " loss : [7.281678]\n",
      "episode: 893/10000, steps: 822, epsilon: 0.40965239434480205\n",
      " loss : [11.20094]\n",
      "episode: 894/10000, steps: 1111, epsilon: 0.40924274195045723\n",
      " loss : [291.95267]\n",
      "episode: 895/10000, steps: 590, epsilon: 0.40883349920850676\n",
      " loss : [12.162935]\n",
      "episode: 896/10000, steps: 485, epsilon: 0.40842466570929825\n",
      " loss : [237.96997]\n",
      "episode: 897/10000, steps: 659, epsilon: 0.40801624104358897\n",
      " loss : [197.44933]\n",
      "episode: 898/10000, steps: 466, epsilon: 0.4076082248025454\n",
      " loss : [42.79892]\n",
      "episode: 899/10000, steps: 700, epsilon: 0.4072006165777428\n",
      " loss : [209.06406]\n",
      "episode: 900/10000, steps: 750, epsilon: 0.4067934159611651\n",
      " loss : [37.678493]\n",
      "episode: 901/10000, steps: 794, epsilon: 0.4063866225452039\n",
      " loss : [24.593798]\n",
      "episode: 902/10000, steps: 790, epsilon: 0.4059802359226587\n",
      " loss : [39.825806]\n",
      "episode: 903/10000, steps: 386, epsilon: 0.40557425568673605\n",
      " loss : [11.749351]\n",
      "episode: 904/10000, steps: 441, epsilon: 0.40516868143104934\n",
      " loss : [13.895878]\n",
      "episode: 905/10000, steps: 819, epsilon: 0.4047635127496183\n",
      " loss : [3.6351826]\n",
      "episode: 906/10000, steps: 910, epsilon: 0.4043587492368687\n",
      " loss : [11.776287]\n",
      "episode: 907/10000, steps: 702, epsilon: 0.4039543904876318\n",
      " loss : [6.536558]\n",
      "episode: 908/10000, steps: 612, epsilon: 0.4035504360971442\n",
      " loss : [4.895936]\n",
      "episode: 909/10000, steps: 597, epsilon: 0.40314688566104706\n",
      " loss : [322.50317]\n",
      "episode: 910/10000, steps: 835, epsilon: 0.402743738775386\n",
      " loss : [6.5184083]\n",
      "episode: 911/10000, steps: 1272, epsilon: 0.4023409950366106\n",
      " loss : [17.66258]\n",
      "episode: 912/10000, steps: 607, epsilon: 0.401938654041574\n",
      " loss : [18.011648]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 913/10000, steps: 633, epsilon: 0.4015367153875324\n",
      " loss : [7.1697507]\n",
      "episode: 914/10000, steps: 545, epsilon: 0.4011351786721449\n",
      " loss : [12.221874]\n",
      "episode: 915/10000, steps: 936, epsilon: 0.40073404349347275\n",
      " loss : [5.10757]\n",
      "episode: 916/10000, steps: 835, epsilon: 0.40033330944997925\n",
      " loss : [10.810204]\n",
      "episode: 917/10000, steps: 1414, epsilon: 0.39993297614052925\n",
      " loss : [8.212806]\n",
      "episode: 918/10000, steps: 783, epsilon: 0.3995330431643887\n",
      " loss : [19.058588]\n",
      "episode: 919/10000, steps: 667, epsilon: 0.39913351012122433\n",
      " loss : [15.5748]\n",
      "episode: 920/10000, steps: 860, epsilon: 0.3987343766111031\n",
      " loss : [7.2167435]\n",
      "episode: 921/10000, steps: 806, epsilon: 0.398335642234492\n",
      " loss : [13.480102]\n",
      "episode: 922/10000, steps: 801, epsilon: 0.3979373065922575\n",
      " loss : [7.6646843]\n",
      "episode: 923/10000, steps: 887, epsilon: 0.39753936928566525\n",
      " loss : [14.515573]\n",
      "episode: 924/10000, steps: 637, epsilon: 0.3971418299163796\n",
      " loss : [365.15045]\n",
      "episode: 925/10000, steps: 845, epsilon: 0.3967446880864632\n",
      " loss : [9.44711]\n",
      "episode: 926/10000, steps: 646, epsilon: 0.3963479433983767\n",
      " loss : [3.8637652]\n",
      "episode: 927/10000, steps: 794, epsilon: 0.3959515954549783\n",
      " loss : [11.100477]\n",
      "episode: 928/10000, steps: 802, epsilon: 0.39555564385952335\n",
      " loss : [10.84478]\n",
      "episode: 929/10000, steps: 374, epsilon: 0.39516008821566384\n",
      " loss : [12.234939]\n",
      "episode: 930/10000, steps: 686, epsilon: 0.3947649281274482\n",
      " loss : [4.9329653]\n",
      "episode: 931/10000, steps: 813, epsilon: 0.3943701631993207\n",
      " loss : [10.758801]\n",
      "episode: 932/10000, steps: 1360, epsilon: 0.3939757930361214\n",
      " loss : [7.4626675]\n",
      "episode: 933/10000, steps: 778, epsilon: 0.3935818172430853\n",
      " loss : [13.797831]\n",
      "episode: 934/10000, steps: 849, epsilon: 0.3931882354258422\n",
      " loss : [5.7767954]\n",
      "episode: 935/10000, steps: 846, epsilon: 0.39279504719041636\n",
      " loss : [6.087354]\n",
      "episode: 936/10000, steps: 642, epsilon: 0.39240225214322594\n",
      " loss : [6.7646303]\n",
      "episode: 937/10000, steps: 798, epsilon: 0.39200984989108273\n",
      " loss : [8.525731]\n",
      "episode: 938/10000, steps: 874, epsilon: 0.39161784004119166\n",
      " loss : [13.415884]\n",
      "episode: 939/10000, steps: 822, epsilon: 0.39122622220115044\n",
      " loss : [2.2854095]\n",
      "episode: 940/10000, steps: 990, epsilon: 0.3908349959789493\n",
      " loss : [7.8280625]\n",
      "episode: 941/10000, steps: 650, epsilon: 0.3904441609829703\n",
      " loss : [4.157198]\n",
      "episode: 942/10000, steps: 857, epsilon: 0.3900537168219873\n",
      " loss : [587.6034]\n",
      "episode: 943/10000, steps: 787, epsilon: 0.3896636631051653\n",
      " loss : [12.571562]\n",
      "episode: 944/10000, steps: 1174, epsilon: 0.38927399944206015\n",
      " loss : [14.691896]\n",
      "episode: 945/10000, steps: 811, epsilon: 0.3888847254426181\n",
      " loss : [19.28199]\n",
      "episode: 946/10000, steps: 803, epsilon: 0.38849584071717547\n",
      " loss : [207.40634]\n",
      "episode: 947/10000, steps: 811, epsilon: 0.3881073448764583\n",
      " loss : [10.576256]\n",
      "episode: 948/10000, steps: 960, epsilon: 0.3877192375315819\n",
      " loss : [13.450467]\n",
      "episode: 949/10000, steps: 793, epsilon: 0.3873315182940503\n",
      " loss : [4.82891]\n",
      "episode: 950/10000, steps: 848, epsilon: 0.38694418677575626\n",
      " loss : [5.181838]\n",
      "episode: 951/10000, steps: 961, epsilon: 0.3865572425889805\n",
      " loss : [6.521121]\n",
      "episode: 952/10000, steps: 1088, epsilon: 0.38617068534639154\n",
      " loss : [8.7361145]\n",
      "episode: 953/10000, steps: 840, epsilon: 0.38578451466104513\n",
      " loss : [8.04172]\n",
      "episode: 954/10000, steps: 854, epsilon: 0.3853987301463841\n",
      " loss : [2.3574562]\n",
      "episode: 955/10000, steps: 1384, epsilon: 0.3850133314162377\n",
      " loss : [3.9032397]\n",
      "episode: 956/10000, steps: 859, epsilon: 0.3846283180848215\n",
      " loss : [3.104098]\n",
      "episode: 957/10000, steps: 856, epsilon: 0.3842436897667367\n",
      " loss : [4.8440924]\n",
      "episode: 958/10000, steps: 842, epsilon: 0.3838594460769699\n",
      " loss : [2.1252737]\n",
      "episode: 959/10000, steps: 857, epsilon: 0.38347558663089293\n",
      " loss : [296.74115]\n",
      "episode: 960/10000, steps: 989, epsilon: 0.38309211104426205\n",
      " loss : [16.336319]\n",
      "episode: 961/10000, steps: 1451, epsilon: 0.3827090189332178\n",
      " loss : [14.58369]\n",
      "episode: 962/10000, steps: 1122, epsilon: 0.38232630991428457\n",
      " loss : [9.663722]\n",
      "episode: 963/10000, steps: 1149, epsilon: 0.38194398360437026\n",
      " loss : [6.2236257]\n",
      "episode: 964/10000, steps: 1185, epsilon: 0.3815620396207659\n",
      " loss : [17.911066]\n",
      "episode: 965/10000, steps: 524, epsilon: 0.38118047758114515\n",
      " loss : [19.97883]\n",
      "episode: 966/10000, steps: 813, epsilon: 0.380799297103564\n",
      " loss : [357.0966]\n",
      "episode: 967/10000, steps: 662, epsilon: 0.3804184978064605\n",
      " loss : [621.29877]\n",
      "episode: 968/10000, steps: 1121, epsilon: 0.380038079308654\n",
      " loss : [18.794155]\n",
      "episode: 969/10000, steps: 688, epsilon: 0.3796580412293454\n",
      " loss : [46.966328]\n",
      "episode: 970/10000, steps: 670, epsilon: 0.379278383188116\n",
      " loss : [11.09151]\n",
      "episode: 971/10000, steps: 751, epsilon: 0.3788991048049279\n",
      " loss : [323.40646]\n",
      "episode: 972/10000, steps: 844, epsilon: 0.37852020570012296\n",
      " loss : [13.214163]\n",
      "episode: 973/10000, steps: 884, epsilon: 0.37814168549442284\n",
      " loss : [17.714954]\n",
      "episode: 974/10000, steps: 835, epsilon: 0.3777635438089284\n",
      " loss : [10.65038]\n",
      "episode: 975/10000, steps: 643, epsilon: 0.3773857802651195\n",
      " loss : [7.9024253]\n",
      "episode: 976/10000, steps: 732, epsilon: 0.37700839448485435\n",
      " loss : [281.23648]\n",
      "episode: 977/10000, steps: 411, epsilon: 0.3766313860903695\n",
      " loss : [327.13297]\n",
      "episode: 978/10000, steps: 617, epsilon: 0.37625475470427916\n",
      " loss : [6.9074006]\n",
      "episode: 979/10000, steps: 995, epsilon: 0.3758784999495749\n",
      " loss : [40.969288]\n",
      "episode: 980/10000, steps: 845, epsilon: 0.3755026214496253\n",
      " loss : [9.639797]\n",
      "episode: 981/10000, steps: 620, epsilon: 0.3751271188281757\n",
      " loss : [9.265322]\n",
      "episode: 982/10000, steps: 798, epsilon: 0.3747519917093475\n",
      " loss : [27.391068]\n",
      "episode: 983/10000, steps: 698, epsilon: 0.37437723971763814\n",
      " loss : [5.3967075]\n",
      "episode: 984/10000, steps: 901, epsilon: 0.37400286247792053\n",
      " loss : [244.95987]\n",
      "episode: 985/10000, steps: 650, epsilon: 0.3736288596154426\n",
      " loss : [6.9487076]\n",
      "episode: 986/10000, steps: 1209, epsilon: 0.37325523075582717\n",
      " loss : [18.607533]\n",
      "episode: 987/10000, steps: 847, epsilon: 0.37288197552507135\n",
      " loss : [15.868811]\n",
      "episode: 988/10000, steps: 1112, epsilon: 0.37250909354954626\n",
      " loss : [23.073965]\n",
      "episode: 989/10000, steps: 710, epsilon: 0.37213658445599673\n",
      " loss : [14.237764]\n",
      "episode: 990/10000, steps: 665, epsilon: 0.3717644478715407\n",
      " loss : [12.875584]\n",
      "episode: 991/10000, steps: 521, epsilon: 0.3713926834236692\n",
      " loss : [15.28393]\n",
      "episode: 992/10000, steps: 1310, epsilon: 0.37102129074024554\n",
      " loss : [15.064919]\n",
      "episode: 993/10000, steps: 1109, epsilon: 0.3706502694495053\n",
      " loss : [9.941614]\n",
      "episode: 994/10000, steps: 833, epsilon: 0.3702796191800558\n",
      " loss : [15.579395]\n",
      "episode: 995/10000, steps: 689, epsilon: 0.36990933956087574\n",
      " loss : [5.929882]\n",
      "episode: 996/10000, steps: 533, epsilon: 0.36953943022131486\n",
      " loss : [17.873562]\n",
      "episode: 997/10000, steps: 667, epsilon: 0.3691698907910935\n",
      " loss : [4.147396]\n",
      "episode: 998/10000, steps: 650, epsilon: 0.3688007209003024\n",
      " loss : [17.789938]\n",
      "episode: 999/10000, steps: 441, epsilon: 0.36843192017940213\n",
      " loss : [12.540634]\n",
      "episode: 1000/10000, steps: 1192, epsilon: 0.36806348825922275\n",
      " loss : [7.894292]\n",
      "episode: 1001/10000, steps: 462, epsilon: 0.3676954247709635\n",
      " loss : [7.536519]\n",
      "episode: 1002/10000, steps: 524, epsilon: 0.36732772934619257\n",
      " loss : [12.58964]\n",
      "episode: 1003/10000, steps: 570, epsilon: 0.3669604016168464\n",
      " loss : [14.317975]\n",
      "episode: 1004/10000, steps: 1136, epsilon: 0.3665934412152296\n",
      " loss : [16.217897]\n",
      "episode: 1005/10000, steps: 1241, epsilon: 0.3662268477740143\n",
      " loss : [298.48706]\n",
      "episode: 1006/10000, steps: 459, epsilon: 0.36586062092624033\n",
      " loss : [16.737972]\n",
      "episode: 1007/10000, steps: 653, epsilon: 0.3654947603053141\n",
      " loss : [14.288242]\n",
      "episode: 1008/10000, steps: 1125, epsilon: 0.36512926554500874\n",
      " loss : [10.874014]\n",
      "episode: 1009/10000, steps: 1071, epsilon: 0.36476413627946375\n",
      " loss : [16.4936]\n",
      "episode: 1010/10000, steps: 951, epsilon: 0.3643993721431843\n",
      " loss : [259.2148]\n",
      "episode: 1011/10000, steps: 552, epsilon: 0.36403497277104113\n",
      " loss : [149.17686]\n",
      "episode: 1012/10000, steps: 553, epsilon: 0.3636709377982701\n",
      " loss : [60.62635]\n",
      "episode: 1013/10000, steps: 878, epsilon: 0.3633072668604718\n",
      " loss : [24.637735]\n",
      "episode: 1014/10000, steps: 697, epsilon: 0.36294395959361136\n",
      " loss : [20.611542]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1015/10000, steps: 663, epsilon: 0.36258101563401773\n",
      " loss : [10.408445]\n",
      "episode: 1016/10000, steps: 393, epsilon: 0.3622184346183837\n",
      " loss : [386.97574]\n",
      "episode: 1017/10000, steps: 1228, epsilon: 0.36185621618376534\n",
      " loss : [362.91943]\n",
      "episode: 1018/10000, steps: 808, epsilon: 0.36149435996758156\n",
      " loss : [19.602533]\n",
      "episode: 1019/10000, steps: 786, epsilon: 0.361132865607614\n",
      " loss : [40.5557]\n",
      "episode: 1020/10000, steps: 887, epsilon: 0.36077173274200636\n",
      " loss : [6.8905582]\n",
      "episode: 1021/10000, steps: 828, epsilon: 0.36041096100926434\n",
      " loss : [11.624802]\n",
      "episode: 1022/10000, steps: 871, epsilon: 0.3600505500482551\n",
      " loss : [372.18304]\n",
      "episode: 1023/10000, steps: 817, epsilon: 0.35969049949820686\n",
      " loss : [3.4831176]\n",
      "episode: 1024/10000, steps: 828, epsilon: 0.3593308089987087\n",
      " loss : [337.10977]\n",
      "episode: 1025/10000, steps: 1150, epsilon: 0.35897147818971\n",
      " loss : [316.76413]\n",
      "episode: 1026/10000, steps: 1157, epsilon: 0.3586125067115203\n",
      " loss : [11.137022]\n",
      "episode: 1027/10000, steps: 657, epsilon: 0.35825389420480874\n",
      " loss : [8.385856]\n",
      "episode: 1028/10000, steps: 1207, epsilon: 0.35789564031060395\n",
      " loss : [353.17496]\n",
      "episode: 1029/10000, steps: 561, epsilon: 0.35753774467029337\n",
      " loss : [35.69847]\n",
      "episode: 1030/10000, steps: 1086, epsilon: 0.3571802069256231\n",
      " loss : [267.88068]\n",
      "episode: 1031/10000, steps: 547, epsilon: 0.3568230267186975\n",
      " loss : [268.54523]\n",
      "episode: 1032/10000, steps: 567, epsilon: 0.3564662036919788\n",
      " loss : [332.41104]\n",
      "episode: 1033/10000, steps: 736, epsilon: 0.35610973748828684\n",
      " loss : [29.281927]\n",
      "episode: 1034/10000, steps: 508, epsilon: 0.35575362775079855\n",
      " loss : [22.676014]\n",
      "episode: 1035/10000, steps: 414, epsilon: 0.35539787412304774\n",
      " loss : [326.11105]\n",
      "episode: 1036/10000, steps: 637, epsilon: 0.3550424762489247\n",
      " loss : [10.869304]\n",
      "episode: 1037/10000, steps: 1196, epsilon: 0.3546874337726758\n",
      " loss : [14.122012]\n",
      "episode: 1038/10000, steps: 912, epsilon: 0.35433274633890316\n",
      " loss : [19.7517]\n",
      "episode: 1039/10000, steps: 648, epsilon: 0.35397841359256427\n",
      " loss : [13.774147]\n",
      "episode: 1040/10000, steps: 509, epsilon: 0.3536244351789717\n",
      " loss : [8.126726]\n",
      "episode: 1041/10000, steps: 1124, epsilon: 0.35327081074379274\n",
      " loss : [17.75398]\n",
      "episode: 1042/10000, steps: 731, epsilon: 0.352917539933049\n",
      " loss : [4.182437]\n",
      "episode: 1043/10000, steps: 1108, epsilon: 0.3525646223931159\n",
      " loss : [12.441274]\n",
      "episode: 1044/10000, steps: 604, epsilon: 0.3522120577707228\n",
      " loss : [300.2882]\n",
      "episode: 1045/10000, steps: 625, epsilon: 0.3518598457129521\n",
      " loss : [4.536267]\n",
      "episode: 1046/10000, steps: 1092, epsilon: 0.35150798586723914\n",
      " loss : [10.173459]\n",
      "episode: 1047/10000, steps: 688, epsilon: 0.3511564778813719\n",
      " loss : [16.045023]\n",
      "episode: 1048/10000, steps: 651, epsilon: 0.3508053214034905\n",
      " loss : [8.201276]\n",
      "episode: 1049/10000, steps: 637, epsilon: 0.35045451608208705\n",
      " loss : [5.823108]\n",
      "episode: 1050/10000, steps: 1075, epsilon: 0.350104061566005\n",
      " loss : [6.473624]\n",
      "episode: 1051/10000, steps: 677, epsilon: 0.349753957504439\n",
      " loss : [9.157278]\n",
      "episode: 1052/10000, steps: 1093, epsilon: 0.3494042035469346\n",
      " loss : [13.591092]\n",
      "episode: 1053/10000, steps: 1103, epsilon: 0.3490547993433876\n",
      " loss : [316.357]\n",
      "episode: 1054/10000, steps: 523, epsilon: 0.34870574454404424\n",
      " loss : [16.163593]\n",
      "episode: 1055/10000, steps: 1132, epsilon: 0.3483570387995002\n",
      " loss : [14.365345]\n",
      "episode: 1056/10000, steps: 572, epsilon: 0.3480086817607007\n",
      " loss : [289.22867]\n",
      "episode: 1057/10000, steps: 525, epsilon: 0.34766067307894\n",
      " loss : [22.173851]\n",
      "episode: 1058/10000, steps: 1072, epsilon: 0.34731301240586104\n",
      " loss : [300.62326]\n",
      "episode: 1059/10000, steps: 670, epsilon: 0.3469656993934552\n",
      " loss : [33.461544]\n",
      "episode: 1060/10000, steps: 625, epsilon: 0.3466187336940617\n",
      " loss : [549.70154]\n",
      "episode: 1061/10000, steps: 668, epsilon: 0.34627211496036764\n",
      " loss : [43.685764]\n",
      "episode: 1062/10000, steps: 1088, epsilon: 0.3459258428454073\n",
      " loss : [374.0977]\n",
      "episode: 1063/10000, steps: 644, epsilon: 0.34557991700256185\n",
      " loss : [26.804949]\n",
      "episode: 1064/10000, steps: 1112, epsilon: 0.3452343370855593\n",
      " loss : [322.2579]\n",
      "episode: 1065/10000, steps: 497, epsilon: 0.34488910274847373\n",
      " loss : [266.60052]\n",
      "episode: 1066/10000, steps: 839, epsilon: 0.34454421364572524\n",
      " loss : [17.996479]\n",
      "episode: 1067/10000, steps: 811, epsilon: 0.3441996694320795\n",
      " loss : [32.971603]\n",
      "episode: 1068/10000, steps: 643, epsilon: 0.34385546976264747\n",
      " loss : [32.274025]\n",
      "episode: 1069/10000, steps: 691, epsilon: 0.34351161429288485\n",
      " loss : [298.19122]\n",
      "episode: 1070/10000, steps: 899, epsilon: 0.34316810267859194\n",
      " loss : [589.66046]\n",
      "episode: 1071/10000, steps: 812, epsilon: 0.34282493457591334\n",
      " loss : [295.86816]\n",
      "episode: 1072/10000, steps: 811, epsilon: 0.3424821096413374\n",
      " loss : [268.02652]\n",
      "episode: 1073/10000, steps: 869, epsilon: 0.3421396275316961\n",
      " loss : [55.684982]\n",
      "episode: 1074/10000, steps: 809, epsilon: 0.34179748790416437\n",
      " loss : [8.864395]\n",
      "episode: 1075/10000, steps: 529, epsilon: 0.3414556904162602\n",
      " loss : [13.083454]\n",
      "episode: 1076/10000, steps: 945, epsilon: 0.34111423472584396\n",
      " loss : [52.4626]\n",
      "episode: 1077/10000, steps: 689, epsilon: 0.34077312049111813\n",
      " loss : [11.567118]\n",
      "episode: 1078/10000, steps: 851, epsilon: 0.340432347370627\n",
      " loss : [43.347134]\n",
      "episode: 1079/10000, steps: 825, epsilon: 0.34009191502325636\n",
      " loss : [14.664886]\n",
      "episode: 1080/10000, steps: 815, epsilon: 0.3397518231082331\n",
      " loss : [12.300595]\n",
      "episode: 1081/10000, steps: 817, epsilon: 0.33941207128512485\n",
      " loss : [20.074347]\n",
      "episode: 1082/10000, steps: 709, epsilon: 0.3390726592138397\n",
      " loss : [14.292259]\n",
      "episode: 1083/10000, steps: 707, epsilon: 0.3387335865546259\n",
      " loss : [14.090445]\n",
      "episode: 1084/10000, steps: 830, epsilon: 0.33839485296807126\n",
      " loss : [11.358768]\n",
      "episode: 1085/10000, steps: 871, epsilon: 0.3380564581151032\n",
      " loss : [11.291385]\n",
      "episode: 1086/10000, steps: 840, epsilon: 0.33771840165698813\n",
      " loss : [11.565224]\n",
      "episode: 1087/10000, steps: 855, epsilon: 0.33738068325533116\n",
      " loss : [3.9995842]\n",
      "episode: 1088/10000, steps: 842, epsilon: 0.3370433025720758\n",
      " loss : [7.0492754]\n",
      "episode: 1089/10000, steps: 825, epsilon: 0.33670625926950376\n",
      " loss : [304.03317]\n",
      "episode: 1090/10000, steps: 830, epsilon: 0.33636955301023425\n",
      " loss : [5.6862006]\n",
      "episode: 1091/10000, steps: 651, epsilon: 0.336033183457224\n",
      " loss : [10.2682905]\n",
      "episode: 1092/10000, steps: 794, epsilon: 0.3356971502737668\n",
      " loss : [16.371912]\n",
      "episode: 1093/10000, steps: 691, epsilon: 0.335361453123493\n",
      " loss : [277.99768]\n",
      "episode: 1094/10000, steps: 386, epsilon: 0.3350260916703695\n",
      " loss : [23.222292]\n",
      "episode: 1095/10000, steps: 1337, epsilon: 0.33469106557869915\n",
      " loss : [8.173496]\n",
      "episode: 1096/10000, steps: 540, epsilon: 0.33435637451312045\n",
      " loss : [2.885886]\n",
      "episode: 1097/10000, steps: 370, epsilon: 0.3340220181386073\n",
      " loss : [8.86937]\n",
      "episode: 1098/10000, steps: 684, epsilon: 0.33368799612046873\n",
      " loss : [7.0209665]\n",
      "episode: 1099/10000, steps: 716, epsilon: 0.33335430812434824\n",
      " loss : [6.605501]\n",
      "episode: 1100/10000, steps: 839, epsilon: 0.3330209538162239\n",
      " loss : [7.73339]\n",
      "episode: 1101/10000, steps: 844, epsilon: 0.33268793286240766\n",
      " loss : [12.215505]\n",
      "episode: 1102/10000, steps: 718, epsilon: 0.33235524492954527\n",
      " loss : [341.5305]\n",
      "episode: 1103/10000, steps: 1525, epsilon: 0.33202288968461574\n",
      " loss : [12.543609]\n",
      "episode: 1104/10000, steps: 357, epsilon: 0.33169086679493115\n",
      " loss : [20.436573]\n",
      "episode: 1105/10000, steps: 554, epsilon: 0.33135917592813624\n",
      " loss : [350.47217]\n",
      "episode: 1106/10000, steps: 626, epsilon: 0.3310278167522081\n",
      " loss : [7.5470533]\n",
      "episode: 1107/10000, steps: 1129, epsilon: 0.3306967889354559\n",
      " loss : [9.643452]\n",
      "episode: 1108/10000, steps: 730, epsilon: 0.33036609214652046\n",
      " loss : [14.404145]\n",
      "episode: 1109/10000, steps: 625, epsilon: 0.3300357260543739\n",
      " loss : [330.08087]\n",
      "episode: 1110/10000, steps: 485, epsilon: 0.32970569032831953\n",
      " loss : [17.788935]\n",
      "episode: 1111/10000, steps: 799, epsilon: 0.3293759846379912\n",
      " loss : [5.4181495]\n",
      "episode: 1112/10000, steps: 691, epsilon: 0.3290466086533532\n",
      " loss : [4.255575]\n",
      "episode: 1113/10000, steps: 306, epsilon: 0.3287175620446999\n",
      " loss : [10.831139]\n",
      "episode: 1114/10000, steps: 606, epsilon: 0.32838884448265515\n",
      " loss : [9.625797]\n",
      "episode: 1115/10000, steps: 1023, epsilon: 0.3280604556381725\n",
      " loss : [16.200642]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1116/10000, steps: 603, epsilon: 0.32773239518253433\n",
      " loss : [10.945991]\n",
      "episode: 1117/10000, steps: 962, epsilon: 0.3274046627873518\n",
      " loss : [9.948686]\n",
      "episode: 1118/10000, steps: 975, epsilon: 0.32707725812456445\n",
      " loss : [14.012023]\n",
      "episode: 1119/10000, steps: 538, epsilon: 0.32675018086643987\n",
      " loss : [25.600288]\n",
      "episode: 1120/10000, steps: 518, epsilon: 0.32642343068557345\n",
      " loss : [26.257326]\n",
      "episode: 1121/10000, steps: 1006, epsilon: 0.3260970072548879\n",
      " loss : [247.49544]\n",
      "episode: 1122/10000, steps: 1466, epsilon: 0.325770910247633\n",
      " loss : [26.54144]\n",
      "episode: 1123/10000, steps: 658, epsilon: 0.32544513933738534\n",
      " loss : [11.8369]\n",
      "episode: 1124/10000, steps: 654, epsilon: 0.3251196941980479\n",
      " loss : [25.295036]\n",
      "episode: 1125/10000, steps: 718, epsilon: 0.32479457450384985\n",
      " loss : [11.087052]\n",
      "episode: 1126/10000, steps: 672, epsilon: 0.324469779929346\n",
      " loss : [13.124035]\n",
      "episode: 1127/10000, steps: 713, epsilon: 0.32414531014941667\n",
      " loss : [26.878668]\n",
      "episode: 1128/10000, steps: 1320, epsilon: 0.32382116483926726\n",
      " loss : [200.18785]\n",
      "episode: 1129/10000, steps: 656, epsilon: 0.323497343674428\n",
      " loss : [30.352716]\n",
      "episode: 1130/10000, steps: 807, epsilon: 0.3231738463307536\n",
      " loss : [34.162483]\n",
      "episode: 1131/10000, steps: 647, epsilon: 0.32285067248442284\n",
      " loss : [24.269548]\n",
      "episode: 1132/10000, steps: 709, epsilon: 0.3225278218119384\n",
      " loss : [7.0912776]\n",
      "episode: 1133/10000, steps: 785, epsilon: 0.3222052939901265\n",
      " loss : [8.669131]\n",
      "episode: 1134/10000, steps: 849, epsilon: 0.32188308869613635\n",
      " loss : [7.3503356]\n",
      "episode: 1135/10000, steps: 652, epsilon: 0.3215612056074402\n",
      " loss : [6.6949234]\n",
      "episode: 1136/10000, steps: 875, epsilon: 0.3212396444018328\n",
      " loss : [9.107359]\n",
      "episode: 1137/10000, steps: 991, epsilon: 0.32091840475743094\n",
      " loss : [224.63293]\n",
      "episode: 1138/10000, steps: 811, epsilon: 0.3205974863526735\n",
      " loss : [10.298476]\n",
      "episode: 1139/10000, steps: 995, epsilon: 0.32027688886632083\n",
      " loss : [7.050071]\n",
      "episode: 1140/10000, steps: 850, epsilon: 0.3199566119774545\n",
      " loss : [5.5560894]\n",
      "episode: 1141/10000, steps: 851, epsilon: 0.31963665536547703\n",
      " loss : [22.085611]\n",
      "episode: 1142/10000, steps: 818, epsilon: 0.31931701871011153\n",
      " loss : [6.972657]\n",
      "episode: 1143/10000, steps: 797, epsilon: 0.3189977016914014\n",
      " loss : [8.838307]\n",
      "episode: 1144/10000, steps: 829, epsilon: 0.31867870398971\n",
      " loss : [137.98097]\n",
      "episode: 1145/10000, steps: 814, epsilon: 0.31836002528572027\n",
      " loss : [15.663997]\n",
      "episode: 1146/10000, steps: 887, epsilon: 0.31804166526043454\n",
      " loss : [27.436708]\n",
      "episode: 1147/10000, steps: 852, epsilon: 0.3177236235951741\n",
      " loss : [3.5862784]\n",
      "episode: 1148/10000, steps: 843, epsilon: 0.3174058999715789\n",
      " loss : [8.388576]\n",
      "episode: 1149/10000, steps: 809, epsilon: 0.31708849407160733\n",
      " loss : [6.550143]\n",
      "episode: 1150/10000, steps: 832, epsilon: 0.31677140557753575\n",
      " loss : [3.4343855]\n",
      "episode: 1151/10000, steps: 882, epsilon: 0.31645463417195824\n",
      " loss : [2.7558842]\n",
      "episode: 1152/10000, steps: 895, epsilon: 0.3161381795377863\n",
      " loss : [2.0746422]\n",
      "episode: 1153/10000, steps: 879, epsilon: 0.3158220413582485\n",
      " loss : [96.43717]\n",
      "episode: 1154/10000, steps: 896, epsilon: 0.3155062193168902\n",
      " loss : [6.8344803]\n",
      "episode: 1155/10000, steps: 835, epsilon: 0.31519071309757335\n",
      " loss : [49.842617]\n",
      "episode: 1156/10000, steps: 831, epsilon: 0.3148755223844758\n",
      " loss : [2.9836612]\n",
      "episode: 1157/10000, steps: 827, epsilon: 0.31456064686209134\n",
      " loss : [4.8917375]\n",
      "episode: 1158/10000, steps: 830, epsilon: 0.31424608621522926\n",
      " loss : [4.3478346]\n",
      "episode: 1159/10000, steps: 978, epsilon: 0.31393184012901404\n",
      " loss : [4.0796742]\n",
      "episode: 1160/10000, steps: 876, epsilon: 0.31361790828888503\n",
      " loss : [3.353829]\n",
      "episode: 1161/10000, steps: 830, epsilon: 0.31330429038059615\n",
      " loss : [10.113084]\n",
      "episode: 1162/10000, steps: 903, epsilon: 0.31299098609021553\n",
      " loss : [6.28094]\n",
      "episode: 1163/10000, steps: 1031, epsilon: 0.3126779951041253\n",
      " loss : [4.2941146]\n",
      "episode: 1164/10000, steps: 818, epsilon: 0.31236531710902116\n",
      " loss : [4.9294314]\n",
      "episode: 1165/10000, steps: 648, epsilon: 0.3120529517919121\n",
      " loss : [1.5379564]\n",
      "episode: 1166/10000, steps: 828, epsilon: 0.3117408988401202\n",
      " loss : [6.9938974]\n",
      "episode: 1167/10000, steps: 847, epsilon: 0.31142915794128007\n",
      " loss : [2.0367403]\n",
      "episode: 1168/10000, steps: 657, epsilon: 0.31111772878333877\n",
      " loss : [1.9872029]\n",
      "episode: 1169/10000, steps: 870, epsilon: 0.3108066110545554\n",
      " loss : [3.3629286]\n",
      "episode: 1170/10000, steps: 818, epsilon: 0.3104958044435009\n",
      " loss : [2.1852582]\n",
      "episode: 1171/10000, steps: 823, epsilon: 0.3101853086390574\n",
      " loss : [3.0457578]\n",
      "episode: 1172/10000, steps: 964, epsilon: 0.30987512333041833\n",
      " loss : [4.560228]\n",
      "episode: 1173/10000, steps: 810, epsilon: 0.3095652482070879\n",
      " loss : [6.7756224]\n",
      "episode: 1174/10000, steps: 845, epsilon: 0.30925568295888084\n",
      " loss : [3.0398033]\n",
      "episode: 1175/10000, steps: 808, epsilon: 0.308946427275922\n",
      " loss : [6.474366]\n",
      "episode: 1176/10000, steps: 825, epsilon: 0.30863748084864606\n",
      " loss : [15.120418]\n",
      "episode: 1177/10000, steps: 800, epsilon: 0.3083288433677974\n",
      " loss : [5.0392675]\n",
      "episode: 1178/10000, steps: 801, epsilon: 0.30802051452442963\n",
      " loss : [274.62985]\n",
      "episode: 1179/10000, steps: 833, epsilon: 0.3077124940099052\n",
      " loss : [12.973435]\n",
      "episode: 1180/10000, steps: 836, epsilon: 0.3074047815158953\n",
      " loss : [10.750945]\n",
      "episode: 1181/10000, steps: 828, epsilon: 0.30709737673437937\n",
      " loss : [8.974939]\n",
      "episode: 1182/10000, steps: 1364, epsilon: 0.30679027935764497\n",
      " loss : [12.6523905]\n",
      "episode: 1183/10000, steps: 988, epsilon: 0.3064834890782873\n",
      " loss : [189.9429]\n",
      "episode: 1184/10000, steps: 992, epsilon: 0.306177005589209\n",
      " loss : [11.201506]\n",
      "episode: 1185/10000, steps: 820, epsilon: 0.3058708285836198\n",
      " loss : [9.058304]\n",
      "episode: 1186/10000, steps: 804, epsilon: 0.30556495775503617\n",
      " loss : [4.4901133]\n",
      "episode: 1187/10000, steps: 963, epsilon: 0.30525939279728115\n",
      " loss : [9.429705]\n",
      "episode: 1188/10000, steps: 880, epsilon: 0.30495413340448385\n",
      " loss : [5.6920853]\n",
      "episode: 1189/10000, steps: 1096, epsilon: 0.3046491792710794\n",
      " loss : [6.0944576]\n",
      "episode: 1190/10000, steps: 854, epsilon: 0.3043445300918083\n",
      " loss : [3.865304]\n",
      "episode: 1191/10000, steps: 424, epsilon: 0.30404018556171647\n",
      " loss : [5.593684]\n",
      "episode: 1192/10000, steps: 937, epsilon: 0.30373614537615473\n",
      " loss : [5.691792]\n",
      "episode: 1193/10000, steps: 819, epsilon: 0.3034324092307786\n",
      " loss : [5.3114877]\n",
      "episode: 1194/10000, steps: 823, epsilon: 0.3031289768215478\n",
      " loss : [3.0022204]\n",
      "episode: 1195/10000, steps: 885, epsilon: 0.30282584784472627\n",
      " loss : [4.8256917]\n",
      "episode: 1196/10000, steps: 896, epsilon: 0.3025230219968815\n",
      " loss : [6.2009397]\n",
      "episode: 1197/10000, steps: 822, epsilon: 0.3022204989748846\n",
      " loss : [2.2451572]\n",
      "episode: 1198/10000, steps: 874, epsilon: 0.30191827847590974\n",
      " loss : [2.455969]\n",
      "episode: 1199/10000, steps: 656, epsilon: 0.3016163601974338\n",
      " loss : [9.836522]\n",
      "episode: 1200/10000, steps: 970, epsilon: 0.3013147438372364\n",
      " loss : [3.4465885]\n",
      "episode: 1201/10000, steps: 1009, epsilon: 0.3010134290933992\n",
      " loss : [1.1485202]\n",
      "episode: 1202/10000, steps: 820, epsilon: 0.3007124156643058\n",
      " loss : [9.907916]\n",
      "episode: 1203/10000, steps: 831, epsilon: 0.30041170324864147\n",
      " loss : [6.7945323]\n",
      "episode: 1204/10000, steps: 619, epsilon: 0.30011129154539284\n",
      " loss : [2.3038456]\n",
      "episode: 1205/10000, steps: 668, epsilon: 0.29981118025384745\n",
      " loss : [10.035869]\n",
      "episode: 1206/10000, steps: 1388, epsilon: 0.2995113690735936\n",
      " loss : [11.534177]\n",
      "episode: 1207/10000, steps: 756, epsilon: 0.29921185770452\n",
      " loss : [10.248392]\n",
      "episode: 1208/10000, steps: 798, epsilon: 0.2989126458468155\n",
      " loss : [11.084284]\n",
      "episode: 1209/10000, steps: 824, epsilon: 0.2986137332009687\n",
      " loss : [2.4594717]\n",
      "episode: 1210/10000, steps: 855, epsilon: 0.29831511946776773\n",
      " loss : [210.62173]\n",
      "episode: 1211/10000, steps: 497, epsilon: 0.29801680434829997\n",
      " loss : [6.195341]\n",
      "episode: 1212/10000, steps: 1079, epsilon: 0.29771878754395165\n",
      " loss : [8.656444]\n",
      "episode: 1213/10000, steps: 471, epsilon: 0.29742106875640767\n",
      " loss : [160.73242]\n",
      "episode: 1214/10000, steps: 837, epsilon: 0.29712364768765126\n",
      " loss : [24.958622]\n",
      "episode: 1215/10000, steps: 785, epsilon: 0.2968265240399636\n",
      " loss : [11.4918995]\n",
      "episode: 1216/10000, steps: 826, epsilon: 0.2965296975159237\n",
      " loss : [53.779163]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1217/10000, steps: 785, epsilon: 0.29623316781840775\n",
      " loss : [19.48968]\n",
      "episode: 1218/10000, steps: 789, epsilon: 0.29593693465058934\n",
      " loss : [2.5654562]\n",
      "episode: 1219/10000, steps: 792, epsilon: 0.29564099771593877\n",
      " loss : [29.120338]\n",
      "episode: 1220/10000, steps: 642, epsilon: 0.2953453567182228\n",
      " loss : [5.69614]\n",
      "episode: 1221/10000, steps: 474, epsilon: 0.2950500113615046\n",
      " loss : [2.7502103]\n",
      "episode: 1222/10000, steps: 402, epsilon: 0.2947549613501431\n",
      " loss : [7.965414]\n",
      "episode: 1223/10000, steps: 649, epsilon: 0.294460206388793\n",
      " loss : [8.414908]\n",
      "episode: 1224/10000, steps: 934, epsilon: 0.2941657461824042\n",
      " loss : [9.158173]\n",
      "episode: 1225/10000, steps: 814, epsilon: 0.2938715804362218\n",
      " loss : [12.288383]\n",
      "episode: 1226/10000, steps: 1375, epsilon: 0.2935777088557856\n",
      " loss : [167.62769]\n",
      "episode: 1227/10000, steps: 919, epsilon: 0.2932841311469298\n",
      " loss : [3.7621691]\n",
      "episode: 1228/10000, steps: 618, epsilon: 0.2929908470157829\n",
      " loss : [8.39559]\n",
      "episode: 1229/10000, steps: 817, epsilon: 0.2926978561687671\n",
      " loss : [12.812112]\n",
      "episode: 1230/10000, steps: 833, epsilon: 0.29240515831259833\n",
      " loss : [4.566207]\n",
      "episode: 1231/10000, steps: 1001, epsilon: 0.29211275315428575\n",
      " loss : [2.2052865]\n",
      "episode: 1232/10000, steps: 874, epsilon: 0.2918206404011315\n",
      " loss : [3.1182938]\n",
      "episode: 1233/10000, steps: 819, epsilon: 0.29152881976073036\n",
      " loss : [6.443549]\n",
      "episode: 1234/10000, steps: 832, epsilon: 0.2912372909409696\n",
      " loss : [54.20449]\n",
      "episode: 1235/10000, steps: 864, epsilon: 0.2909460536500286\n",
      " loss : [125.081154]\n",
      "episode: 1236/10000, steps: 992, epsilon: 0.2906551075963786\n",
      " loss : [71.429016]\n",
      "episode: 1237/10000, steps: 879, epsilon: 0.2903644524887822\n",
      " loss : [2.8292499]\n",
      "episode: 1238/10000, steps: 813, epsilon: 0.2900740880362934\n",
      " loss : [4.4896593]\n",
      "episode: 1239/10000, steps: 814, epsilon: 0.28978401394825715\n",
      " loss : [36.505898]\n",
      "episode: 1240/10000, steps: 804, epsilon: 0.2894942299343089\n",
      " loss : [16.108644]\n",
      "episode: 1241/10000, steps: 802, epsilon: 0.2892047357043746\n",
      " loss : [5.994505]\n",
      "episode: 1242/10000, steps: 848, epsilon: 0.28891553096867023\n",
      " loss : [147.38185]\n",
      "episode: 1243/10000, steps: 840, epsilon: 0.2886266154377016\n",
      " loss : [18.330639]\n",
      "episode: 1244/10000, steps: 1029, epsilon: 0.28833798882226386\n",
      " loss : [5.1440897]\n",
      "episode: 1245/10000, steps: 824, epsilon: 0.2880496508334416\n",
      " loss : [113.34]\n",
      "episode: 1246/10000, steps: 791, epsilon: 0.28776160118260813\n",
      " loss : [56.28276]\n",
      "episode: 1247/10000, steps: 816, epsilon: 0.2874738395814255\n",
      " loss : [10.652444]\n",
      "episode: 1248/10000, steps: 836, epsilon: 0.2871863657418441\n",
      " loss : [27.669682]\n",
      "episode: 1249/10000, steps: 835, epsilon: 0.28689917937610226\n",
      " loss : [18.851875]\n",
      "episode: 1250/10000, steps: 827, epsilon: 0.2866122801967262\n",
      " loss : [8.60215]\n",
      "episode: 1251/10000, steps: 833, epsilon: 0.28632566791652947\n",
      " loss : [10.486074]\n",
      "episode: 1252/10000, steps: 833, epsilon: 0.28603934224861294\n",
      " loss : [7.7448454]\n",
      "episode: 1253/10000, steps: 651, epsilon: 0.2857533029063643\n",
      " loss : [6.85829]\n",
      "episode: 1254/10000, steps: 635, epsilon: 0.28546754960345794\n",
      " loss : [8.549583]\n",
      "episode: 1255/10000, steps: 830, epsilon: 0.2851820820538545\n",
      " loss : [35.900627]\n",
      "episode: 1256/10000, steps: 1078, epsilon: 0.2848968999718006\n",
      " loss : [10.115718]\n",
      "episode: 1257/10000, steps: 807, epsilon: 0.2846120030718288\n",
      " loss : [4.2205157]\n",
      "episode: 1258/10000, steps: 839, epsilon: 0.284327391068757\n",
      " loss : [5.4910975]\n",
      "episode: 1259/10000, steps: 769, epsilon: 0.2840430636776882\n",
      " loss : [8.88714]\n",
      "episode: 1260/10000, steps: 805, epsilon: 0.28375902061401054\n",
      " loss : [5.7746353]\n",
      "episode: 1261/10000, steps: 1100, epsilon: 0.28347526159339653\n",
      " loss : [7.56089]\n",
      "episode: 1262/10000, steps: 789, epsilon: 0.28319178633180314\n",
      " loss : [14.217862]\n",
      "episode: 1263/10000, steps: 638, epsilon: 0.28290859454547135\n",
      " loss : [27.91703]\n",
      "episode: 1264/10000, steps: 585, epsilon: 0.28262568595092585\n",
      " loss : [16.328825]\n",
      "episode: 1265/10000, steps: 811, epsilon: 0.2823430602649749\n",
      " loss : [36.40253]\n",
      "episode: 1266/10000, steps: 1480, epsilon: 0.28206071720470993\n",
      " loss : [33.66696]\n",
      "episode: 1267/10000, steps: 800, epsilon: 0.2817786564875052\n",
      " loss : [50.157986]\n",
      "episode: 1268/10000, steps: 782, epsilon: 0.28149687783101773\n",
      " loss : [27.108574]\n",
      "episode: 1269/10000, steps: 664, epsilon: 0.2812153809531867\n",
      " loss : [4.9217253]\n",
      "episode: 1270/10000, steps: 813, epsilon: 0.28093416557223355\n",
      " loss : [6.1539903]\n",
      "episode: 1271/10000, steps: 824, epsilon: 0.2806532314066613\n",
      " loss : [97.892555]\n",
      "episode: 1272/10000, steps: 644, epsilon: 0.2803725781752547\n",
      " loss : [33.310047]\n",
      "episode: 1273/10000, steps: 866, epsilon: 0.2800922055970794\n",
      " loss : [22.69405]\n",
      "episode: 1274/10000, steps: 864, epsilon: 0.2798121133914823\n",
      " loss : [38.258003]\n",
      "episode: 1275/10000, steps: 972, epsilon: 0.2795323012780908\n",
      " loss : [14.1424675]\n",
      "episode: 1276/10000, steps: 878, epsilon: 0.2792527689768127\n",
      " loss : [5.696244]\n",
      "episode: 1277/10000, steps: 802, epsilon: 0.2789735162078359\n",
      " loss : [10.41562]\n",
      "episode: 1278/10000, steps: 835, epsilon: 0.2786945426916281\n",
      " loss : [3.5112426]\n",
      "episode: 1279/10000, steps: 798, epsilon: 0.27841584814893644\n",
      " loss : [11.450628]\n",
      "episode: 1280/10000, steps: 618, epsilon: 0.2781374323007875\n",
      " loss : [7.8570385]\n",
      "episode: 1281/10000, steps: 813, epsilon: 0.27785929486848676\n",
      " loss : [6.0824704]\n",
      "episode: 1282/10000, steps: 817, epsilon: 0.27758143557361825\n",
      " loss : [5.6636434]\n",
      "episode: 1283/10000, steps: 1544, epsilon: 0.27730385413804465\n",
      " loss : [317.08853]\n",
      "episode: 1284/10000, steps: 614, epsilon: 0.2770265502839066\n",
      " loss : [14.229664]\n",
      "episode: 1285/10000, steps: 791, epsilon: 0.2767495237336227\n",
      " loss : [29.142614]\n",
      "episode: 1286/10000, steps: 627, epsilon: 0.2764727742098891\n",
      " loss : [5.448814]\n",
      "episode: 1287/10000, steps: 828, epsilon: 0.2761963014356792\n",
      " loss : [2.128782]\n",
      "episode: 1288/10000, steps: 788, epsilon: 0.27592010513424353\n",
      " loss : [4.4030375]\n",
      "episode: 1289/10000, steps: 854, epsilon: 0.2756441850291093\n",
      " loss : [339.09814]\n",
      "episode: 1290/10000, steps: 646, epsilon: 0.2753685408440802\n",
      " loss : [11.757505]\n",
      "episode: 1291/10000, steps: 640, epsilon: 0.2750931723032361\n",
      " loss : [37.332508]\n",
      "episode: 1292/10000, steps: 1079, epsilon: 0.27481807913093287\n",
      " loss : [380.16074]\n",
      "episode: 1293/10000, steps: 845, epsilon: 0.27454326105180193\n",
      " loss : [5.962807]\n",
      "episode: 1294/10000, steps: 850, epsilon: 0.2742687177907501\n",
      " loss : [16.421175]\n",
      "episode: 1295/10000, steps: 515, epsilon: 0.2739944490729594\n",
      " loss : [3.9311903]\n",
      "episode: 1296/10000, steps: 704, epsilon: 0.27372045462388644\n",
      " loss : [131.86461]\n",
      "episode: 1297/10000, steps: 499, epsilon: 0.2734467341692626\n",
      " loss : [6.000569]\n",
      "episode: 1298/10000, steps: 990, epsilon: 0.27317328743509334\n",
      " loss : [29.760363]\n",
      "episode: 1299/10000, steps: 1390, epsilon: 0.27290011414765825\n",
      " loss : [18.216217]\n",
      "episode: 1300/10000, steps: 592, epsilon: 0.2726272140335106\n",
      " loss : [13.88128]\n",
      "episode: 1301/10000, steps: 943, epsilon: 0.27235458681947705\n",
      " loss : [13.847661]\n",
      "episode: 1302/10000, steps: 608, epsilon: 0.2720822322326576\n",
      " loss : [8.934216]\n",
      "episode: 1303/10000, steps: 646, epsilon: 0.2718101500004249\n",
      " loss : [20.152082]\n",
      "episode: 1304/10000, steps: 840, epsilon: 0.27153833985042447\n",
      " loss : [8.142237]\n",
      "episode: 1305/10000, steps: 870, epsilon: 0.27126680151057403\n",
      " loss : [4.9265914]\n",
      "episode: 1306/10000, steps: 817, epsilon: 0.27099553470906346\n",
      " loss : [12.907629]\n",
      "episode: 1307/10000, steps: 668, epsilon: 0.2707245391743544\n",
      " loss : [11.315168]\n",
      "episode: 1308/10000, steps: 515, epsilon: 0.27045381463518003\n",
      " loss : [14.773489]\n",
      "episode: 1309/10000, steps: 530, epsilon: 0.27018336082054484\n",
      " loss : [8.022093]\n",
      "episode: 1310/10000, steps: 541, epsilon: 0.2699131774597243\n",
      " loss : [12.386571]\n",
      "episode: 1311/10000, steps: 649, epsilon: 0.2696432642822646\n",
      " loss : [12.665834]\n",
      "episode: 1312/10000, steps: 881, epsilon: 0.26937362101798235\n",
      " loss : [12.613364]\n",
      "episode: 1313/10000, steps: 797, epsilon: 0.26910424739696437\n",
      " loss : [17.026041]\n",
      "episode: 1314/10000, steps: 777, epsilon: 0.2688351431495674\n",
      " loss : [24.07724]\n",
      "episode: 1315/10000, steps: 834, epsilon: 0.26856630800641784\n",
      " loss : [328.27676]\n",
      "episode: 1316/10000, steps: 854, epsilon: 0.2682977416984114\n",
      " loss : [229.36266]\n",
      "episode: 1317/10000, steps: 649, epsilon: 0.268029443956713\n",
      " loss : [16.439634]\n",
      "episode: 1318/10000, steps: 851, epsilon: 0.2677614145127563\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " loss : [267.4002]\n",
      "episode: 1319/10000, steps: 863, epsilon: 0.2674936530982436\n",
      " loss : [18.49612]\n",
      "episode: 1320/10000, steps: 661, epsilon: 0.26722615944514533\n",
      " loss : [74.67768]\n",
      "episode: 1321/10000, steps: 811, epsilon: 0.2669589332857002\n",
      " loss : [21.460003]\n",
      "episode: 1322/10000, steps: 797, epsilon: 0.2666919743524145\n",
      " loss : [10.60117]\n",
      "episode: 1323/10000, steps: 821, epsilon: 0.26642528237806207\n",
      " loss : [19.866064]\n",
      "episode: 1324/10000, steps: 658, epsilon: 0.266158857095684\n",
      " loss : [14.262849]\n",
      "episode: 1325/10000, steps: 844, epsilon: 0.2658926982385883\n",
      " loss : [8.029356]\n",
      "episode: 1326/10000, steps: 795, epsilon: 0.26562680554034973\n",
      " loss : [7.112507]\n",
      "episode: 1327/10000, steps: 781, epsilon: 0.26536117873480936\n",
      " loss : [5.6215196]\n",
      "episode: 1328/10000, steps: 932, epsilon: 0.26509581755607453\n",
      " loss : [6.5695934]\n",
      "episode: 1329/10000, steps: 584, epsilon: 0.26483072173851846\n",
      " loss : [6.2789903]\n",
      "episode: 1330/10000, steps: 644, epsilon: 0.26456589101677996\n",
      " loss : [9.256517]\n",
      "episode: 1331/10000, steps: 1044, epsilon: 0.2643013251257632\n",
      " loss : [7.6908846]\n",
      "episode: 1332/10000, steps: 611, epsilon: 0.2640370238006374\n",
      " loss : [6.4654636]\n",
      "episode: 1333/10000, steps: 386, epsilon: 0.2637729867768368\n",
      " loss : [59.01617]\n",
      "episode: 1334/10000, steps: 917, epsilon: 0.26350921379006\n",
      " loss : [13.697506]\n",
      "episode: 1335/10000, steps: 953, epsilon: 0.26324570457626995\n",
      " loss : [15.740121]\n",
      "episode: 1336/10000, steps: 920, epsilon: 0.2629824588716937\n",
      " loss : [241.34386]\n",
      "episode: 1337/10000, steps: 973, epsilon: 0.262719476412822\n",
      " loss : [353.44208]\n",
      "episode: 1338/10000, steps: 1444, epsilon: 0.2624567569364092\n",
      " loss : [10.3595705]\n",
      "episode: 1339/10000, steps: 946, epsilon: 0.26219430017947276\n",
      " loss : [48.3881]\n",
      "episode: 1340/10000, steps: 341, epsilon: 0.2619321058792933\n",
      " loss : [28.162218]\n",
      "episode: 1341/10000, steps: 645, epsilon: 0.261670173773414\n",
      " loss : [15.103224]\n",
      "episode: 1342/10000, steps: 310, epsilon: 0.2614085035996406\n",
      " loss : [19.138062]\n",
      "episode: 1343/10000, steps: 1088, epsilon: 0.26114709509604095\n",
      " loss : [26.539728]\n",
      "episode: 1344/10000, steps: 629, epsilon: 0.2608859480009449\n",
      " loss : [337.40967]\n",
      "episode: 1345/10000, steps: 363, epsilon: 0.26062506205294395\n",
      " loss : [301.89056]\n",
      "episode: 1346/10000, steps: 1142, epsilon: 0.260364436990891\n",
      " loss : [42.571095]\n",
      "episode: 1347/10000, steps: 519, epsilon: 0.2601040725539001\n",
      " loss : [19.389957]\n",
      "episode: 1348/10000, steps: 512, epsilon: 0.2598439684813462\n",
      " loss : [18.5488]\n",
      "episode: 1349/10000, steps: 1159, epsilon: 0.2595841245128649\n",
      " loss : [340.1975]\n",
      "episode: 1350/10000, steps: 556, epsilon: 0.259324540388352\n",
      " loss : [298.73816]\n",
      "episode: 1351/10000, steps: 842, epsilon: 0.25906521584796366\n",
      " loss : [15.994112]\n",
      "episode: 1352/10000, steps: 880, epsilon: 0.2588061506321157\n",
      " loss : [262.9738]\n",
      "episode: 1353/10000, steps: 828, epsilon: 0.2585473444814836\n",
      " loss : [12.332922]\n",
      "episode: 1354/10000, steps: 801, epsilon: 0.2582887971370021\n",
      " loss : [218.08096]\n",
      "episode: 1355/10000, steps: 808, epsilon: 0.2580305083398651\n",
      " loss : [17.72696]\n",
      "episode: 1356/10000, steps: 852, epsilon: 0.2577724778315252\n",
      " loss : [28.68492]\n",
      "episode: 1357/10000, steps: 798, epsilon: 0.25751470535369364\n",
      " loss : [7.826819]\n",
      "episode: 1358/10000, steps: 804, epsilon: 0.25725719064833996\n",
      " loss : [7.628858]\n",
      "episode: 1359/10000, steps: 853, epsilon: 0.2569999334576916\n",
      " loss : [10.3376045]\n",
      "episode: 1360/10000, steps: 845, epsilon: 0.25674293352423394\n",
      " loss : [195.62555]\n",
      "episode: 1361/10000, steps: 816, epsilon: 0.2564861905907097\n",
      " loss : [20.72051]\n",
      "episode: 1362/10000, steps: 799, epsilon: 0.256229704400119\n",
      " loss : [7.2437015]\n",
      "episode: 1363/10000, steps: 653, epsilon: 0.25597347469571885\n",
      " loss : [4.506697]\n",
      "episode: 1364/10000, steps: 895, epsilon: 0.25571750122102316\n",
      " loss : [39.697636]\n",
      "episode: 1365/10000, steps: 855, epsilon: 0.25546178371980216\n",
      " loss : [11.130647]\n",
      "episode: 1366/10000, steps: 656, epsilon: 0.25520632193608234\n",
      " loss : [4.6218405]\n",
      "episode: 1367/10000, steps: 636, epsilon: 0.25495111561414624\n",
      " loss : [6.763547]\n",
      "episode: 1368/10000, steps: 841, epsilon: 0.2546961644985321\n",
      " loss : [6.2135177]\n",
      "episode: 1369/10000, steps: 810, epsilon: 0.2544414683340336\n",
      " loss : [7.1823683]\n",
      "episode: 1370/10000, steps: 799, epsilon: 0.25418702686569955\n",
      " loss : [3.716873]\n",
      "episode: 1371/10000, steps: 800, epsilon: 0.25393283983883386\n",
      " loss : [5.61232]\n",
      "episode: 1372/10000, steps: 663, epsilon: 0.25367890699899504\n",
      " loss : [5.3103943]\n",
      "episode: 1373/10000, steps: 684, epsilon: 0.253425228091996\n",
      " loss : [6.1017394]\n",
      "episode: 1374/10000, steps: 836, epsilon: 0.253171802863904\n",
      " loss : [5.0475016]\n",
      "episode: 1375/10000, steps: 802, epsilon: 0.2529186310610401\n",
      " loss : [6.1165013]\n",
      "episode: 1376/10000, steps: 645, epsilon: 0.2526657124299791\n",
      " loss : [6.860281]\n",
      "episode: 1377/10000, steps: 829, epsilon: 0.2524130467175491\n",
      " loss : [3.7854764]\n",
      "episode: 1378/10000, steps: 667, epsilon: 0.2521606336708316\n",
      " loss : [4.5484385]\n",
      "episode: 1379/10000, steps: 791, epsilon: 0.25190847303716074\n",
      " loss : [2.8398373]\n",
      "episode: 1380/10000, steps: 655, epsilon: 0.25165656456412355\n",
      " loss : [4.1228495]\n",
      "episode: 1381/10000, steps: 1937, epsilon: 0.25140490799955945\n",
      " loss : [2.5097775]\n",
      "episode: 1382/10000, steps: 865, epsilon: 0.25115350309155987\n",
      " loss : [4.8883324]\n",
      "episode: 1383/10000, steps: 817, epsilon: 0.2509023495884683\n",
      " loss : [3.229779]\n",
      "episode: 1384/10000, steps: 641, epsilon: 0.25065144723887983\n",
      " loss : [3.177977]\n",
      "episode: 1385/10000, steps: 659, epsilon: 0.25040079579164093\n",
      " loss : [3.9161654]\n",
      "episode: 1386/10000, steps: 843, epsilon: 0.2501503949958493\n",
      " loss : [2.7582054]\n",
      "episode: 1387/10000, steps: 792, epsilon: 0.24990024460085344\n",
      " loss : [5.9221215]\n",
      "episode: 1388/10000, steps: 658, epsilon: 0.24965034435625258\n",
      " loss : [1.9033698]\n",
      "episode: 1389/10000, steps: 669, epsilon: 0.24940069401189632\n",
      " loss : [212.66434]\n",
      "episode: 1390/10000, steps: 1105, epsilon: 0.24915129331788444\n",
      " loss : [16.910269]\n",
      "episode: 1391/10000, steps: 825, epsilon: 0.24890214202456656\n",
      " loss : [4.268394]\n",
      "episode: 1392/10000, steps: 791, epsilon: 0.248653239882542\n",
      " loss : [4.796628]\n",
      "episode: 1393/10000, steps: 666, epsilon: 0.24840458664265946\n",
      " loss : [164.46686]\n",
      "episode: 1394/10000, steps: 672, epsilon: 0.2481561820560168\n",
      " loss : [89.37064]\n",
      "episode: 1395/10000, steps: 801, epsilon: 0.2479080258739608\n",
      " loss : [434.20288]\n",
      "episode: 1396/10000, steps: 803, epsilon: 0.24766011784808684\n",
      " loss : [5.4192257]\n",
      "episode: 1397/10000, steps: 643, epsilon: 0.24741245773023876\n",
      " loss : [31.629793]\n",
      "episode: 1398/10000, steps: 646, epsilon: 0.24716504527250852\n",
      " loss : [10.698927]\n",
      "episode: 1399/10000, steps: 829, epsilon: 0.24691788022723601\n",
      " loss : [6.498626]\n",
      "episode: 1400/10000, steps: 644, epsilon: 0.24667096234700878\n",
      " loss : [6.0654125]\n",
      "episode: 1401/10000, steps: 656, epsilon: 0.24642429138466176\n",
      " loss : [5.072942]\n",
      "episode: 1402/10000, steps: 821, epsilon: 0.2461778670932771\n",
      " loss : [4.143526]\n",
      "episode: 1403/10000, steps: 790, epsilon: 0.24593168922618383\n",
      " loss : [5.252256]\n",
      "episode: 1404/10000, steps: 791, epsilon: 0.24568575753695765\n",
      " loss : [2.2632122]\n",
      "episode: 1405/10000, steps: 815, epsilon: 0.2454400717794207\n",
      " loss : [15.105639]\n",
      "episode: 1406/10000, steps: 848, epsilon: 0.24519463170764128\n",
      " loss : [2.195721]\n",
      "episode: 1407/10000, steps: 976, epsilon: 0.24494943707593364\n",
      " loss : [3.5566022]\n",
      "episode: 1408/10000, steps: 872, epsilon: 0.24470448763885772\n",
      " loss : [6.218093]\n",
      "episode: 1409/10000, steps: 655, epsilon: 0.24445978315121886\n",
      " loss : [6.5262623]\n",
      "episode: 1410/10000, steps: 647, epsilon: 0.24421532336806764\n",
      " loss : [6.685812]\n",
      "episode: 1411/10000, steps: 812, epsilon: 0.24397110804469957\n",
      " loss : [3.2366507]\n",
      "episode: 1412/10000, steps: 793, epsilon: 0.24372713693665488\n",
      " loss : [8.315348]\n",
      "episode: 1413/10000, steps: 648, epsilon: 0.24348340979971822\n",
      " loss : [3.8999226]\n",
      "episode: 1414/10000, steps: 646, epsilon: 0.2432399263899185\n",
      " loss : [6.7403564]\n",
      "episode: 1415/10000, steps: 663, epsilon: 0.2429966864635286\n",
      " loss : [5.174313]\n",
      "episode: 1416/10000, steps: 822, epsilon: 0.24275368977706507\n",
      " loss : [4.640052]\n",
      "episode: 1417/10000, steps: 663, epsilon: 0.24251093608728802\n",
      " loss : [3.0294445]\n",
      "episode: 1418/10000, steps: 811, epsilon: 0.24226842515120073\n",
      " loss : [8.363237]\n",
      "episode: 1419/10000, steps: 668, epsilon: 0.24202615672604952\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " loss : [2.1788323]\n",
      "episode: 1420/10000, steps: 790, epsilon: 0.24178413056932346\n",
      " loss : [7.984494]\n",
      "episode: 1421/10000, steps: 894, epsilon: 0.24154234643875414\n",
      " loss : [2.6035047]\n",
      "episode: 1422/10000, steps: 668, epsilon: 0.24130080409231539\n",
      " loss : [3.4284859]\n",
      "episode: 1423/10000, steps: 1360, epsilon: 0.24105950328822306\n",
      " loss : [7.198179]\n",
      "episode: 1424/10000, steps: 1806, epsilon: 0.24081844378493483\n",
      " loss : [287.19116]\n",
      "episode: 1425/10000, steps: 847, epsilon: 0.2405776253411499\n",
      " loss : [408.40875]\n",
      "episode: 1426/10000, steps: 820, epsilon: 0.24033704771580874\n",
      " loss : [8.131878]\n",
      "episode: 1427/10000, steps: 829, epsilon: 0.24009671066809293\n",
      " loss : [7.8482914]\n",
      "episode: 1428/10000, steps: 898, epsilon: 0.23985661395742483\n",
      " loss : [6.2913933]\n",
      "episode: 1429/10000, steps: 640, epsilon: 0.2396167573434674\n",
      " loss : [4.3501215]\n",
      "episode: 1430/10000, steps: 789, epsilon: 0.23937714058612394\n",
      " loss : [420.59857]\n",
      "episode: 1431/10000, steps: 831, epsilon: 0.23913776344553783\n",
      " loss : [71.27079]\n",
      "episode: 1432/10000, steps: 818, epsilon: 0.2388986256820923\n",
      " loss : [10.076855]\n",
      "episode: 1433/10000, steps: 824, epsilon: 0.2386597270564102\n",
      " loss : [38.76593]\n",
      "episode: 1434/10000, steps: 824, epsilon: 0.23842106732935378\n",
      " loss : [115.35338]\n",
      "episode: 1435/10000, steps: 844, epsilon: 0.23818264626202443\n",
      " loss : [78.84085]\n",
      "episode: 1436/10000, steps: 831, epsilon: 0.2379444636157624\n",
      " loss : [4.92331]\n",
      "episode: 1437/10000, steps: 804, epsilon: 0.23770651915214663\n",
      " loss : [13.9204855]\n",
      "episode: 1438/10000, steps: 793, epsilon: 0.2374688126329945\n",
      " loss : [29.106155]\n",
      "episode: 1439/10000, steps: 804, epsilon: 0.23723134382036148\n",
      " loss : [19.697182]\n",
      "episode: 1440/10000, steps: 705, epsilon: 0.23699411247654112\n",
      " loss : [28.637331]\n",
      "episode: 1441/10000, steps: 800, epsilon: 0.23675711836406457\n",
      " loss : [5.2195463]\n",
      "episode: 1442/10000, steps: 685, epsilon: 0.2365203612457005\n",
      " loss : [16.141489]\n",
      "episode: 1443/10000, steps: 837, epsilon: 0.2362838408844548\n",
      " loss : [9.430595]\n",
      "episode: 1444/10000, steps: 643, epsilon: 0.23604755704357036\n",
      " loss : [12.222512]\n",
      "episode: 1445/10000, steps: 1027, epsilon: 0.2358115094865268\n",
      " loss : [9.74295]\n",
      "episode: 1446/10000, steps: 897, epsilon: 0.23557569797704025\n",
      " loss : [8.213869]\n",
      "episode: 1447/10000, steps: 821, epsilon: 0.2353401222790632\n",
      " loss : [9.275111]\n",
      "episode: 1448/10000, steps: 851, epsilon: 0.23510478215678413\n",
      " loss : [4.339728]\n",
      "episode: 1449/10000, steps: 826, epsilon: 0.23486967737462733\n",
      " loss : [6.037481]\n",
      "episode: 1450/10000, steps: 656, epsilon: 0.2346348076972527\n",
      " loss : [5.41171]\n",
      "episode: 1451/10000, steps: 1097, epsilon: 0.23440017288955545\n",
      " loss : [6.2114825]\n",
      "episode: 1452/10000, steps: 831, epsilon: 0.2341657727166659\n",
      " loss : [9.770364]\n",
      "episode: 1453/10000, steps: 818, epsilon: 0.2339316069439492\n",
      " loss : [6.015654]\n",
      "episode: 1454/10000, steps: 861, epsilon: 0.23369767533700525\n",
      " loss : [9.047793]\n",
      "episode: 1455/10000, steps: 658, epsilon: 0.23346397766166824\n",
      " loss : [4.6209636]\n",
      "episode: 1456/10000, steps: 876, epsilon: 0.23323051368400657\n",
      " loss : [3.060834]\n",
      "episode: 1457/10000, steps: 665, epsilon: 0.23299728317032256\n",
      " loss : [2.9843166]\n",
      "episode: 1458/10000, steps: 652, epsilon: 0.23276428588715223\n",
      " loss : [44.442177]\n",
      "episode: 1459/10000, steps: 815, epsilon: 0.23253152160126508\n",
      " loss : [8.317057]\n",
      "episode: 1460/10000, steps: 827, epsilon: 0.23229899007966381\n",
      " loss : [6.294771]\n",
      "episode: 1461/10000, steps: 875, epsilon: 0.23206669108958414\n",
      " loss : [8.968405]\n",
      "episode: 1462/10000, steps: 860, epsilon: 0.23183462439849456\n",
      " loss : [8.098299]\n",
      "episode: 1463/10000, steps: 792, epsilon: 0.23160278977409607\n",
      " loss : [8.663453]\n",
      "episode: 1464/10000, steps: 775, epsilon: 0.23137118698432196\n",
      " loss : [13.355125]\n",
      "episode: 1465/10000, steps: 843, epsilon: 0.23113981579733764\n",
      " loss : [7.3989162]\n",
      "episode: 1466/10000, steps: 974, epsilon: 0.2309086759815403\n",
      " loss : [11.645386]\n",
      "episode: 1467/10000, steps: 845, epsilon: 0.23067776730555875\n",
      " loss : [5.6250224]\n",
      "episode: 1468/10000, steps: 865, epsilon: 0.23044708953825319\n",
      " loss : [11.859168]\n",
      "episode: 1469/10000, steps: 1478, epsilon: 0.23021664244871493\n",
      " loss : [182.34702]\n",
      "episode: 1470/10000, steps: 663, epsilon: 0.22998642580626621\n",
      " loss : [310.85846]\n",
      "episode: 1471/10000, steps: 935, epsilon: 0.22975643938045995\n",
      " loss : [31.747005]\n",
      "episode: 1472/10000, steps: 801, epsilon: 0.2295266829410795\n",
      " loss : [10.5375]\n",
      "episode: 1473/10000, steps: 805, epsilon: 0.22929715625813843\n",
      " loss : [10.442149]\n",
      "episode: 1474/10000, steps: 822, epsilon: 0.2290678591018803\n",
      " loss : [52.361496]\n",
      "episode: 1475/10000, steps: 386, epsilon: 0.22883879124277842\n",
      " loss : [6.7227855]\n",
      "episode: 1476/10000, steps: 785, epsilon: 0.22860995245153565\n",
      " loss : [226.0719]\n",
      "episode: 1477/10000, steps: 809, epsilon: 0.22838134249908412\n",
      " loss : [219.1072]\n",
      "episode: 1478/10000, steps: 812, epsilon: 0.22815296115658504\n",
      " loss : [15.112442]\n",
      "episode: 1479/10000, steps: 1042, epsilon: 0.22792480819542846\n",
      " loss : [5.4208794]\n",
      "episode: 1480/10000, steps: 863, epsilon: 0.22769688338723304\n",
      " loss : [11.661135]\n",
      "episode: 1481/10000, steps: 822, epsilon: 0.22746918650384582\n",
      " loss : [4.1153502]\n",
      "episode: 1482/10000, steps: 613, epsilon: 0.22724171731734197\n",
      " loss : [12.71143]\n",
      "episode: 1483/10000, steps: 766, epsilon: 0.22701447560002463\n",
      " loss : [71.051285]\n",
      "episode: 1484/10000, steps: 1156, epsilon: 0.2267874611244246\n",
      " loss : [2.5888994]\n",
      "episode: 1485/10000, steps: 785, epsilon: 0.22656067366330018\n",
      " loss : [5.6801496]\n",
      "episode: 1486/10000, steps: 443, epsilon: 0.22633411298963688\n",
      " loss : [7.3782973]\n",
      "episode: 1487/10000, steps: 649, epsilon: 0.22610777887664724\n",
      " loss : [16.251219]\n",
      "episode: 1488/10000, steps: 532, epsilon: 0.22588167109777058\n",
      " loss : [10.88614]\n",
      "episode: 1489/10000, steps: 648, epsilon: 0.2256557894266728\n",
      " loss : [7.7043033]\n",
      "episode: 1490/10000, steps: 639, epsilon: 0.22543013363724612\n",
      " loss : [295.05298]\n",
      "episode: 1491/10000, steps: 921, epsilon: 0.22520470350360888\n",
      " loss : [12.920666]\n",
      "episode: 1492/10000, steps: 1035, epsilon: 0.2249794988001053\n",
      " loss : [15.444512]\n",
      "episode: 1493/10000, steps: 676, epsilon: 0.2247545193013052\n",
      " loss : [8.997508]\n",
      "episode: 1494/10000, steps: 663, epsilon: 0.22452976478200387\n",
      " loss : [20.175013]\n",
      "episode: 1495/10000, steps: 510, epsilon: 0.22430523501722185\n",
      " loss : [4.924583]\n",
      "episode: 1496/10000, steps: 334, epsilon: 0.22408092978220462\n",
      " loss : [16.431679]\n",
      "episode: 1497/10000, steps: 526, epsilon: 0.2238568488524224\n",
      " loss : [8.709829]\n",
      "episode: 1498/10000, steps: 647, epsilon: 0.22363299200356998\n",
      " loss : [9.487505]\n",
      "episode: 1499/10000, steps: 652, epsilon: 0.22340935901156642\n",
      " loss : [329.92987]\n",
      "episode: 1500/10000, steps: 1050, epsilon: 0.22318594965255484\n",
      " loss : [318.07108]\n",
      "episode: 1501/10000, steps: 707, epsilon: 0.22296276370290227\n",
      " loss : [293.67792]\n",
      "episode: 1502/10000, steps: 698, epsilon: 0.22273980093919937\n",
      " loss : [16.239273]\n",
      "episode: 1503/10000, steps: 646, epsilon: 0.22251706113826017\n",
      " loss : [32.1784]\n",
      "episode: 1504/10000, steps: 1209, epsilon: 0.2222945440771219\n",
      " loss : [18.157082]\n",
      "episode: 1505/10000, steps: 644, epsilon: 0.22207224953304477\n",
      " loss : [15.60867]\n",
      "episode: 1506/10000, steps: 948, epsilon: 0.22185017728351172\n",
      " loss : [18.355793]\n",
      "episode: 1507/10000, steps: 647, epsilon: 0.2216283271062282\n",
      " loss : [294.11066]\n",
      "episode: 1508/10000, steps: 652, epsilon: 0.22140669877912197\n",
      " loss : [16.160078]\n",
      "episode: 1509/10000, steps: 1094, epsilon: 0.22118529208034285\n",
      " loss : [26.943132]\n",
      "episode: 1510/10000, steps: 903, epsilon: 0.2209641067882625\n",
      " loss : [23.193975]\n",
      "episode: 1511/10000, steps: 374, epsilon: 0.22074314268147424\n",
      " loss : [10.219963]\n",
      "episode: 1512/10000, steps: 644, epsilon: 0.22052239953879277\n",
      " loss : [4.807685]\n",
      "episode: 1513/10000, steps: 686, epsilon: 0.22030187713925398\n",
      " loss : [297.16467]\n",
      "episode: 1514/10000, steps: 641, epsilon: 0.22008157526211472\n",
      " loss : [245.71227]\n",
      "episode: 1515/10000, steps: 671, epsilon: 0.2198614936868526\n",
      " loss : [15.978423]\n",
      "episode: 1516/10000, steps: 615, epsilon: 0.21964163219316574\n",
      " loss : [47.949917]\n",
      "episode: 1517/10000, steps: 654, epsilon: 0.21942199056097256\n",
      " loss : [25.81177]\n",
      "episode: 1518/10000, steps: 636, epsilon: 0.2192025685704116\n",
      " loss : [13.61994]\n",
      "episode: 1519/10000, steps: 675, epsilon: 0.21898336600184118\n",
      " loss : [7.2239695]\n",
      "episode: 1520/10000, steps: 1133, epsilon: 0.21876438263583933\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " loss : [22.183989]\n",
      "episode: 1521/10000, steps: 1167, epsilon: 0.21854561825320348\n",
      " loss : [12.851026]\n",
      "episode: 1522/10000, steps: 1260, epsilon: 0.21832707263495027\n",
      " loss : [43.922485]\n",
      "episode: 1523/10000, steps: 558, epsilon: 0.21810874556231533\n",
      " loss : [341.85547]\n",
      "episode: 1524/10000, steps: 619, epsilon: 0.217890636816753\n",
      " loss : [38.034466]\n",
      "episode: 1525/10000, steps: 1116, epsilon: 0.21767274617993623\n",
      " loss : [17.607824]\n",
      "episode: 1526/10000, steps: 1121, epsilon: 0.2174550734337563\n",
      " loss : [21.442184]\n",
      "episode: 1527/10000, steps: 971, epsilon: 0.21723761836032254\n",
      " loss : [18.23622]\n",
      "episode: 1528/10000, steps: 624, epsilon: 0.21702038074196223\n",
      " loss : [20.00313]\n",
      "episode: 1529/10000, steps: 510, epsilon: 0.21680336036122028\n",
      " loss : [336.9256]\n",
      "episode: 1530/10000, steps: 618, epsilon: 0.21658655700085905\n",
      " loss : [5.6384478]\n",
      "episode: 1531/10000, steps: 1114, epsilon: 0.2163699704438582\n",
      " loss : [305.2]\n",
      "episode: 1532/10000, steps: 630, epsilon: 0.21615360047341434\n",
      " loss : [265.50922]\n",
      "episode: 1533/10000, steps: 1115, epsilon: 0.21593744687294092\n",
      " loss : [29.620779]\n",
      "episode: 1534/10000, steps: 1112, epsilon: 0.21572150942606796\n",
      " loss : [237.36809]\n",
      "episode: 1535/10000, steps: 1122, epsilon: 0.2155057879166419\n",
      " loss : [31.936007]\n",
      "episode: 1536/10000, steps: 1136, epsilon: 0.21529028212872525\n",
      " loss : [45.489193]\n",
      "episode: 1537/10000, steps: 1125, epsilon: 0.21507499184659654\n",
      " loss : [17.33577]\n",
      "episode: 1538/10000, steps: 958, epsilon: 0.21485991685474995\n",
      " loss : [17.270529]\n",
      "episode: 1539/10000, steps: 953, epsilon: 0.2146450569378952\n",
      " loss : [272.79846]\n",
      "episode: 1540/10000, steps: 1112, epsilon: 0.21443041188095732\n",
      " loss : [29.415552]\n",
      "episode: 1541/10000, steps: 528, epsilon: 0.21421598146907636\n",
      " loss : [267.10327]\n",
      "episode: 1542/10000, steps: 1117, epsilon: 0.21400176548760727\n",
      " loss : [37.598873]\n",
      "episode: 1543/10000, steps: 1141, epsilon: 0.21378776372211966\n",
      " loss : [288.33545]\n",
      "episode: 1544/10000, steps: 633, epsilon: 0.21357397595839755\n",
      " loss : [245.99986]\n",
      "episode: 1545/10000, steps: 1346, epsilon: 0.21336040198243916\n",
      " loss : [46.6023]\n",
      "episode: 1546/10000, steps: 1107, epsilon: 0.21314704158045672\n",
      " loss : [11.875368]\n",
      "episode: 1547/10000, steps: 949, epsilon: 0.21293389453887626\n",
      " loss : [15.392559]\n",
      "episode: 1548/10000, steps: 550, epsilon: 0.21272096064433738\n",
      " loss : [511.18243]\n",
      "episode: 1549/10000, steps: 686, epsilon: 0.21250823968369303\n",
      " loss : [223.9742]\n",
      "episode: 1550/10000, steps: 673, epsilon: 0.21229573144400934\n",
      " loss : [190.48506]\n",
      "episode: 1551/10000, steps: 968, epsilon: 0.21208343571256533\n",
      " loss : [51.430977]\n",
      "episode: 1552/10000, steps: 655, epsilon: 0.21187135227685275\n",
      " loss : [17.837278]\n",
      "episode: 1553/10000, steps: 1139, epsilon: 0.2116594809245759\n",
      " loss : [21.344303]\n",
      "episode: 1554/10000, steps: 519, epsilon: 0.21144782144365132\n",
      " loss : [31.072897]\n",
      "episode: 1555/10000, steps: 531, epsilon: 0.21123637362220768\n",
      " loss : [27.898937]\n",
      "episode: 1556/10000, steps: 473, epsilon: 0.21102513724858546\n",
      " loss : [45.326103]\n",
      "episode: 1557/10000, steps: 978, epsilon: 0.21081411211133688\n",
      " loss : [36.108517]\n",
      "episode: 1558/10000, steps: 445, epsilon: 0.21060329799922556\n",
      " loss : [31.928246]\n",
      "episode: 1559/10000, steps: 630, epsilon: 0.21039269470122635\n",
      " loss : [21.784157]\n",
      "episode: 1560/10000, steps: 663, epsilon: 0.21018230200652513\n",
      " loss : [31.065432]\n",
      "episode: 1561/10000, steps: 647, epsilon: 0.2099721197045186\n",
      " loss : [13.800681]\n",
      "episode: 1562/10000, steps: 548, epsilon: 0.20976214758481407\n",
      " loss : [30.938515]\n",
      "episode: 1563/10000, steps: 339, epsilon: 0.20955238543722926\n",
      " loss : [38.605724]\n",
      "episode: 1564/10000, steps: 605, epsilon: 0.20934283305179202\n",
      " loss : [20.246677]\n",
      "episode: 1565/10000, steps: 345, epsilon: 0.20913349021874023\n",
      " loss : [45.505424]\n",
      "episode: 1566/10000, steps: 526, epsilon: 0.2089243567285215\n",
      " loss : [22.381514]\n",
      "episode: 1567/10000, steps: 364, epsilon: 0.20871543237179296\n",
      " loss : [16.09639]\n",
      "episode: 1568/10000, steps: 628, epsilon: 0.20850671693942116\n",
      " loss : [21.955122]\n",
      "episode: 1569/10000, steps: 442, epsilon: 0.20829821022248174\n",
      " loss : [36.519455]\n",
      "episode: 1570/10000, steps: 506, epsilon: 0.20808991201225926\n",
      " loss : [21.198595]\n",
      "episode: 1571/10000, steps: 387, epsilon: 0.207881822100247\n",
      " loss : [20.256031]\n",
      "episode: 1572/10000, steps: 755, epsilon: 0.20767394027814676\n",
      " loss : [19.902184]\n",
      "episode: 1573/10000, steps: 905, epsilon: 0.2074662663378686\n",
      " loss : [37.587906]\n",
      "episode: 1574/10000, steps: 499, epsilon: 0.20725880007153075\n",
      " loss : [30.524433]\n",
      "episode: 1575/10000, steps: 899, epsilon: 0.20705154127145922\n",
      " loss : [21.380373]\n",
      "episode: 1576/10000, steps: 593, epsilon: 0.20684448973018776\n",
      " loss : [29.323801]\n",
      "episode: 1577/10000, steps: 578, epsilon: 0.20663764524045758\n",
      " loss : [46.27378]\n",
      "episode: 1578/10000, steps: 488, epsilon: 0.20643100759521713\n",
      " loss : [34.995426]\n",
      "episode: 1579/10000, steps: 675, epsilon: 0.20622457658762192\n",
      " loss : [24.649954]\n",
      "episode: 1580/10000, steps: 558, epsilon: 0.2060183520110343\n",
      " loss : [25.134455]\n",
      "episode: 1581/10000, steps: 513, epsilon: 0.20581233365902327\n",
      " loss : [21.468761]\n",
      "episode: 1582/10000, steps: 593, epsilon: 0.20560652132536425\n",
      " loss : [14.201117]\n",
      "episode: 1583/10000, steps: 779, epsilon: 0.20540091480403888\n",
      " loss : [19.898863]\n",
      "episode: 1584/10000, steps: 521, epsilon: 0.20519551388923485\n",
      " loss : [17.908365]\n",
      "episode: 1585/10000, steps: 686, epsilon: 0.20499031837534562\n",
      " loss : [8.072662]\n",
      "episode: 1586/10000, steps: 763, epsilon: 0.20478532805697028\n",
      " loss : [11.986399]\n",
      "episode: 1587/10000, steps: 959, epsilon: 0.2045805427289133\n",
      " loss : [13.814704]\n",
      "episode: 1588/10000, steps: 425, epsilon: 0.20437596218618437\n",
      " loss : [12.106258]\n",
      "episode: 1589/10000, steps: 683, epsilon: 0.2041715862239982\n",
      " loss : [7.4933248]\n",
      "episode: 1590/10000, steps: 644, epsilon: 0.2039674146377742\n",
      " loss : [3.9863162]\n",
      "episode: 1591/10000, steps: 627, epsilon: 0.20376344722313644\n",
      " loss : [7.247322]\n",
      "episode: 1592/10000, steps: 403, epsilon: 0.2035596837759133\n",
      " loss : [10.64039]\n",
      "episode: 1593/10000, steps: 829, epsilon: 0.20335612409213738\n",
      " loss : [9.045626]\n",
      "episode: 1594/10000, steps: 657, epsilon: 0.20315276796804524\n",
      " loss : [4.566738]\n",
      "episode: 1595/10000, steps: 686, epsilon: 0.2029496152000772\n",
      " loss : [7.4520397]\n",
      "episode: 1596/10000, steps: 1171, epsilon: 0.20274666558487714\n",
      " loss : [5.2821074]\n",
      "episode: 1597/10000, steps: 347, epsilon: 0.20254391891929227\n",
      " loss : [5.2370977]\n",
      "episode: 1598/10000, steps: 662, epsilon: 0.20234137500037297\n",
      " loss : [11.172555]\n",
      "episode: 1599/10000, steps: 364, epsilon: 0.20213903362537258\n",
      " loss : [4.358799]\n",
      "episode: 1600/10000, steps: 356, epsilon: 0.2019368945917472\n",
      " loss : [3.1377864]\n",
      "episode: 1601/10000, steps: 732, epsilon: 0.20173495769715546\n",
      " loss : [9.317043]\n",
      "episode: 1602/10000, steps: 358, epsilon: 0.2015332227394583\n",
      " loss : [4.406347]\n",
      "episode: 1603/10000, steps: 705, epsilon: 0.20133168951671884\n",
      " loss : [9.039899]\n",
      "episode: 1604/10000, steps: 808, epsilon: 0.20113035782720212\n",
      " loss : [4.3291097]\n",
      "episode: 1605/10000, steps: 714, epsilon: 0.20092922746937492\n",
      " loss : [12.078204]\n",
      "episode: 1606/10000, steps: 670, epsilon: 0.20072829824190555\n",
      " loss : [4.8159738]\n",
      "episode: 1607/10000, steps: 921, epsilon: 0.20052756994366364\n",
      " loss : [7.611611]\n",
      "episode: 1608/10000, steps: 810, epsilon: 0.20032704237371998\n",
      " loss : [5.867831]\n",
      "episode: 1609/10000, steps: 813, epsilon: 0.20012671533134627\n",
      " loss : [8.731652]\n",
      "episode: 1610/10000, steps: 1458, epsilon: 0.19992658861601492\n",
      " loss : [3.136865]\n",
      "episode: 1611/10000, steps: 769, epsilon: 0.19992658861601492\n",
      " loss : [8.651942]\n",
      "episode: 1612/10000, steps: 937, epsilon: 0.19992658861601492\n",
      " loss : [8.98479]\n",
      "episode: 1613/10000, steps: 737, epsilon: 0.19992658861601492\n",
      " loss : [223.01767]\n",
      "episode: 1614/10000, steps: 660, epsilon: 0.19992658861601492\n",
      " loss : [7.5339293]\n",
      "episode: 1615/10000, steps: 805, epsilon: 0.19992658861601492\n",
      " loss : [15.839808]\n",
      "episode: 1616/10000, steps: 825, epsilon: 0.19992658861601492\n",
      " loss : [19.648232]\n",
      "episode: 1617/10000, steps: 806, epsilon: 0.19992658861601492\n",
      " loss : [17.184216]\n",
      "episode: 1618/10000, steps: 1417, epsilon: 0.19992658861601492\n",
      " loss : [12.576607]\n",
      "episode: 1619/10000, steps: 745, epsilon: 0.19992658861601492\n",
      " loss : [6.136322]\n",
      "episode: 1620/10000, steps: 817, epsilon: 0.19992658861601492\n",
      " loss : [3.7336]\n",
      "episode: 1621/10000, steps: 825, epsilon: 0.19992658861601492\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " loss : [150.46375]\n",
      "episode: 1622/10000, steps: 358, epsilon: 0.19992658861601492\n",
      " loss : [9.877317]\n",
      "episode: 1623/10000, steps: 377, epsilon: 0.19992658861601492\n",
      " loss : [7.00169]\n",
      "episode: 1624/10000, steps: 808, epsilon: 0.19992658861601492\n",
      " loss : [12.145598]\n",
      "episode: 1625/10000, steps: 413, epsilon: 0.19992658861601492\n",
      " loss : [8.081223]\n",
      "episode: 1626/10000, steps: 868, epsilon: 0.19992658861601492\n",
      " loss : [6.244005]\n",
      "episode: 1627/10000, steps: 652, epsilon: 0.19992658861601492\n",
      " loss : [10.70327]\n",
      "episode: 1628/10000, steps: 652, epsilon: 0.19992658861601492\n",
      " loss : [5.7465873]\n",
      "episode: 1629/10000, steps: 945, epsilon: 0.19992658861601492\n",
      " loss : [7.254288]\n",
      "episode: 1630/10000, steps: 380, epsilon: 0.19992658861601492\n",
      " loss : [5.5750284]\n",
      "episode: 1631/10000, steps: 392, epsilon: 0.19992658861601492\n",
      " loss : [18.371912]\n",
      "episode: 1632/10000, steps: 731, epsilon: 0.19992658861601492\n",
      " loss : [12.811673]\n",
      "episode: 1633/10000, steps: 773, epsilon: 0.19992658861601492\n",
      " loss : [8.89652]\n",
      "episode: 1634/10000, steps: 795, epsilon: 0.19992658861601492\n",
      " loss : [2.9428394]\n",
      "episode: 1635/10000, steps: 820, epsilon: 0.19992658861601492\n",
      " loss : [8.704081]\n",
      "episode: 1636/10000, steps: 1009, epsilon: 0.19992658861601492\n",
      " loss : [3.9447906]\n",
      "episode: 1637/10000, steps: 716, epsilon: 0.19992658861601492\n",
      " loss : [11.747063]\n",
      "episode: 1638/10000, steps: 822, epsilon: 0.19992658861601492\n",
      " loss : [4.2154994]\n",
      "episode: 1639/10000, steps: 1199, epsilon: 0.19992658861601492\n",
      " loss : [10.775308]\n",
      "episode: 1640/10000, steps: 795, epsilon: 0.19992658861601492\n",
      " loss : [17.59905]\n",
      "episode: 1641/10000, steps: 633, epsilon: 0.19992658861601492\n",
      " loss : [16.147062]\n",
      "episode: 1642/10000, steps: 642, epsilon: 0.19992658861601492\n",
      " loss : [7.545294]\n",
      "episode: 1643/10000, steps: 1116, epsilon: 0.19992658861601492\n",
      " loss : [38.74102]\n",
      "episode: 1644/10000, steps: 1637, epsilon: 0.19992658861601492\n",
      " loss : [13.326882]\n",
      "episode: 1645/10000, steps: 380, epsilon: 0.19992658861601492\n",
      " loss : [22.161104]\n",
      "episode: 1646/10000, steps: 767, epsilon: 0.19992658861601492\n",
      " loss : [8.880999]\n",
      "episode: 1647/10000, steps: 655, epsilon: 0.19992658861601492\n",
      " loss : [21.62263]\n",
      "episode: 1648/10000, steps: 810, epsilon: 0.19992658861601492\n",
      " loss : [15.74364]\n",
      "episode: 1649/10000, steps: 889, epsilon: 0.19992658861601492\n",
      " loss : [9.287793]\n",
      "episode: 1650/10000, steps: 1476, epsilon: 0.19992658861601492\n",
      " loss : [5.2492814]\n",
      "episode: 1651/10000, steps: 835, epsilon: 0.19992658861601492\n",
      " loss : [10.08231]\n",
      "episode: 1652/10000, steps: 948, epsilon: 0.19992658861601492\n",
      " loss : [13.868933]\n",
      "episode: 1653/10000, steps: 840, epsilon: 0.19992658861601492\n",
      " loss : [10.284385]\n",
      "episode: 1654/10000, steps: 787, epsilon: 0.19992658861601492\n",
      " loss : [5.123669]\n",
      "episode: 1655/10000, steps: 852, epsilon: 0.19992658861601492\n",
      " loss : [5.8458886]\n",
      "episode: 1656/10000, steps: 848, epsilon: 0.19992658861601492\n",
      " loss : [5.844885]\n",
      "episode: 1657/10000, steps: 808, epsilon: 0.19992658861601492\n",
      " loss : [6.017797]\n",
      "episode: 1658/10000, steps: 965, epsilon: 0.19992658861601492\n",
      " loss : [5.8430753]\n",
      "episode: 1659/10000, steps: 815, epsilon: 0.19992658861601492\n",
      " loss : [3.1022666]\n",
      "episode: 1660/10000, steps: 968, epsilon: 0.19992658861601492\n",
      " loss : [3.8514268]\n",
      "episode: 1661/10000, steps: 873, epsilon: 0.19992658861601492\n",
      " loss : [4.9297066]\n",
      "episode: 1662/10000, steps: 814, epsilon: 0.19992658861601492\n",
      " loss : [1.9385378]\n",
      "episode: 1663/10000, steps: 832, epsilon: 0.19992658861601492\n",
      " loss : [2.4941862]\n",
      "episode: 1664/10000, steps: 808, epsilon: 0.19992658861601492\n",
      " loss : [2.4309044]\n",
      "episode: 1665/10000, steps: 951, epsilon: 0.19992658861601492\n",
      " loss : [4.307095]\n",
      "episode: 1666/10000, steps: 799, epsilon: 0.19992658861601492\n",
      " loss : [1.9045144]\n",
      "episode: 1667/10000, steps: 809, epsilon: 0.19992658861601492\n",
      " loss : [3.0704386]\n",
      "episode: 1668/10000, steps: 833, epsilon: 0.19992658861601492\n",
      " loss : [151.27489]\n",
      "episode: 1669/10000, steps: 742, epsilon: 0.19992658861601492\n",
      " loss : [8.482254]\n",
      "episode: 1670/10000, steps: 801, epsilon: 0.19992658861601492\n",
      " loss : [3.0899732]\n",
      "episode: 1671/10000, steps: 827, epsilon: 0.19992658861601492\n",
      " loss : [6.384292]\n",
      "episode: 1672/10000, steps: 815, epsilon: 0.19992658861601492\n",
      " loss : [5.995566]\n",
      "episode: 1673/10000, steps: 844, epsilon: 0.19992658861601492\n",
      " loss : [5.2596755]\n",
      "episode: 1674/10000, steps: 910, epsilon: 0.19992658861601492\n",
      " loss : [1.7921981]\n",
      "episode: 1675/10000, steps: 815, epsilon: 0.19992658861601492\n",
      " loss : [7.7431946]\n",
      "episode: 1676/10000, steps: 860, epsilon: 0.19992658861601492\n",
      " loss : [29.56068]\n",
      "episode: 1677/10000, steps: 839, epsilon: 0.19992658861601492\n",
      " loss : [10.032547]\n",
      "episode: 1678/10000, steps: 840, epsilon: 0.19992658861601492\n",
      " loss : [9.528581]\n",
      "episode: 1679/10000, steps: 677, epsilon: 0.19992658861601492\n",
      " loss : [10.109619]\n",
      "episode: 1680/10000, steps: 818, epsilon: 0.19992658861601492\n",
      " loss : [3.5295172]\n",
      "episode: 1681/10000, steps: 810, epsilon: 0.19992658861601492\n",
      " loss : [3.7002478]\n",
      "episode: 1682/10000, steps: 2116, epsilon: 0.19992658861601492\n",
      " loss : [10.64553]\n",
      "episode: 1683/10000, steps: 777, epsilon: 0.19992658861601492\n",
      " loss : [10.254862]\n",
      "episode: 1684/10000, steps: 822, epsilon: 0.19992658861601492\n",
      " loss : [3.3067663]\n",
      "episode: 1685/10000, steps: 785, epsilon: 0.19992658861601492\n",
      " loss : [29.595816]\n",
      "episode: 1686/10000, steps: 657, epsilon: 0.19992658861601492\n",
      " loss : [10.101009]\n",
      "episode: 1687/10000, steps: 384, epsilon: 0.19992658861601492\n",
      " loss : [35.26406]\n",
      "episode: 1688/10000, steps: 429, epsilon: 0.19992658861601492\n",
      " loss : [34.813683]\n",
      "episode: 1689/10000, steps: 796, epsilon: 0.19992658861601492\n",
      " loss : [7.5973306]\n",
      "episode: 1690/10000, steps: 876, epsilon: 0.19992658861601492\n",
      " loss : [5.5270915]\n",
      "episode: 1691/10000, steps: 873, epsilon: 0.19992658861601492\n",
      " loss : [4.7970476]\n",
      "episode: 1692/10000, steps: 879, epsilon: 0.19992658861601492\n",
      " loss : [13.271454]\n",
      "episode: 1693/10000, steps: 812, epsilon: 0.19992658861601492\n",
      " loss : [8.097008]\n",
      "episode: 1694/10000, steps: 1101, epsilon: 0.19992658861601492\n",
      " loss : [9.311691]\n",
      "episode: 1695/10000, steps: 874, epsilon: 0.19992658861601492\n",
      " loss : [15.762756]\n",
      "episode: 1696/10000, steps: 793, epsilon: 0.19992658861601492\n",
      " loss : [7.886951]\n",
      "episode: 1697/10000, steps: 663, epsilon: 0.19992658861601492\n",
      " loss : [24.773354]\n",
      "episode: 1698/10000, steps: 794, epsilon: 0.19992658861601492\n",
      " loss : [6.1694374]\n",
      "episode: 1699/10000, steps: 824, epsilon: 0.19992658861601492\n",
      " loss : [4.463212]\n",
      "episode: 1700/10000, steps: 875, epsilon: 0.19992658861601492\n",
      " loss : [5.6070895]\n",
      "episode: 1701/10000, steps: 807, epsilon: 0.19992658861601492\n",
      " loss : [7.9949913]\n",
      "episode: 1702/10000, steps: 832, epsilon: 0.19992658861601492\n",
      " loss : [5.183024]\n",
      "episode: 1703/10000, steps: 825, epsilon: 0.19992658861601492\n",
      " loss : [3.3846507]\n",
      "episode: 1704/10000, steps: 882, epsilon: 0.19992658861601492\n",
      " loss : [10.084425]\n",
      "episode: 1705/10000, steps: 862, epsilon: 0.19992658861601492\n",
      " loss : [7.0941143]\n",
      "episode: 1706/10000, steps: 892, epsilon: 0.19992658861601492\n",
      " loss : [5.630731]\n",
      "episode: 1707/10000, steps: 1445, epsilon: 0.19992658861601492\n",
      " loss : [37.63547]\n",
      "episode: 1708/10000, steps: 799, epsilon: 0.19992658861601492\n",
      " loss : [16.32524]\n",
      "episode: 1709/10000, steps: 916, epsilon: 0.19992658861601492\n",
      " loss : [58.673634]\n",
      "episode: 1710/10000, steps: 812, epsilon: 0.19992658861601492\n",
      " loss : [50.285686]\n",
      "episode: 1711/10000, steps: 688, epsilon: 0.19992658861601492\n",
      " loss : [5.0032535]\n",
      "episode: 1712/10000, steps: 832, epsilon: 0.19992658861601492\n",
      " loss : [26.870312]\n",
      "episode: 1713/10000, steps: 818, epsilon: 0.19992658861601492\n",
      " loss : [11.378688]\n",
      "episode: 1714/10000, steps: 795, epsilon: 0.19992658861601492\n",
      " loss : [15.189139]\n",
      "episode: 1715/10000, steps: 1418, epsilon: 0.19992658861601492\n",
      " loss : [19.430872]\n",
      "episode: 1716/10000, steps: 824, epsilon: 0.19992658861601492\n",
      " loss : [11.248692]\n",
      "episode: 1717/10000, steps: 1013, epsilon: 0.19992658861601492\n",
      " loss : [17.812256]\n",
      "episode: 1718/10000, steps: 485, epsilon: 0.19992658861601492\n",
      " loss : [3.1723168]\n",
      "episode: 1719/10000, steps: 796, epsilon: 0.19992658861601492\n",
      " loss : [11.795888]\n",
      "episode: 1720/10000, steps: 661, epsilon: 0.19992658861601492\n",
      " loss : [7.109703]\n",
      "episode: 1721/10000, steps: 1372, epsilon: 0.19992658861601492\n",
      " loss : [8.365797]\n",
      "episode: 1722/10000, steps: 624, epsilon: 0.19992658861601492\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " loss : [5.940935]\n",
      "episode: 1723/10000, steps: 899, epsilon: 0.19992658861601492\n",
      " loss : [10.06136]\n",
      "episode: 1724/10000, steps: 785, epsilon: 0.19992658861601492\n",
      " loss : [6.878736]\n",
      "episode: 1725/10000, steps: 1874, epsilon: 0.19992658861601492\n",
      " loss : [11.116806]\n",
      "episode: 1726/10000, steps: 812, epsilon: 0.19992658861601492\n",
      " loss : [11.328137]\n",
      "episode: 1727/10000, steps: 802, epsilon: 0.19992658861601492\n",
      " loss : [5.459611]\n",
      "episode: 1728/10000, steps: 678, epsilon: 0.19992658861601492\n",
      " loss : [93.60383]\n",
      "episode: 1729/10000, steps: 1480, epsilon: 0.19992658861601492\n",
      " loss : [23.758724]\n",
      "episode: 1730/10000, steps: 1076, epsilon: 0.19992658861601492\n",
      " loss : [16.247162]\n",
      "episode: 1731/10000, steps: 813, epsilon: 0.19992658861601492\n",
      " loss : [167.6563]\n",
      "episode: 1732/10000, steps: 854, epsilon: 0.19992658861601492\n",
      " loss : [6.035251]\n",
      "episode: 1733/10000, steps: 979, epsilon: 0.19992658861601492\n",
      " loss : [7.2121778]\n",
      "episode: 1734/10000, steps: 512, epsilon: 0.19992658861601492\n",
      " loss : [129.92989]\n",
      "episode: 1735/10000, steps: 1388, epsilon: 0.19992658861601492\n",
      " loss : [6.0711555]\n",
      "episode: 1736/10000, steps: 856, epsilon: 0.19992658861601492\n",
      " loss : [12.816464]\n",
      "episode: 1737/10000, steps: 863, epsilon: 0.19992658861601492\n",
      " loss : [31.856852]\n",
      "episode: 1738/10000, steps: 838, epsilon: 0.19992658861601492\n",
      " loss : [13.604438]\n",
      "episode: 1739/10000, steps: 804, epsilon: 0.19992658861601492\n",
      " loss : [96.746925]\n",
      "episode: 1740/10000, steps: 986, epsilon: 0.19992658861601492\n",
      " loss : [14.850161]\n",
      "episode: 1741/10000, steps: 835, epsilon: 0.19992658861601492\n",
      " loss : [7.708644]\n",
      "episode: 1742/10000, steps: 661, epsilon: 0.19992658861601492\n",
      " loss : [7.071096]\n",
      "episode: 1743/10000, steps: 770, epsilon: 0.19992658861601492\n",
      " loss : [22.138718]\n",
      "episode: 1744/10000, steps: 828, epsilon: 0.19992658861601492\n",
      " loss : [6.448456]\n",
      "episode: 1745/10000, steps: 804, epsilon: 0.19992658861601492\n",
      " loss : [21.541384]\n",
      "episode: 1746/10000, steps: 953, epsilon: 0.19992658861601492\n",
      " loss : [14.38301]\n",
      "episode: 1747/10000, steps: 665, epsilon: 0.19992658861601492\n",
      " loss : [45.27025]\n",
      "episode: 1748/10000, steps: 832, epsilon: 0.19992658861601492\n",
      " loss : [8.095515]\n",
      "episode: 1749/10000, steps: 954, epsilon: 0.19992658861601492\n",
      " loss : [24.761497]\n",
      "episode: 1750/10000, steps: 1176, epsilon: 0.19992658861601492\n",
      " loss : [13.762298]\n",
      "episode: 1751/10000, steps: 916, epsilon: 0.19992658861601492\n",
      " loss : [6.3201675]\n",
      "episode: 1752/10000, steps: 884, epsilon: 0.19992658861601492\n",
      " loss : [14.151124]\n",
      "episode: 1753/10000, steps: 949, epsilon: 0.19992658861601492\n",
      " loss : [282.35745]\n",
      "episode: 1754/10000, steps: 350, epsilon: 0.19992658861601492\n",
      " loss : [14.013832]\n",
      "episode: 1755/10000, steps: 1517, epsilon: 0.19992658861601492\n",
      " loss : [19.392744]\n",
      "episode: 1756/10000, steps: 899, epsilon: 0.19992658861601492\n",
      " loss : [5.295958]\n",
      "episode: 1757/10000, steps: 1211, epsilon: 0.19992658861601492\n",
      " loss : [31.757893]\n",
      "episode: 1758/10000, steps: 501, epsilon: 0.19992658861601492\n",
      " loss : [14.158678]\n",
      "episode: 1759/10000, steps: 1114, epsilon: 0.19992658861601492\n",
      " loss : [9.795832]\n",
      "episode: 1760/10000, steps: 640, epsilon: 0.19992658861601492\n",
      " loss : [6.7757144]\n",
      "episode: 1761/10000, steps: 910, epsilon: 0.19992658861601492\n",
      " loss : [331.37997]\n",
      "episode: 1762/10000, steps: 447, epsilon: 0.19992658861601492\n",
      " loss : [15.486849]\n",
      "episode: 1763/10000, steps: 1154, epsilon: 0.19992658861601492\n",
      " loss : [8.804245]\n",
      "episode: 1764/10000, steps: 817, epsilon: 0.19992658861601492\n",
      " loss : [10.284132]\n",
      "episode: 1765/10000, steps: 640, epsilon: 0.19992658861601492\n",
      " loss : [307.41095]\n",
      "episode: 1766/10000, steps: 1163, epsilon: 0.19992658861601492\n",
      " loss : [31.79289]\n",
      "episode: 1767/10000, steps: 343, epsilon: 0.19992658861601492\n",
      " loss : [17.486565]\n",
      "episode: 1768/10000, steps: 633, epsilon: 0.19992658861601492\n",
      " loss : [7.022937]\n",
      "episode: 1769/10000, steps: 638, epsilon: 0.19992658861601492\n",
      " loss : [10.1424055]\n",
      "episode: 1770/10000, steps: 1085, epsilon: 0.19992658861601492\n",
      " loss : [12.894768]\n",
      "episode: 1771/10000, steps: 629, epsilon: 0.19992658861601492\n",
      " loss : [8.768812]\n",
      "episode: 1772/10000, steps: 370, epsilon: 0.19992658861601492\n",
      " loss : [9.154139]\n",
      "episode: 1773/10000, steps: 517, epsilon: 0.19992658861601492\n",
      " loss : [349.80725]\n",
      "episode: 1774/10000, steps: 897, epsilon: 0.19992658861601492\n",
      " loss : [21.763376]\n",
      "episode: 1775/10000, steps: 613, epsilon: 0.19992658861601492\n",
      " loss : [317.67786]\n",
      "episode: 1776/10000, steps: 709, epsilon: 0.19992658861601492\n",
      " loss : [274.8907]\n",
      "episode: 1777/10000, steps: 1078, epsilon: 0.19992658861601492\n",
      " loss : [27.562773]\n",
      "episode: 1778/10000, steps: 662, epsilon: 0.19992658861601492\n",
      " loss : [23.362488]\n",
      "episode: 1779/10000, steps: 371, epsilon: 0.19992658861601492\n",
      " loss : [23.487532]\n",
      "episode: 1780/10000, steps: 931, epsilon: 0.19992658861601492\n",
      " loss : [20.434917]\n",
      "episode: 1781/10000, steps: 587, epsilon: 0.19992658861601492\n",
      " loss : [14.691509]\n",
      "episode: 1782/10000, steps: 933, epsilon: 0.19992658861601492\n",
      " loss : [7.112643]\n",
      "episode: 1783/10000, steps: 826, epsilon: 0.19992658861601492\n",
      " loss : [8.248786]\n",
      "episode: 1784/10000, steps: 948, epsilon: 0.19992658861601492\n",
      " loss : [10.562109]\n",
      "episode: 1785/10000, steps: 512, epsilon: 0.19992658861601492\n",
      " loss : [9.040593]\n",
      "episode: 1786/10000, steps: 380, epsilon: 0.19992658861601492\n",
      " loss : [3.8208468]\n",
      "episode: 1787/10000, steps: 1162, epsilon: 0.19992658861601492\n",
      " loss : [13.525128]\n",
      "episode: 1788/10000, steps: 640, epsilon: 0.19992658861601492\n",
      " loss : [9.684697]\n",
      "episode: 1789/10000, steps: 603, epsilon: 0.19992658861601492\n",
      " loss : [7.767078]\n",
      "episode: 1790/10000, steps: 353, epsilon: 0.19992658861601492\n",
      " loss : [4.8777366]\n",
      "episode: 1791/10000, steps: 645, epsilon: 0.19992658861601492\n",
      " loss : [9.427179]\n",
      "episode: 1792/10000, steps: 905, epsilon: 0.19992658861601492\n",
      " loss : [15.149891]\n",
      "episode: 1793/10000, steps: 656, epsilon: 0.19992658861601492\n",
      " loss : [16.386381]\n",
      "episode: 1794/10000, steps: 408, epsilon: 0.19992658861601492\n",
      " loss : [12.303032]\n",
      "episode: 1795/10000, steps: 822, epsilon: 0.19992658861601492\n",
      " loss : [12.72054]\n",
      "episode: 1796/10000, steps: 768, epsilon: 0.19992658861601492\n",
      " loss : [3.9929874]\n",
      "episode: 1797/10000, steps: 648, epsilon: 0.19992658861601492\n",
      " loss : [2.4904754]\n",
      "episode: 1798/10000, steps: 774, epsilon: 0.19992658861601492\n",
      " loss : [17.65773]\n",
      "episode: 1799/10000, steps: 921, epsilon: 0.19992658861601492\n",
      " loss : [7.1553802]\n",
      "episode: 1800/10000, steps: 495, epsilon: 0.19992658861601492\n",
      " loss : [11.4767275]\n",
      "episode: 1801/10000, steps: 801, epsilon: 0.19992658861601492\n",
      " loss : [10.483179]\n",
      "episode: 1802/10000, steps: 924, epsilon: 0.19992658861601492\n",
      " loss : [5.7301784]\n",
      "episode: 1803/10000, steps: 1263, epsilon: 0.19992658861601492\n",
      " loss : [71.220665]\n",
      "episode: 1804/10000, steps: 1421, epsilon: 0.19992658861601492\n",
      " loss : [10.959181]\n",
      "episode: 1805/10000, steps: 585, epsilon: 0.19992658861601492\n",
      " loss : [9.736385]\n",
      "episode: 1806/10000, steps: 840, epsilon: 0.19992658861601492\n",
      " loss : [8.040762]\n",
      "episode: 1807/10000, steps: 852, epsilon: 0.19992658861601492\n",
      " loss : [14.29814]\n",
      "episode: 1808/10000, steps: 643, epsilon: 0.19992658861601492\n",
      " loss : [17.37691]\n",
      "episode: 1809/10000, steps: 710, epsilon: 0.19992658861601492\n",
      " loss : [9.159891]\n",
      "episode: 1810/10000, steps: 803, epsilon: 0.19992658861601492\n",
      " loss : [13.80168]\n",
      "episode: 1811/10000, steps: 784, epsilon: 0.19992658861601492\n",
      " loss : [17.618423]\n",
      "episode: 1812/10000, steps: 1421, epsilon: 0.19992658861601492\n",
      " loss : [13.246119]\n",
      "episode: 1813/10000, steps: 563, epsilon: 0.19992658861601492\n",
      " loss : [14.995886]\n",
      "episode: 1814/10000, steps: 379, epsilon: 0.19992658861601492\n",
      " loss : [12.169296]\n",
      "episode: 1815/10000, steps: 837, epsilon: 0.19992658861601492\n",
      " loss : [7.7822423]\n",
      "episode: 1816/10000, steps: 655, epsilon: 0.19992658861601492\n",
      " loss : [14.201779]\n",
      "episode: 1817/10000, steps: 1409, epsilon: 0.19992658861601492\n",
      " loss : [7.3764825]\n",
      "episode: 1818/10000, steps: 753, epsilon: 0.19992658861601492\n",
      " loss : [9.881236]\n",
      "episode: 1819/10000, steps: 401, epsilon: 0.19992658861601492\n",
      " loss : [9.268282]\n",
      "episode: 1820/10000, steps: 796, epsilon: 0.19992658861601492\n",
      " loss : [9.560187]\n",
      "episode: 1821/10000, steps: 770, epsilon: 0.19992658861601492\n",
      " loss : [6.700516]\n",
      "episode: 1822/10000, steps: 1353, epsilon: 0.19992658861601492\n",
      " loss : [12.408746]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1823/10000, steps: 427, epsilon: 0.19992658861601492\n",
      " loss : [9.879454]\n",
      "episode: 1824/10000, steps: 692, epsilon: 0.19992658861601492\n",
      " loss : [11.322287]\n",
      "episode: 1825/10000, steps: 819, epsilon: 0.19992658861601492\n",
      " loss : [12.075897]\n",
      "episode: 1826/10000, steps: 706, epsilon: 0.19992658861601492\n",
      " loss : [8.224628]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-ed5c0f10980d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m             \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m             \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m             \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-95540ed498a7>\u001b[0m in \u001b[0;36mchoice_action\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     74\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_env\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m             \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mZ:\\anaconda3\\anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1623\u001b[0m       \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_predict_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1624\u001b[0m       \u001b[0mbatch_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1625\u001b[1;33m       \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menumerate_epochs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Single epoch.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1626\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcatch_stop_iteration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1627\u001b[0m           \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mZ:\\anaconda3\\anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36menumerate_epochs\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1131\u001b[0m     \u001b[1;34m\"\"\"Yields `(epoch, tf.data.Iterator)`.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_truncate_execution_to_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1133\u001b[1;33m       \u001b[0mdata_iterator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1134\u001b[0m       \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initial_epoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1135\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_insufficient_data\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Set by `catch_stop_iteration`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mZ:\\anaconda3\\anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    420\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minside_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    421\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 422\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOwnedIterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    423\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    424\u001b[0m       raise RuntimeError(\"__iter__() is only supported inside of tf.function \"\n",
      "\u001b[1;32mZ:\\anaconda3\\anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[0;32m    680\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mcomponents\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0melement_spec\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    681\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_message\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 682\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    683\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    684\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_create_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mZ:\\anaconda3\\anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36m_create_iterator\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    703\u001b[0m               \u001b[0moutput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m               output_shapes=self._flat_output_shapes))\n\u001b[1;32m--> 705\u001b[1;33m       \u001b[0mgen_dataset_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mds_variant\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    706\u001b[0m       \u001b[1;31m# Delete the resource when this object is deleted\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    707\u001b[0m       self._resource_deleter = IteratorResourceDeleter(\n",
      "\u001b[1;32mZ:\\anaconda3\\anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py\u001b[0m in \u001b[0;36mmake_iterator\u001b[1;34m(dataset, iterator, name)\u001b[0m\n\u001b[0;32m   2969\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2970\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[1;32m-> 2971\u001b[1;33m         _ctx, \"MakeIterator\", name, dataset, iterator)\n\u001b[0m\u001b[0;32m   2972\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2973\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    env = gym.make('SpaceInvaders-v0')\n",
    "    agent = DQNAgent(env)\n",
    "\n",
    "    episodes = 10000\n",
    "\n",
    "    all_loss = []\n",
    "    for e in range(episodes):\n",
    "        state = env.reset()\n",
    "        state = preprocess(state)\n",
    "\n",
    "        all_rewards = 0\n",
    "\n",
    "        for time_t in range(5000):\n",
    "\n",
    "            env.render()\n",
    "\n",
    "            action = agent.choice_action(state)\n",
    "\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            next_state = preprocess(next_state)\n",
    "\n",
    "            if reward != 0.0:\n",
    "                agent.save_memory(state, action, reward, next_state, done)\n",
    "\n",
    "            state = copy.deepcopy(next_state)\n",
    "\n",
    "            all_rewards += reward\n",
    "\n",
    "            if done:\n",
    "                print(\"episode: {}/{}, steps: {}, epsilon: {}\"\n",
    "                      .format(e + 1, episodes, time_t, agent.epsilon))\n",
    "                break\n",
    "\n",
    "        loss = agent.train()\n",
    "        all_loss.append(loss[0])\n",
    "\n",
    "    plt.plot(all_loss)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ready-species",
   "metadata": {},
   "source": [
    "# 6. Conclusion\n",
    "After about 12 hours' training, there are nearly 2000 episode results and we get several gif visualization (at epsidoe =250, 500, 1255, 1510, and 1740, see them in attached gifs).\n",
    "Through the analysis and comparison of the results, we can conclude that deep-q learning network has a certain effect to this game, because this agent can get higher score and performs better and better along with trainning. \n",
    "However, the difficulty of adjusting parameters is relatively high and the learning for this agent is slow and it still need much more hours to train to get idea outcomes.So deep-q learning is not the perfect model to SpaceInvaders game. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "intelligent-clarity",
   "metadata": {},
   "source": [
    "# 7. Questions Answers\n",
    "\n",
    "#1. Establish a baseline performance\n",
    "\n",
    "Implementation is showed above.\n",
    "\n",
    "In this performance:\n",
    "memory = deque(maxlen=128)\n",
    "gamma = 0.5  # Discounting rate\n",
    "epsilon = 1.0\n",
    "epsilon_decay = 0.999 # exponential decay rate for exploration prob\n",
    "epsilon_min = 0.2\n",
    "simple_size = 16 # action size\n",
    "input_shape=(None, 40, 40, 1) # state size\n",
    "total_episodes = 10000  \n",
    "\n",
    "The agent need much more time to train in order to get a perfect result. So this model is not very suitable for this game.\n",
    "\n",
    "#2.The states, the actions, and the size of the Q-table\n",
    "\n",
    "States: \n",
    "Whether the agent get strike from invaders or/and agent shoot invaders or flying bonus taget and how much score the agent earn from shooting invaders and bonus.\n",
    "\n",
    "Actions:\n",
    "The agent moves to shoot or dodge attack and choose to launch laser or not. \n",
    "\n",
    "Size:\n",
    "Q-table has a row for each possible state and a column for each possible action.\n",
    "states should have: shoot invader, shoot nothing, shoot bonus, get strike, only move, death\n",
    "action size=16\n",
    "So it is:16*6\n",
    "\n",
    "#3.The rewards\n",
    "\n",
    "In this SpaceInvaders game, the reward is the points which represents how many invaders the agent shoot and destroy and how many bonus the agent can shoot.\n",
    "The game's goal is to kill as many enemies as possible and can get bonus as well while trying the best to protect the agent itself by moving and get sheltered. Trying to shoot bonus flying at far is challenging, it can add difficulty and fun when trying to avoid invaders' attack. So set this reward can achive this game goal best so that to attact the players.\n",
    "\n",
    "#4.Alpha and gamma in the Bellman equation\n",
    "\n",
    "Alpha is the learning rate, set generally between 0 and 1. Setting the alpha value to 0 means that the Q-values are never updated, thereby nothing is learned. If we set the alpha to a high value such as 0.9, it means that the learning can occur quickly.Learning rate is how big you take a leap in finding optimal policy. Higher alpha means you are updating your Q values in big steps.\n",
    "Gamma is the discount factor that is set between 0 and 1. This models the fact that future rewards are worth less than immediate rewards.the return at the time t can be obtained using the return at the time t+1.\n",
    "\n",
    "Thus, alpha is the learning rate. If the reward or transition function is random, then alpha should change over the period, approaching zero at infinity. This has to effect approximating the expected outcome of an inner product (T(transition)*R(reward)), when one of the two, or both, has random behavior.\n",
    "Whereas, gamma is the value of future rewards. It can change the learning quite a bit and can be a dynamic or static value. If it is equal to one, the agent values future reward just as much as a current reward. This means, in ten actions, if an agent does something good this is just as valuable as doing this action directly. So learning doesn't work well at high gamma values.\n",
    "Similarly, a gamma of zero will cause the agent to only value immediate rewards, which only works with very detailed reward functions.\n",
    "\n",
    "Try at least one additional value for alpha and gamma:\n",
    "Baseline performance changed shown in the run result："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "narrow-condition",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 38, 38, 32)        320       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 38, 38, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 19, 19, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 11552)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               1478784   \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 6)                 198       \n",
      "=================================================================\n",
      "Total params: 1,483,430\n",
      "Trainable params: 1,483,430\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "episode: 1/10, steps: 1175, epsilon: 1\n",
      " loss : [81.61536]\n",
      "episode: 2/10, steps: 995, epsilon: 0.999\n",
      " loss : [75.832886]\n",
      "episode: 3/10, steps: 497, epsilon: 0.998001\n",
      " loss : [125.53765]\n",
      "episode: 4/10, steps: 834, epsilon: 0.997002999\n",
      " loss : [580.00104]\n",
      "episode: 5/10, steps: 414, epsilon: 0.996005996001\n",
      " loss : [1311.6854]\n",
      "episode: 6/10, steps: 708, epsilon: 0.995009990004999\n",
      " loss : [715.1908]\n",
      "episode: 7/10, steps: 690, epsilon: 0.994014980014994\n",
      " loss : [1013.2086]\n",
      "episode: 8/10, steps: 766, epsilon: 0.993020965034979\n",
      " loss : [588.84094]\n",
      "episode: 9/10, steps: 624, epsilon: 0.9920279440699441\n",
      " loss : [1530.7373]\n",
      "episode: 10/10, steps: 388, epsilon: 0.9910359161258742\n",
      " loss : [834.48175]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "class DQNAgent2:\n",
    "    def __init__(self, env):\n",
    "        self._env = env\n",
    "        self.memory = deque(maxlen=128)\n",
    "        self.gamma = 0.8 #changed\n",
    "        self.epsilon = 1  \n",
    "        self.epsilon_decay = 0.999 \n",
    "        self.epsilon_min = 0.5 #changed\n",
    "\n",
    "        self.simple_size = 16\n",
    "\n",
    "        self.model = self._Build_Deep_Q_Network()\n",
    "        self.opt = tf.keras.optimizers.Adam()\n",
    "\n",
    "    def _Build_Deep_Q_Network(self):\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(layers.Conv2D(filters=32, kernel_size=(3, 3), strides=1, padding='valid'))\n",
    "        model.add(layers.Activation('relu'))\n",
    "        model.add(layers.MaxPooling2D(pool_size=(2, 2), strides=2, padding='same'))\n",
    "\n",
    "        model.add(layers.Flatten())\n",
    "        model.add(layers.Dense(units=128, activation='relu'))\n",
    "        model.add(layers.Dense(units=32, activation='relu'))\n",
    "        model.add(layers.Dense(units=6))\n",
    "\n",
    "        model.build(input_shape=(None, 40, 40, 1))\n",
    "        model.summary()\n",
    "        return model\n",
    "\n",
    "    def save_memory(self, state, action, reward, next_state, done):\n",
    "\n",
    "        _state = np.array([state], dtype=np.float64)/255.0\n",
    "        _next_state = np.array([next_state], dtype=np.float64)/255.0\n",
    "\n",
    "        self.memory.append((_state, action, reward, _next_state, done))\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        batchs = min(self.simple_size, len(self.memory))\n",
    "        training_data = random.sample(self.memory, batchs)\n",
    "\n",
    "        loss_sum = 0.\n",
    "        count = 0\n",
    "        for i in training_data:\n",
    "            state, action, reward, next_state, done = i\n",
    "\n",
    "            y_reward = reward\n",
    "            if not done:\n",
    "                y_reward = reward + self.gamma * np.amax(self.model.predict(next_state)[0])\n",
    "\n",
    "            _y = self.model.predict(state)\n",
    "            _y[0][action] = y_reward\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                loss = tf.losses.mean_squared_error(y_true=_y, y_pred=self.model(state))\n",
    "                grad = tape.gradient(loss, self.model.trainable_variables)\n",
    "                self.opt.apply_gradients(zip(grad, self.model.trainable_variables))\n",
    "            loss_sum += loss\n",
    "\n",
    "            count += 1\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "        print(\" loss : {}\".format(loss_sum / count))\n",
    "        return loss_sum / count\n",
    "\n",
    "\n",
    "    def choice_action(self, state):\n",
    "        _state = np.array([state], dtype=np.float64)/255.0\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return self._env.action_space.sample()\n",
    "        else:\n",
    "            action = self.model.predict(_state)\n",
    "            return np.argmax(action[0])\n",
    "\n",
    "def preprocess(state):\n",
    "    state = state[20:]\n",
    "    state = np.mean(state, axis=2)\n",
    "    state = cv2.resize(state, dsize=(40, 40))\n",
    "    state = np.reshape(state, newshape=(40, 40, 1))\n",
    "\n",
    "    state[state < 10] = 0\n",
    "    state[state >= 10] = 255\n",
    "\n",
    "    return state\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    env = gym.make('SpaceInvaders-v0')\n",
    "    agent = DQNAgent2(env)\n",
    "\n",
    "    episodes = 10\n",
    "\n",
    "    all_loss = []\n",
    "    for e in range(episodes):\n",
    "        state = env.reset()\n",
    "        state = preprocess(state)\n",
    "\n",
    "        all_rewards = 0\n",
    "\n",
    "        for time_t in range(5000):\n",
    "\n",
    "            action = agent.choice_action(state)\n",
    "\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            next_state = preprocess(next_state)\n",
    "\n",
    "            if reward != 0.0:\n",
    "                agent.save_memory(state, action, reward, next_state, done)\n",
    "\n",
    "            state = copy.deepcopy(next_state)\n",
    "\n",
    "            all_rewards += reward\n",
    "\n",
    "            if done:\n",
    "                print(\"episode: {}/{}, steps: {}, epsilon: {}\"\n",
    "                      .format(e + 1, episodes, time_t, agent.epsilon))\n",
    "                break\n",
    "\n",
    "        loss = agent.train()\n",
    "        all_loss.append(loss[0])\n",
    "\n",
    "    plt.plot(all_loss)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inner-phrase",
   "metadata": {},
   "source": [
    "#5.Policy other than maxQ(s', a')\n",
    "\n",
    "If policy is not maxQ(s',a'), the learning target will change.\n",
    "\n",
    "If I change the policy to stay survive as long as possible, the agent will ignore the shooting score and bonus and try best to hide under the shelter and move to avoide attack gradually along with more and more training. \n",
    "\n",
    "#6.Decay rate and starting epsilon\n",
    "\n",
    "Epsilon is used when selecting specific actions base on the Q values we already have. As an example if I select pure greedy method ( epsilon = 0 ) then it will always selecting the highest q value among the all the q values for a specific state. This causes issue in exploration as I can get stuck easily at a local optima. Therefore introducing a randomness using epsilon is a good idea. \n",
    "\n",
    "Since epsilon denotes the amount of randomness in the policy (action is greedy with probability 1-epsilon and random with probability epsilon), I want to start with a fairly randomized policy and later slowly move towards a deterministic policy. Therefore, I usually start with a large epsilon and decay it to a small value. Most common and simple approaches are linear decay and exponential decay. \n",
    "\n",
    "Try at least one additional value for epsilon and the decay rate:\n",
    "Baseline performance changed shown in the run result：\n",
    "Reach max steps: 1526, and epsilon is 0.14770414800000004"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fifth-buying",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 38, 38, 32)        320       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 38, 38, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 19, 19, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 11552)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               1478784   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 198       \n",
      "=================================================================\n",
      "Total params: 1,483,430\n",
      "Trainable params: 1,483,430\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "episode: 1/10, steps: 737, epsilon: 0.5\n",
      " loss : [57.94394]\n",
      "episode: 2/10, steps: 681, epsilon: 0.333\n",
      " loss : [49.923172]\n",
      "episode: 3/10, steps: 395, epsilon: 0.22177800000000003\n",
      " loss : [29.728111]\n",
      "episode: 4/10, steps: 788, epsilon: 0.14770414800000004\n",
      " loss : [24.307]\n",
      "episode: 5/10, steps: 674, epsilon: 0.14770414800000004\n",
      " loss : [14.621981]\n",
      "episode: 6/10, steps: 1526, epsilon: 0.14770414800000004\n",
      " loss : [40.206112]\n",
      "episode: 7/10, steps: 1270, epsilon: 0.14770414800000004\n",
      " loss : [12.162366]\n",
      "episode: 8/10, steps: 697, epsilon: 0.14770414800000004\n",
      " loss : [12.3292885]\n",
      "episode: 9/10, steps: 1444, epsilon: 0.14770414800000004\n",
      " loss : [10.123312]\n",
      "episode: 10/10, steps: 765, epsilon: 0.14770414800000004\n",
      " loss : [8.399645]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "class DQNAgent3:\n",
    "    def __init__(self, env):\n",
    "        self._env = env\n",
    "        self.memory = deque(maxlen=128)\n",
    "        self.gamma = 0.5\n",
    "        self.epsilon = 0.5  #changed\n",
    "        self.epsilon_decay = 0.666 #changed\n",
    "        self.epsilon_min = 0.2\n",
    "\n",
    "        self.simple_size = 16\n",
    "\n",
    "        self.model = self._Build_Deep_Q_Network()\n",
    "        self.opt = tf.keras.optimizers.Adam()\n",
    "\n",
    "    def _Build_Deep_Q_Network(self):\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(layers.Conv2D(filters=32, kernel_size=(3, 3), strides=1, padding='valid'))\n",
    "        model.add(layers.Activation('relu'))\n",
    "        model.add(layers.MaxPooling2D(pool_size=(2, 2), strides=2, padding='same'))\n",
    "\n",
    "        model.add(layers.Flatten())\n",
    "        model.add(layers.Dense(units=128, activation='relu'))\n",
    "        model.add(layers.Dense(units=32, activation='relu'))\n",
    "        model.add(layers.Dense(units=6))\n",
    "\n",
    "        model.build(input_shape=(None, 40, 40, 1))\n",
    "        model.summary()\n",
    "        return model\n",
    "\n",
    "    def save_memory(self, state, action, reward, next_state, done):\n",
    "\n",
    "        _state = np.array([state], dtype=np.float64)/255.0\n",
    "        _next_state = np.array([next_state], dtype=np.float64)/255.0\n",
    "\n",
    "        self.memory.append((_state, action, reward, _next_state, done))\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        batchs = min(self.simple_size, len(self.memory))\n",
    "        training_data = random.sample(self.memory, batchs)\n",
    "\n",
    "        loss_sum = 0.\n",
    "        count = 0\n",
    "        for i in training_data:\n",
    "            state, action, reward, next_state, done = i\n",
    "\n",
    "            y_reward = reward\n",
    "            if not done:\n",
    "                y_reward = reward + self.gamma * np.amax(self.model.predict(next_state)[0])\n",
    "\n",
    "            _y = self.model.predict(state)\n",
    "            _y[0][action] = y_reward\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                loss = tf.losses.mean_squared_error(y_true=_y, y_pred=self.model(state))\n",
    "                grad = tape.gradient(loss, self.model.trainable_variables)\n",
    "                self.opt.apply_gradients(zip(grad, self.model.trainable_variables))\n",
    "            loss_sum += loss\n",
    "\n",
    "            count += 1\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "        print(\" loss : {}\".format(loss_sum / count))\n",
    "        return loss_sum / count\n",
    "\n",
    "\n",
    "    def choice_action(self, state):\n",
    "        _state = np.array([state], dtype=np.float64)/255.0\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return self._env.action_space.sample()\n",
    "        else:\n",
    "            action = self.model.predict(_state)\n",
    "            return np.argmax(action[0])\n",
    "\n",
    "def preprocess(state):\n",
    "    state = state[20:]\n",
    "    state = np.mean(state, axis=2)\n",
    "    state = cv2.resize(state, dsize=(40, 40))\n",
    "    state = np.reshape(state, newshape=(40, 40, 1))\n",
    "\n",
    "    state[state < 10] = 0\n",
    "    state[state >= 10] = 255\n",
    "\n",
    "    return state\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    env = gym.make('SpaceInvaders-v0')\n",
    "    agent = DQNAgent3(env)\n",
    "\n",
    "    episodes = 10\n",
    "\n",
    "    all_loss = []\n",
    "    for e in range(episodes):\n",
    "        state = env.reset()\n",
    "        state = preprocess(state)\n",
    "\n",
    "        all_rewards = 0\n",
    "\n",
    "        for time_t in range(5000):\n",
    "\n",
    "            action = agent.choice_action(state)\n",
    "\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            next_state = preprocess(next_state)\n",
    "\n",
    "            if reward != 0.0:\n",
    "                agent.save_memory(state, action, reward, next_state, done)\n",
    "\n",
    "            state = copy.deepcopy(next_state)\n",
    "\n",
    "            all_rewards += reward\n",
    "\n",
    "            if done:\n",
    "                print(\"episode: {}/{}, steps: {}, epsilon: {}\"\n",
    "                      .format(e + 1, episodes, time_t, agent.epsilon))\n",
    "                break\n",
    "\n",
    "        loss = agent.train()\n",
    "        all_loss.append(loss[0])\n",
    "\n",
    "    plt.plot(all_loss)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "behind-institute",
   "metadata": {},
   "source": [
    "#7. Average number of steps taken per episode\n",
    "\n",
    "Detailed average steps for each episode is shown in part 5 result.\n",
    "\n",
    "#8.Does Q-learning use value-based or policy-based iteration?\n",
    "\n",
    "Q learning is a value-based off-policy temporal difference(TD) reinforcement learning. Off-policy means an agent follows a behaviour policy for choosing the action to reach the next state s_t+1 from state s_t. From s_t+1, it uses a policy π that is different from behaviour policy.\n",
    "\n",
    "#9.Could you use SARSA for this problem? \n",
    "\n",
    "No, SARSA is not suitable for this problem.\n",
    "The main difference between SARSA and Q-learning is that SARSA is on-policy -- it learns the values of acting including the action choices used for exploration.All variants of SARSA are on-policy algorithms. This means that they learn Q-values for the policy that they are also following to generate experience.\n",
    "We take SpaceInvaders game by deep Q learning, our goal is to maximize the shooting score of the agent.We don't need the agent to change the policy or make the goal and action flexible by itself.This game should eventually settle down on a single policy, such that your Q values can converge to the values that are optimal for that policy.\n",
    "\n",
    "\n",
    "#10.The expected lifetime value in the Bellman equation\n",
    "\n",
    "Mathematically we can define Bellman Expectation Equation as :\n",
    "![image-6.png](attachment:image-6.png)\n",
    "The above equation tells us that the value of a particular state is determined by the immediate reward plus the value of successor states when we are following a certain policy(π).\n",
    "\n",
    "The term 'Bellman equation' usually refers to the dynamic programming equation associated with discrete-time optimization problems.Dynamic programming breaks a multi-period planning problem into simpler steps at different points in time. Therefore, it requires keeping track of how the decision situation is evolving over time.The Bellman equation is classified as a functional equation, because solving it means finding the unknown function V, which is the value function. Recall that the value function describes the best possible value of the objective, as a function of the state x. By calculating the value function, we will also find the function a(x) that describes the optimal action as a function of the state; this is called the policy function[3].\n",
    "\n",
    "\n",
    "#11.When would SARSA likely do better than Q-learning?\n",
    "\n",
    "Q-learning assumes that the agent is following the best possible policy without attempting to resolve what that policy actually is, while SARSA takes into account the agent's actual policy (that is, what it ends up doing when it moves to the next state as opposed to the best possible thing it could be assumed to do).\n",
    "If the goal is to train an optimal agent in simulation, or in a low-cost and fast-iterating environment, then Q-learning is a good choice, due to the first point (learning optimal policy directly). If the agent learns online, and you care about rewards gained whilst learning, then SARSA may be a better choice.\n",
    "In many problems, SARSA will perform better than Q-learning, especially when there is a good chance that the agent will choose to take a random suboptimal action in the next step.Q-learning's assumption that the agent is following the optimal policy may be far enough from true that SARSA will converge faster and with fewer errors.\n",
    "\n",
    "\n",
    "#12. SARSA differ from Q-learning\n",
    "\n",
    "Sarsa : On-policy TD control\n",
    "Q-learning : Off-policy TD control\n",
    "In the Sarsa loop,we will always choose the action A(t+1) of state S(t+1) first in state St under the policy. And then, update the Q(St, At) using Q(St+1, At+1). After that, St ← S(t+1), At ← A(t+1).\n",
    "![image-4.png](attachment:image-4.png)\n",
    "As for Q-learning loop, we will always choose the current At in current state St. And then, estimate the biggest Q(St+1, a) of next state. But when we go the next loop, we will choose the action a again. Whether a is A(t+1) chosen before, it is not decided.\n",
    "![image-5.png](attachment:image-5.png)\n",
    "\n",
    "The major difference between Sarsa and Q-Learning is that the maximum reward for the next state is not necessarily used for updating the Q-values. Instead, a new action, and therefore reward, is selected using the same policy that determined the original action.\n",
    "the difference between the two updates mathematically is indeed that, when updating the Q-value for a state-action pair (St, At):\n",
    "Sarsa uses the behaviour policy (meaning, the policy used by the agent to generate experience in the environment, which is typically epsilon-greedy) to select an additional action At+1, and then uses Q(St+1, At+1) (discounted by gamma) as expected future returns in the computation of the update target; Q-learning does not use the behaviour policy to select an additional action At+1. Instead, it estimates the expected future returns in the update rule as maxA Q(St+1, A). The max operator used here can be viewed as \"following\" the completely greedy policy. \n",
    "Watkin's Q-learning updates an estimate of the optimal state-action value function Q* based on the maximum reward of available actions. While SARSA learns the Q values associated with taking the policy it follows itself, Watkin's Q-learning learns the Q values associated with taking the optimal policy while following an exploration/exploitation policy.\n",
    " \n",
    "\n",
    "#13.Q-learning algorithm\n",
    "\n",
    "Q-learning is a model-free reinforcement learning algorithm to learn quality of actions telling an agent what action to take under what circumstances. It does not require a model (hence the connotation \"model-free\") of the environment, and it can handle problems with stochastic transitions and rewards, without requiring adaptations[1].Q-learning is an off policy reinforcement learning algorithm that seeks to find the best action to take given the current state.More specifically, q-learning seeks to learn a policy that maximizes the total reward.\n",
    "The ‘q’ in q-learning stands for quality.Q-Learning is based on the notion of a Q-function. The Q-function uses the Bellman equation and takes two inputs: state (s) and action (a).\n",
    "![image-3.png](attachment:image-3.png)\n",
    "𝑄(𝑠𝑡,𝑎𝑡)←𝑄(𝑠𝑡,𝑎𝑡)+α[𝑟𝑡+1+γmax𝑎′𝑄(𝑠𝑡+1,𝑎′)−𝑄(𝑠𝑡,𝑎𝑡)]\n",
    "The procedural form of the algorithm is:\n",
    "Initialize Q(s,a) arbitrarily\n",
    "Repeat (for each episode):\n",
    "    Initialize s\n",
    "    Repeat (for each step of episode):\n",
    "        Choose a from s using policy derived from Q\n",
    "            (e.g., e-greedy)\n",
    "        Take action a. observe r, s'\n",
    "        Q(s,a)<-- Q(s,a)+alpha[r+ gamma* maxalpha, Q(s',a')-Q(s,a)]\n",
    "        s<--s';\n",
    "    until s is terminal\n",
    "  \n",
    "#14.SARSA algorithm\n",
    "\n",
    "State–action–reward–state–action (SARSA) is an algorithm for learning a Markov decision process policy, used in the reinforcement learning area of machine learning.This name simply reflects the fact that the main function for updating the Q-value depends on the current state of the agent \"S1\", the action the agent chooses \"A1\", the reward \"R\" the agent gets for choosing this action, the state \"S2\" that the agent enters after taking that action, and finally the next action \"A2\" the agent chooses in its new state. The acronym for the quintuple (st, at, rt, st+1, at+1) is SARSA.\n",
    "Q(s,a)=Q(s,a)+α[r+γQ(s′,a′)−Q(s,a))]\n",
    "𝑄(𝑠𝑡,𝑎𝑡)←𝑄(𝑠𝑡,𝑎𝑡)+α[𝑟𝑡+1+γ𝑄(𝑠𝑡+1,𝑎𝑡+1)−𝑄(𝑠𝑡,𝑎𝑡)]\n",
    "{\\displaystyle Q(s_{t},a_{t})\\leftarrow Q(s_{t},a_{t})+\\alpha \\,[r_{t}+\\gamma \\,Q(s_{t+1},a_{t+1})-Q(s_{t},a_{t})]}\n",
    "A SARSA agent interacts with the environment and updates the policy based on actions taken, hence this is known as an on-policy learning algorithm. The Q value for a state-action is updated by an error, adjusted by the learning rate alpha. Q values represent the possible reward received in the next time step for taking action a in state s, plus the discounted future reward received from the next state-action observation[2].\n",
    "The procedural form of the algorithm is:\n",
    "Initialize Q(s,a) arbitrarily\n",
    "Repeat (for each episode):\n",
    "    Initialize s\n",
    "    Choose a from s using policy derived from Q\n",
    "            (e.g., e-greedy)\n",
    "    Repeat(for each step of episode):\n",
    "        Take action a. observe r, s'\n",
    "        Choose a'from s' using policy derived from Q\n",
    "            (e.g., e-greedy)\n",
    "        Q(s,a)<-- Q(s,a)+alpha[r+ gamma* Q(s',a')-Q(s,a)]\n",
    "        s<--s'; a<--a';\n",
    "    until s is terminal\n",
    "\n",
    "There are two action selection steps needed, for determining the next state-action pair along with the first. The parameters  and  have the same meaning as they do in Q-Learning.\n",
    "\n",
    "#15.Citing and Reference\n",
    "\n",
    "The reference and cited code are listed below.\n",
    "MIT License and copyright is shown at the end of this file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nearby-healing",
   "metadata": {},
   "source": [
    "# Reference\n",
    "\n",
    "[1]: Q-learning-Wikipedia. URL: https://en.wikipedia.org/wiki/Q-learning\n",
    "[2]: State–action–reward–state–action-Wikipedia. URL:https://en.wikipedia.org/wiki/State%E2%80%93action%E2%80%93reward%E2%80%93state%E2%80%93action.\n",
    "[3]: Bellman equation-Wikipedia. URL: https://en.wikipedia.org/wiki/Bellman_equation\n",
    "[4]: Deep Q Networks-Deep Q Learning Explained. URL:https://towardsdatascience.com/dqn-part-1-vanilla-deep-q-networks-6eb4a00febfb\n",
    "[5]: Deep Q Learning with Atari© Space Invaders©. URL:https://github.com/simoninithomas/Deep_reinforcement_learning_Course/blob/master/Deep%20Q%20Learning/Space%20Invaders/DQN%20Atari%20Space%20Invaders.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organic-seller",
   "metadata": {},
   "source": [
    "Copyright <2021> <Mengzhe Zhang>\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files\n",
    "(the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge,\n",
    "publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do \n",
    "so,subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF \n",
    "MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE \n",
    "FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR \n",
    "IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
